
```ad-note
近端策略优化（Proximal Policy Optimization，PPO），其性能与最先进的方法相当或更好，同时代码实现和优化起来更简单。
```

实际上，与其实际影响相比，这是一个非常谦虚的说法。策略梯度方法存在收敛问题，自然策略梯度可以解决这个问题。然而，在实践中，自然策略梯度涉及二阶导数矩阵，这使得它不适用于大规模问题。对于实际任务来说，计算复杂度太高。人们进行了深入的研究，通过近似二阶方法来降低复杂度。PPO 采用了略有不同的方法。它没有施加硬约束，而是将约束形式化为目标函数中的惩罚。通过不惜一切代价避免约束，我们可以使用像梯度下降法这样的一阶优化器来优化目标。即使我们偶尔违反约束，损害也要小得多，计算也简单得多。在详细解释 PPO 之前，让我们快速过一遍基本概念。

## 最小化-最大化 MM 算法

我们如何优化策略以最大化回报？

使用 Minorize-Maximization **MM** 算法，通过最大化下限函数 $M$ （下图蓝线）来 **迭代** 实现这一点，该函数在局部近似预期奖励 $η$ 。

![[3.6.excalidraw|600x100]]

首先，我们从一个初始策略猜测开始，并找到该策略下 $η$ 的下界 $M$ 。我们优化 $M$，并将 $M$ 的最优策略作为下一个猜测。我们再次在新的猜测下逼近新的下界，并重复迭代，直到策略收敛。为了使其有效，我们需要找到一个更容易优化的下界 $M$ 。

## 线性搜索

主要的优化方法有两种：类似于梯度下降的线性搜索和置信域。梯度下降在优化目标函数方面简单易行、快速且准确。这也是它在深度学习中如此受欢迎的原因，甚至有更精确的方法可供选择。

线性搜索首先选择最陡峭的方向，然后向前移动一个步长。但在强化学习中，这种策略怎么会出错呢？让我们带一个机器人去天使降临峰徒步旅行。如下所示，我们先确定方向，然后爬山。如果步长太小，到达山顶将需要很长时间。但如果步长太大，我们可能会掉下悬崖。即使机器人在坠落中幸存下来，它落在的高度也会比我们之前的位置低得多。策略梯度主要是一种 **同策略方法** 。它从当前状态搜索动作。因此，我们用局部糟糕的策略从糟糕的状态重新开始探索。这会严重损害性能。

![[1_k9NFvgjf2XGiLiiarp0Q2Q.webp]]

## 置信域

在置信域中，我们首先确定想要探索的最大步长（下图黄色圆圈）。然后，我们在置信域内找到最优点，并从那里继续搜索。

![[1_eQDsFaMkSXw0g6gVIErXaQ.webp]]

**置信域的最大步长是多少？** 在置信域方法中，我们从一个初始猜测开始。我们也可以动态地重新调整区域大小。例如，如果新策略和当前策略的差异变大（反之亦然），我们可以缩小区域。为了避免做出错误的决策，如果策略变化过大，我们可以缩小置信域。

在 PPO 中，我们通过 KL 散度来限制每次迭代中策略的改变程度。KL 散度衡量两个数据分布 $P$ 和 $Q$ 之间的差异。

$$
D_{KL}(P\Vert Q)=\mathbb{E}_x\log\frac{P(x)}{Q(x)}
$$

KL 散度用来衡量两个策略之间的差异。我们不希望新策略与现有策略之间有太大的差异。

![[1_3QNN7laZTbW7guFT-YnJNg.webp]]

那么，我们如何限制策略变化，以确保我们不会做出错误的决定呢？事实证明，我们可以找到一个下限函数 $M$ ，如下所示：

$$
M=L(\theta)-C\cdot\overset{\_\_\_\_}{KL}
$$

![[1_FQDiuNJ1WptAN3LnBKYzWw.webp]]

其中 $L(\theta)$ 等于

$$
\hat{\mathbb{E}_t}\left[{\frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}}\hat{A}_t\right]
$$

第二项是 KL 散度。

$L$ 是新策略的预期优势函数（预期奖励减去基线奖励，例如 $V(s)$ ）。它由旧策略（或当前策略）估算，然后使用新旧策略之间的概率比重新校准。==我们使用优势函数代替预期奖励，因为它减少了估计的方差== ，只要基线不依赖于我们的策略参数，最优策略就会是一样的。

让我们更详细地研究 $M$ 中的第二项。经过 TRPO 论文中两页的证明，我们可以建立以下下限。

![[1_xqv1aAr6DSXnYFSUMWb6bg.webp]]

$M$ 中的第二项是上面红色下划线所示的 KL 散度的最大值。但是它太难找到了，所以我们稍微放宽了要求，改用 KL 散度的平均值。让我们直观地解释一下。

## 直观解释

$L$ 在当前策略下局部近似优势函数。但随着它远离旧策略，其准确度会降低。这种不准确性有一个上限，即 $M$ 中的第二项。考虑到这个误差的上限，我们可以保证在置信域内计算出的最优策略始终优于旧策略。如果策略在置信域之外，即使计算值可能更好，但准确度可能偏差太大，不可信。所以不能超出置信域。我们将目标总结如下：

![[1_LH3dbwj7MEYCICAQMzr-zg.webp]]

从数学上讲，上述两个方程都可以解析为相同的最优策略。然而， $δ$ 的理论阈值非常小，被认为过于保守。因此，我们再次放宽条件，将其设置为可调超参数。

如前所述， $M$ 应该易于优化。因此，我们进一步将其近似为一个凸函数形式的二次方程，并深入研究如何在高维空间中对其进行优化。

我们使用泰勒级数将这些项展开到二阶。但是 𝓛 的二阶项比 KL 散度项小得多，因此将被忽略。

![[1_QKn9GhIMxJmXK_-nAQ8ojA.webp]]

![[1_A386qkCNen7IXjwfFbH2Og.webp]]

因此，目标和约束可以近似为：

![[1_Sm49fOBxZBvzgSIo8zm-fA.webp]]

我们的目标是：

![[1_3aO-ncPjkdXwfTEsoik9Hg.webp]]

我们可以解析地解这个二次方程。解是：

![[3.22.excalidraw]]

## 存在哪些问题？

该解决方案涉及二阶导数及其逆的计算，这是一项非常昂贵的操作。

$$
\mathbf{H} = \nabla^2 f = 
\begin{bmatrix}
\frac{\partial^2 f}{\partial x_1^2} & \frac{\partial^2 f}{\partial x_1 \partial x_2} & \cdots & \frac{\partial^2 f}{\partial x_1 \partial x_n} \\
\frac{\partial^2 f}{\partial x_2 \partial x_1} & \frac{\partial^2 f}{\partial x_2^2} & \cdots & \frac{\partial^2 f}{\partial x_2 \partial x_n} \\
\vdots & \vdots & \ddots & \vdots \\
\frac{\partial^2 f}{\partial x_n \partial x_1} & \frac{\partial^2 f}{\partial x_n \partial x_2} & \cdots & \frac{\partial^2 f}{\partial x_n^2}
\end{bmatrix}
$$


因此，有两种方法可以解决这个问题：

- 对涉及二阶导数及其逆的一些计算进行近似，以降低计算复杂度
- 通过添加软约束，使一阶导数解像梯度下降一样更接近二阶导数解。

TRPO 采用第一种方法。PPO 更接近第二种方法。我们仍然可以忍受偶尔出现的错误策略决策，因此我们坚持使用像随机梯度下降这样的一阶解。但我们将在目标函数中添加一个软约束，这样优化过程就能更好地确保我们在置信域内进行优化。因此，出现错误决策的概率会更小。

## 具有自适应 KL 惩罚的 PPO

制定目标的一种方法是将目标函数中的约束改为惩罚：

$$
\underset{\theta}{\text{最大化}}\quad\hat{\mathbb{E}}_t\left[{\frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}\hat{A}_t}\right]-\beta\hat{\mathbb{E}_t}[KL[\pi_{\theta_{old}}(\cdot|s_t),\pi_\theta(\cdot|s_t)]]
$$

$β$ 控制惩罚的权重。如果新策略与旧策略不同，则惩罚目标。借鉴置信域的概念，我们可以动态调整 $β$ 。下方的 $d$ 是新旧策略之间的 KL 散度。如果它高于目标值，我们就缩小 $β$ 。同样，如果它低于另一个目标值，我们就扩大置信域。

$$
d=\hat{\mathbb{E}_t}[KL[\pi_{\theta_{old}}(\cdot|s_t),\pi_\theta(\cdot|s_t)]]
$$

算法如下

```pseudo
    \begin{algorithm}
    \caption{1 带自适应KL惩罚的PPO}
    \begin{algorithmic}
      \State 输入：初始化策略参数 $\theta_0$ ，初始化 KL 惩罚 $\beta_0$ ，KL散度的目标 $\delta$
      \For{$k = 0,1,2,\dots$}
      \State 使用策略 $\pi_k=\pi(\theta_k)$ 采集一组策略 $\mathcal{D}_k$
      \State 计算优势 $\hat{A}_t^{\pi_k}$
      \State 计算策略的更新（通过执行K步的微批次SGD，使用Adam优化器）
      \State
      $
      \theta_{k+1}=\underset{\theta}{\arg\max}\mathcal{L}_{\theta_k}(\theta)-\beta_k\overset{\_}{D}_{KL}(\theta\Vert\theta_k)
      $
      \If{$\overset{\_}{D}_{KL}(\theta_{k+1}\Vert\theta_k)\ge 1.5\delta$}
      \State $\beta_{k+1}=2\beta_k$
      \Elif{$\overset{\_}{D}_{KL}(\theta_{k+1}\Vert\theta_k)\le \delta/1.5$}
      \State $\beta_{k+1}=\beta_k/2$
      \EndIf
      \EndFor
      \end{algorithmic}
    \end{algorithm}
```

这使得 TRPO 的性能更接近梯度下降法的速度。但我们能做得更好吗？

## 带裁剪目标的PPO

带有裁剪目标的 PPO 甚至可以做得更好。在其实现中，我们维护两个策略网络。第一个是我们想要改进的当前策略。

$$
\pi_\theta(a_t|s_t)
$$

第二个是我们上次采集样本的策略。

$$
\pi_{\theta_k}(a_t|s_t)
$$

利用重要性采样的思想，我们可以用从旧策略中收集的样本来评估新策略，从而提高样本效率。

$$
\underset{\theta}{\text{最大化}}\quad\hat{\mathbb{E}}_t\left[{\frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}\hat{A}_t}\right]
$$

但是，随着我们对现有策略进行改进，现有策略与旧策略之间的差异会越来越大。估计的方差会增加，我们可能会因为不准确而做出错误的决策。因此，假设每进行4次迭代，我们就将第二个网络与改进后的策略再次同步。

$$
\pi_{\theta_{k+1}}(a_t|s_t)\leftarrow\pi_\theta(a_t|s_t)
$$

通过修剪目标，我们计算新策略与旧策略之间的比率：

$$
r_t(\theta)=\frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{k}}(a_t|s_t)}
$$

这个比率衡量了两个策略之间的差异程度。如果新策略与旧策略相差甚远，我们会构建一个新的目标函数来限制估计的优势函数。我们的新目标函数变为：

$$
\mathcal{L}^{CLIP}_{\theta_k}=\underset{\tau\sim\pi_k}{\mathbb{E}}\left[{\sum_{t=0}^T[\min(r_t(\theta)\hat{A}_t^{\pi_k},\text{clip}(r_t(\theta),1-\epsilon,1+\epsilon)\hat{A}_t^{\pi_k})]}\right]
$$

如果新策略与旧策略的概率比超出 $(1-\epsilon)$ 和 $(1+\epsilon)$ 的范围，则优势函数将被剪裁。在 PPO 论文中的实验中，$\epsilon$ 设置为 $0.2$ 。

![[1_MpPiARNoNGCxJE2a8m9itA.webp]]

实际上，如果策略发生重大变化超出了我们的舒适区，那么这将阻碍策略的实现。

```ad-note
分情况讨论：

- $A>0$ 且 $\frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{k}}(a_t|s_t)}>1$
	- $A>0$ 说明当前动作比平均策略产生的动作表现好，策略应该 **增加** 该动作的概率
	- $\frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{k}}(a_t|s_t)}>1$ 说明新策略相比于旧策略提高了该动作的概率，为了不让策略更新过于剧烈，会截断在 $1+\epsilon$
- $A>0$ 且 $\frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{k}}(a_t|s_t)}<1$
	- $A>0$ 说明当前动作比平均策略产生的动作表现好，策略应该 **增加** 该动作的概率
	- $\frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{k}}(a_t|s_t)}<1$ 说明新策略相比于旧策略降低了该动作的概率，这与策略改进目标相反，所以，PPO的目标函数会 **惩罚** 这种行为，促使策略恢复增加优势动作的概率。裁剪机制防止惩罚过重，保证训练稳定。
- $A<0$ 且 $\frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{k}}(a_t|s_t)}>1$
	- $A<0$ 说明当前动作比平均策略产生的动作表现差，策略应该 **减小** 该动作的概率
	- $\frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{k}}(a_t|s_t)}>1$ 说明新策略相比于旧策略提高了该动作的概率，这与策略改进目标相反，所以，PPO的目标函数会 **惩罚** 这种行为，促使策略恢复减小动作的概率。裁剪机制防止惩罚过重，保证训练稳定。
- $A<0$ 且 $\frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{k}}(a_t|s_t)}<1$
	- $A<0$ 说明当前动作比平均策略产生的动作表现差，策略应该 **减小** 该动作的概率
	- $\frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{k}}(a_t|s_t)}<1$ 说明新策略相比于旧策略减小了该动作的概率，为了不让策略更新过于剧烈，会截断在 $1-\epsilon$
```

算法如下

```pseudo
    \begin{algorithm}
    \caption{2 带裁剪目标的PPO}
    \begin{algorithmic}
      \State 输入：初始化策略参数 $\theta_0$ ，初始化裁剪阈值参数 $\epsilon$
      \For{$k = 0,1,2,\dots$}
      \State 使用策略 $\pi_k=\pi(\theta_k)$ 采集一组策略 $\mathcal{D}_k$
      \State 计算优势 $\hat{A}_t^{\pi_k}$
      \State 计算策略的更新（通过执行K步的微批次SGD，使用Adam优化器）
      \State
      $
      \theta_{k+1}=\underset{\theta}{\arg\max}\mathcal{L}_{\theta_k}^{CLIP}(\theta)
      $
      \State 其中
      \State
      $\quad\quad
      \mathcal{L}^{CLIP}_{\theta_k}=\underset{\tau\sim\pi_k}{\mathbb{E}}\left[{\sum_{t=0}^T[\min(r_t(\theta)\hat{A}_t^{\pi_k},\text{clip}(r_t(\theta),1-\epsilon,1+\epsilon)\hat{A}_t^{\pi_k})]}\right]
      $
      \EndFor
      \end{algorithmic}
    \end{algorithm}
```

这个新方法很简单，而且可以像 Adam 一样使用梯度下降来优化。其实，对这个问题分析得这么详细，却得出这么简单的解决方案，实在是有点虎头蛇尾。

PPO 增加了一个软约束，可以通过一阶优化器进行优化。我们偶尔可能会做出一些错误的决策，但它在优化速度上取得了良好的平衡。实验结果证明，这种平衡能够以最简单的方式实现最佳性能。

```ad-note
深度学习中的简单规则。
```

或者至少等我们发明超快的 GPU 。而这是不可能的。

## PPO 代码实践

我们还是使用倒立摆环境来测试 PPO 算法。大量实现表明：带裁剪目标的PPO算法比带KL惩罚的PPO算法表现更好。所以我们实现带裁剪目标的PPO算法。

首先定义策略的网络结构和价值网络

```python
import gym
import torch
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt

class PolicyNet(torch.nn.Module):
	'''策略网络'''
    def __init__(self, state_dim, hidden_dim, action_dim):
        super(PolicyNet, self).__init__()
        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)
        self.fc2 = torch.nn.Linear(hidden_dim, action_dim)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        return F.softmax(self.fc2(x), dim=1)

class ValueNet(torch.nn.Module):
	'''价值网络'''
    def __init__(self, state_dim, hidden_dim):
        super(ValueNet, self).__init__()
        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)
        self.fc2 = torch.nn.Linear(hidden_dim, 1)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        return self.fc2(x)
```

然后我们来实现带裁剪目标的PPO算法

```python
class PPO:
    ''' PPO算法,采用截断方式 '''
    def __init__(self, state_dim, hidden_dim, action_dim, actor_lr, critic_lr,
                 lmbda, epochs, eps, gamma, device):
        # 演员是策略网络
        self.actor = PolicyNet(state_dim, hidden_dim, action_dim).to(device)
        # 评论家是价值网络
        self.critic = ValueNet(state_dim, hidden_dim).to(device)
        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(),
                                                lr=actor_lr)
        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(),
                                                 lr=critic_lr)
        self.gamma = gamma
        self.lmbda = lmbda
        self.epochs = epochs
        self.eps = eps  # PPO中截断范围的参数
        self.device = device

	# 采取动作
    def take_action(self, state):
        state = torch.tensor([state], dtype=torch.float).to(self.device)
        probs = self.actor(state)
        action_dist = torch.distributions.Categorical(probs)
        action = action_dist.sample()
        return action.item()

	# 更新策略
    def update(self, transition_dict):
        states = torch.tensor(transition_dict['states'], dtype=torch.float).to(self.device)
        actions = torch.tensor(transition_dict['actions']).view(-1, 1).to(self.device)
        rewards = torch.tensor(transition_dict['rewards'], dtype=torch.float).view(-1, 1).to(self.device)
        next_states = torch.tensor(transition_dict['next_states'], dtype=torch.float).to(self.device)
        dones = torch.tensor(transition_dict['dones'], dtype=torch.float).view(-1, 1).to(self.device)

        td_target = rewards + self.gamma * self.critic(next_states) * (1 - dones)
        td_delta = td_target - self.critic(states)

        advantage = compute_advantage(self.gamma, self.lmbda, td_delta.cpu()).to(self.device)

        old_log_probs = torch.log(self.actor(states).gather(1, actions)).detach()

        for _ in range(self.epochs):
            log_probs = torch.log(self.actor(states).gather(1, actions))
            ratio = torch.exp(log_probs - old_log_probs)
            surr1 = ratio * advantage
            surr2 = torch.clamp(ratio, 1 - self.eps, 1 + self.eps) * advantage
            actor_loss = torch.mean(-torch.min(surr1, surr2))  # PPO损失函数
            critic_loss = torch.mean(
                F.mse_loss(self.critic(states), td_target.detach()))
            self.actor_optimizer.zero_grad()
            self.critic_optimizer.zero_grad()
            actor_loss.backward()
            critic_loss.backward()
            self.actor_optimizer.step()
            self.critic_optimizer.step()
```

算法中计算优势的代码 `compute_advantage` 如下

```python
def compute_advantage(gamma, lmbda, td_delta):
    td_delta = td_delta.detach().numpy()
    advantage_list = []
    advantage = 0.0
    for delta in td_delta[::-1]:
        advantage = gamma * lmbda * advantage + delta
        advantage_list.append(advantage)
    advantage_list.reverse()
    return torch.tensor(advantage_list, dtype=torch.float)
```

接下来，在倒立摆环境中训练 PPO 算法。

```python
actor_lr = 1e-3
critic_lr = 1e-2
num_episodes = 500
hidden_dim = 128
gamma = 0.98
lmbda = 0.95
epochs = 10
eps = 0.2
device = torch.device("cuda") if torch.cuda.is_available() else torch.device(
    "cpu")

env_name = 'CartPole-v0'
env = gym.make(env_name)
env.seed(0)
torch.manual_seed(0)
state_dim = env.observation_space.shape[0]
action_dim = env.action_space.n
agent = PPO(state_dim, hidden_dim, action_dim, actor_lr, critic_lr, lmbda,
            epochs, eps, gamma, device)

return_list = train_on_policy_agent(env, agent, num_episodes)
```

其中 `train_on_policy_agent` 如下

```python
def train_on_policy_agent(env, agent, num_episodes):
    return_list = []
    for i in range(10):
        with tqdm(total=int(num_episodes/10), desc='Iteration %d' % i) as pbar:
            for i_episode in range(int(num_episodes/10)):
                episode_return = 0
                transition_dict = {
	                'states': [],
	                'actions': [],
	                'next_states': [],
	                'rewards': [],
	                'dones': []
	            }
                state = env.reset()
                done = False
                while not done:
                    action = agent.take_action(state)
                    next_state, reward, done, _ = env.step(action)
                    transition_dict['states'].append(state)
                    transition_dict['actions'].append(action)
                    transition_dict['next_states'].append(next_state)
                    transition_dict['rewards'].append(reward)
                    transition_dict['dones'].append(done)
                    state = next_state
                    episode_return += reward
                return_list.append(episode_return)
                agent.update(transition_dict)
                if (i_episode+1) % 10 == 0:
                    pbar.set_postfix({'episode': '%d' % (num_episodes/10 * i + i_episode+1), 'return': '%.3f' % np.mean(return_list[-10:])})
                pbar.update(1)
    return return_list
```

可视化一下训练结果

```python
episodes_list = list(range(len(return_list)))
plt.plot(episodes_list, return_list)
plt.xlabel('Episodes')
plt.ylabel('Returns')
plt.title('CartPole-v0')
plt.show()
```

![[Figure_1.png]]

## 总结

PPO 是 TRPO 的一种改进算法，它在实现上简化了 TRPO 中的复杂计算，并且它在实验中的性能大多数情况下会比 TRPO 更好，因此目前常被用作一种常用的基准算法。需要注意的是，TRPO 和 PPO 都属于同策略学习算法，即使优化目标中包含重要性采样的过程，但其只是用到了上一轮策略的数据，而不是过去所有策略的数据。

PPO 是 TRPO 的第一作者 John Schulman 从加州大学伯克利分校博士毕业后在 OpenAI 公司研究出来的。通过对 TRPO 计算方式的改进，PPO 成为了最受关注的深度强化学习算法之一，并且其论文的引用量也超越了 TRPO。