
```ad-note
Trust Region Policy Optimization(TRPO)
```

![[2.13.excalidraw]]

策略梯度法在强化学习中非常流行。其基本原理是利用梯度上升法来找到奖励增长最快的策略。然而，一阶优化器对于曲线区域来说，不太准确。我们可能会过度自信，做出错误的举动，从而破坏训练的进程。TRPO 是解决这一问题最常被引用的论文之一。然而，人们经常在解释 TRPO 时忽略其基本概念。在本章开始，我们将重点探讨策略梯度法的问题，并介绍三个基本概念：MM 算法、置信域和重要性采样。

## 策略梯度法的问题

在强化学习中，我们会优化策略 $\theta$ ，以获得最大预期折扣奖励。然而，仍存在一些挑战会影响策略梯度法的性能。

$$
\max_\theta J(\pi_\theta) = \underset{\tau\sim\pi_\theta}{\mathbb{E}}\left\lbrack{\sum_{t=0}^\infty\gamma^tr_t}\right\rbrack
$$

首先，策略梯度法计算奖励的最陡上升方向（ **策略梯度** $g$ ），并朝该方向更新策略。

![[3.1.excalidraw|600x100]]

然而，这种方法使用一阶导数，并将表面近似为平面。如果表面曲率较大，我们可能会做出可怕的举动。

![[3.2.excalidraw|600x100]]

**步子迈得太大会导致灾难** 。但如果步子太小，模型学习速度就会太慢。把奖励函数想象成高耸的山峰。如果新策略走得太远，它采取的行动可能会差之毫厘，最终从悬崖上掉下来。当我们重新开始探索时，我们从一个表现不佳的状态开始，采用局部糟糕的策略。性能会崩溃，而且需要很长时间才能恢复。

![[3.3.excalidraw|600x100]]

其次，在强化学习中很难找到合适的学习率。假设学习率是专门针对上图黄点调整的。该区域相对平坦，因此为了获得良好的学习速度，学习率应该高于平均值。但是，一步走错，我们就会从悬崖上掉到红点。红点处的梯度很高，当前的学习率会触发爆炸式策略更新。由于学习率对地形不敏感，策略梯度算法的收敛问题非常严重。

第三，我们是否应该限制策略变化，以免做出过于激进的举动？事实上，TRPO 就是这么做的。它限制了那些对地形敏感的参数变化。但提供这种解决方案并不显而易见。我们通过低水平的模型参数来调整策略。为了限制策略变化，模型参数的相应阈值是什么？我们如何将策略空间的变化转化为模型参数空间的变化？

第四，我们仅对一次策略更新采样整个轨迹。我们无法在每个时间步都更新策略。

![[3.4.excalidraw|600x100]]

为什么？将策略模型可视化为一张网络。增加某一点 $\pi(s)$ 的概率，也会拉高相邻点。轨迹内的状态相似，尤其是在用原始像素表示时。如果我们在每个时间步升级策略，我们实际上会在相似的点多次拉高网络。这些变化相互强化和放大，使训练变得非常敏感且不稳定。

![[3.5.excalidraw|600x100]]

考虑到一条轨迹可能有数百或数千步，每条轨迹都进行一次更新的 **样本效率** 不高。策略梯度法需要超过 1000 万甚至更多的训练时间步来进行小规模实验。对于机器人技术的实际模拟来说，这太昂贵了。

```ad-note
样本效率不高的意思是对样本的使用效率不高。因为策略梯度法每次采样完轨迹更新完策略后，这条轨迹就被抛弃了。
```

那么让我们从技术上总结一下策略梯度法面临的挑战：

- 重大的策略变化破坏了训练
- 无法轻松地映射策略和参数空间之间的变化，
- 不适当的学习率会导致梯度消失或爆炸
- 对样本的使用效率很差

```ad-note
事后看来，我们希望限制策略的变更，更好的是，任何变更都应该保证奖励的提升。我们需要一种更好、更精准的优化方法来制定更好的策略。
```

为了理解 TRPO，最好先讨论三个关键概念。

## 最小化最大化MM算法（Minorize-Maximization MM algorithm）

我们能保证任何策略更新都能提升预期奖励吗？这听起来似乎有些牵强，但理论上是可行的。**MM 算法** 通过最大化局部近似预期奖励的下界函数（下图中的蓝线） **来迭代** 实现这一点。

![[3.6.excalidraw|800x100]]

让我们更详细地讨论一下。我们从一个初始策略猜测开始。我们找到一个下限 $M$ ，它在当前猜测下局部近似于预期奖励 $\eta$ 。我们找到 $M$ 的最优点并将其用作下一个猜测。我们再次近似下限并重复迭代。最终，我们的猜测将收敛到最优策略。为了实现这一点， $M$ 应该比 $\eta$ 更容易优化。$M$ 可以是一个二次方程

$$
ax^2+bx+c
$$

矢量形式如下

$$
g \cdot (\theta-\theta_{old})-\frac{\beta}{2}(\theta-\theta_{old})^TF(\theta-\theta_{old})
$$

它是一个凸函数，而凸函数的优化已经研究的非常充分了。

**为什么 MM 算法会收敛到最优策略？** 如果 $M$ 是下限，它永远不会越过红线 $\eta$ 。但假设新策略的预期奖励 $\eta$ 更低，那么蓝线必然会越过 $\eta$ （如下图右侧所示），这与它是下限的说法相矛盾。

![[3.7.excalidraw|600x100]]

由于我们的策略有限，随着我们不断迭代，它将引导我们得到局部或全局最优策略。

```ad-note
通过优化局部近似 $\eta$ 的下限函数，它 **可以保证** 每次策略的改进，并最终引导我们找到最优策略。
```

## 置信域（信任区域）

优化方法主要有两种：线性搜索和置信域。梯度下降是一种线性搜索。我们首先确定下降方向，然后朝该方向迈出一步。

![[3.8.excalidraw|600x100]]

在置信域中，我们确定想要探索的最大步长，然后在该置信域内找到最优点。我们以初始最大步长 $\delta$ 作为置信域的半径（黄色圆圈）。

$$
\begin{split}
\max_{s\in\mathbb{R}^n}m_k(s) \\
s.t. \quad\Vert{s}\Vert\le\delta
\end{split}
$$

$m$ 是对原始目标函数 $f$ 的近似值。我们现在的目标是在半径 $\delta$ 内找到 $m$ 的最优点。我们不断重复这个过程，直到达到峰值。

![[3.9.excalidraw|600x100]]

为了更好地控制学习速度，我们可以根据曲面的曲率在运行时扩大或缩小 $\delta$ 。在传统的置信域方法中，由于我们用 $m$ 来近似目标函数 $f$ ，因此如果 $m$ 在最佳点处不能很好地近似 $f$ ，则一种可能性是缩小置信域。相反，如果近似值很好，我们就扩大它。但在强化学习中计算 $f$ 可能并不简单。或者，如果新策略和当前策略的差异变大（反之亦然），我们也可以缩小该区域。例如，为了不过度自信，如果策略变化太大，我们可以缩小置信域。

## 重要性采样

### 动机

重要性采样在抽样推理和强化学习中起着关键作用。在强化学习中，重要性采样利用先前从旧策略 $\pi'$ 中收集的样本来估计策略 $\pi$ 的价值函数。简单来说，计算采取某个动作的总奖励消耗的算力非常的大。但是，如果新动作与旧动作相对接近，重要性采样允许我们基于旧计算结果计算新的奖励。

具体来说，使用强化学习中的蒙特卡洛方法，每当我们更新策略 $\theta$ 时，我们都需要收集一个全新的轨迹来计算预期奖励。



一条轨迹可能包含数百步，单次更新效率极低。使用重要性采样，我们只需重复使用旧样本即可重新计算总奖励。然而，当当前策略与旧策略偏差过大时，准确率就会下降。因此，我们需要定期重新同步两个策略。

**重要性采样** 计算 $f(x)$ 的期望，其中 $x$ 具有数据分布 $p$ 。

$$
\mathbb{E}_{x\sim p}[f(x)]
$$

在重要性采样中，我们提供了不从 $p$ 中采样 $f(x)$ 值的选项。相反，我们从 $q$ 中采样数据，并使用 $p$ 和 $q$ 之间的概率比来重新校准结果。

$$
\mathbb{E}_{x\sim q}\left\lbrack{\frac{f(x)p(x)}{q(x)}}\right\rbrack
$$

在策略梯度法中，我们使用当前策略来计算策略梯度。


![[3.10.excalidraw|600x100]]

所以，每当策略改变时，我们都会收集新的样本。旧的样本无法重复使用。因此，策略梯度法的样本效率很差。 ==通过重要性采样，我们的目标可以重写，并且我们可以使用旧策略中的样本来计算策略梯度。==



![[3.11.excalidraw|600x100]]

但有一点需要注意，使用 $q$ 进行估计，

$$
\mathbb{E}_{x\sim q}\left[{\frac{f(x)p(x)}{q(x)}}\right]
$$

方差为

$$
\frac{1}{N}\left({\color{red}{\underset{x\sim P}{\mathbb{E}}\left[{\frac{P(x)}{Q(x)}f(x)^2}\right]}}-\underset{x\sim P}{\mathbb{E}}[f(x)]^2\right)
$$

如果 $P(x)/Q(x)$ 比率很高，估计的方差可能会爆炸式增长。所以，如果两个策略彼此差异很大，方差（也就是误差）可能会非常高。因此，我们不能长期使用旧样本。我们仍然需要使用当前策略相当频繁地重新采样轨迹（比如每 4 次迭代一次）。

### 使用重要性采样的目标函数

让我们详细讨论如何在策略梯度法中应用重要性采样概念。策略梯度方法中的方程如下：

$$
g = \nabla_\theta J(\pi_\theta) = \underset{\tau\sim\pi_\theta}{\mathbb{E}}\left\lbrack{\sum_{t=0}^\infty\gamma^t\nabla_\theta\log\pi_\theta(a_t|s_t)A^{\pi_\theta}(s_t,a_t)}\right\rbrack
$$

$$
\theta_{k+1} = \theta_k + \alpha g
$$

根据上面的梯度表达式，我们可以反向推导（积分）出目标函数。为了简单起见，我们将折扣因子 $\gamma$ 设置为 1 。

$$
L^{PG}(\theta) = \hat{\mathbb{E}_t}\left[{\log\pi_\theta(a_t|s_t)\hat{A_t}}\right]
$$

