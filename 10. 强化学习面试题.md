
## 1. **on-policy** 和 **off-policy** 有什么区别？

```ad-note
on-policy: 在策略，同策略，在线策略
off-policy: 离策略，异策略，离线策略
```

**on-policy**：智能体和环境进行实时交互，实时获得反馈并更新策略。

小明边下棋边学习下棋就是同策略学习。

所以原始策略梯度法就是同策略学习。

**off-policy**：智能体与环境无实时交互，通过事先收集的离线数据集学习。

小明通过看别人下棋来学习下棋。就是异策略学习。

PPO：使用旧的策略采样的数据来学习新的策略，但由于旧的策略和新的策略偏差不大，所以 PPO 算是同策略学习。

DPO：使用了偏好数据集，是不是强化学习都有争议。

GRPO：针对 PPO 的改进，所以是同策略学习。

## 2. PPO算法中使用GAE的好处以及参数 $\gamma$ 和 $\lambda$ 的作用是什么？

```ad-note
GAE: Generalized Advantage Estimation, 广义优势估计
```

1. 改进对动作的优势的估计，提高训练稳定性和收敛速度。
2. 减小训练中的方差，并更准确地估计动作的优势。

$\gamma$ ：折扣因子，一个重要的强化学习超参数，通常用于衡量未来奖励的重要性，其控制了在计算奖励时对未来奖励的折扣程度。当 $\gamma$ 接近 $1$ 时，智能体更关注未来的奖励，即长期奖励，而当 $\gamma$ 接近 $0$ 时，智能体更关注即时奖励，即短期奖励。在PPO中， $\gamma$ 被用来计算GAE，即GAE的折扣因子。通常，合理的 $\gamma$ 值可以帮助平衡长期和短期奖励，使策略学习更加稳定。

$\lambda$ ：GAE中的另一个重要参数，用于平衡多个时间步上的奖励估计。$\lambda$ 介于 $0$ 和 $1$ 之间，它决定了在计算GAE时考虑多个时间步的奖励。具体来说，它平衡了在单个时间步上估计的TD（时序差分）误差和多个时间步的估计之间的权衡。选择合适的 $\lambda$ 值可以帮助平衡方差和偏差，以获得更准确的优势估计。

在PPO中，使用GAE结合 $\gamma$ 和 $\lambda$ 的好处

1. 可以更准确地估计每个时间步上的优劣行为，改进策略梯度的估计。
2. 有助于提高PPO算法的稳定性和训练性能，使其能够更好地处理复杂的强化学习问题。

## 3. 有哪些PPO算法的调参经验？

直接采用默认的参数基本就可以。例如裁剪的范围 $\epsilon$ 可以根据原始论文设置为 $\epsilon=0.2$ 。然后就是折扣因子和 $\lambda$ 也可以调整一下。

PPO 最麻烦的是训练一个好的 **奖励模型** 。后面的算法 DPO 和 GRPO 都是试图在避免训练奖励模型。

训练奖励模型的挑战：

1. 奖励的目标是什么？是数学推理充分，还是回答温柔无害，还是......
2. 高质量的数据如何收集？大量的金钱。
3. 奖励模型的参数量要多大？奖励模型如果尺寸太大，每次给奖励（前向传播，推理）都要耗费很大的算力。尺寸太小，给的奖励不准确。
4. 奖励模型 $R_\theta$ 的损失函数：$\mathcal{L}(\theta)=-\sum_{(A,B)}\log\sigma(R_\theta(A)-R_\theta(B))$ 。如果人类希望输出回答 A 的概率高于输出回答 B 的概率，那么需要最小化损失函数。

## 4. 现阶段LLM的对齐阶段分为sft和rlhf阶段，我们可以跳过sft阶段直接进行rlhf么？

当然可以。GRPO 就可以做到这一点。让模型自动产生思维链。然后训练出了 DeepSeek-R1-Zero 。但有 SFT 数据集不用也太浪费了，所以一般都是在 sft 微调后的模型上做 rlhf 。

## 5. DPO的第0步loss是多少？

DPO 的 reward 是根据策略模型和参考模型的概率比确定的，在初始的时候，`chosen_reward = rejected_reward = 0` ，所以 `loss` 是 `-logsigmoid(0)` 。

## 6. DPO训练时，为什么chosen和rejected的reward一起下降的猜想？

如果数据没有标注好，那么可能出现下面的数据标注，偏好为

A > B > C > A

```
{
	"chosen": A,
	"rejected": B,
}

{
	"chosen": B,
	"rejected": C,
}

{
	"chosen": C,
	"rejected": A,
}
```

这样的数据偏好，直接把dpo整懵了。不知道该提高输出哪个回答的概率了。

## 7. sft 模型出现各种复读问题如何解决？

DPO 处理这种问题，不需要多少偏好数据集就能解决掉这种问题。

## 8. 如何看待各种ppo rlhf的平替算法dpo/kto/rrhf/slic/orpo/samug/remax等算法号称性能等能超过ppo？

我们注意到 Reward Model 有个很有意思的点，当使用大模型来搭建 reward model 时，我们获得的可能不仅仅是人类标注中蕴含的那部分知识，同时还激发了模型在预训练过程中学到的一部分能力来做判别，这是我认为 PPO 和 DPO 一个比较重要的区别，甚至我们还可以对 reward model 做一些提示词工程。举一个具体例子，可能我们给的所有标注都是没有长度偏好的，但是 reward model 在预训练过程中就理解到详细的回答可能是更专业，更受人喜欢的，这样的知识就会通过 PPO 传导给 policy 模型，而 DPO 没有这样的效果。

## 9. 如何处理reward model中的噪声数据？

实践中可以通过让多个 reward model 进行投票，把一致性低的拿出来检查或者丢掉；也可以寄希望于闭源的大模型，比如 GPT4，替代人完成清洗工作。

## 10. 在PPO过程中，reward model的效果上会有什么问题？

reward model 的数据本身是有限标注的，那么在 PPO 训练过程中，模型产生的新样本可能是分布外的（OOD问题，out of distribution），那么 reward model 的准确率可能会降低；更极端的情况下，policy 模型可能找到一些 hacking 解，虽然可能毫无意义，但是获得了很高的 reward；这里可能可以用多个奖励模型投票的方式来增强鲁棒性。

## 11. 现有的 reward model 泛化能力怎么样？我们需要用大 reward model 吗？

首先原始的 InstructGPT 是用 7B 模型指导 175B 模型对齐，但是后续大部分人用的 reward model 至少不小于 policy 模型。我想到几个理由：

1. 我曾经调研过大模型的涌现，小模型的指令跟随能力是比较差的，那么对于复杂问题，小模型连问题都无法理解，应该是很难评判回答质量的。
2. 大模型的知识储量大，判断回答中的幻觉会更准确。3. 大模型通常具有更好的 **分布外** （OOD） 检测能力。

## 12. DPO训练可能会出现什么问题？

梯度爆炸或消失：由于 DPO 更直接地优化策略目标函数，可能导致策略更新过快或过剧，从而导致梯度爆炸或消失的问题。

收敛性问题：DPO 没有像 PPO 那样的机制来限制策略更新，因此可能在训练过程中出现不稳定或策略崩溃的情况。

探索和利用之间的平衡问题: 由于 DPO 直接最小化目标函数，可能会倾向于过早地进行利用，导致探索不足，从而无法找到全局最优解。

## 13. 讲一下DPO和PPO，DPO和PPO有什么区别？

- PPO (Proximal Policy Optimization): PPO 是一种强化学习算法，采用了策略优化方法。它的目标是通过限制策略更新的幅度来避免策略剧烈变化，减小策略崩溃的风险。具体做法是通过剪裁损失函数，确保策略变化在一个较小的范围内，从而提高训练的稳定性。PPO 的核心是引入了一种近端目标函数，利用优势函数更新策略，兼顾了策略的探索和收敛。
- DPO (Direct Preference Optimization): DPO 是一种最近提出的算法，旨在简化传统强化学习中的策略优化问题。它的主要思想是通过直接最小化目标函数来优化策略，而不是像 PPO 一样通过对数比率和剪裁损失函数来进行策略更新。DPO 采用了更直接的优化方式，简化了策略更新的过程。

区别

- 策略更新: PPO 通过限制策略变化幅度（例如剪裁）来实现稳定训练，而 DPO 更倾向于直接优化目标函数。
- 稳定性和效率: PPO 通常能够保持较高的稳定性，但训练效率可能较低；DPO 则更高效，但可能在一定程度上牺牲了训练的稳定性。

## 14. PPO的缺点是什么？

在训练 PPO 的过程中，需要 4 个模型同时加载到 GPU 中，策略模型（要微调的大模型），冻结的参考模型，价值函数模型（value head，线性层），以及奖励模型。需要很大的算力。

## 15. DPO的偏好数据集长什么样子

```
{
	"prompt": "....",
	"chosen": "....",
	"rejected": "....",
}
```

三元组。

## 16. DPO的损失函数是什么？

参见教程，需要背会。

## 17. 在什么情况下，DPO 在数学上等价于 PPO ？

DPO 的一个关键特性是，当 Bradley-Terry 模型完美拟合我们的偏好数据，并且 RLHF 学习到最优奖励函数时，RHLF 和 DPO 的全局优化器是相同的。

这是一个重要的等价结果；然而在实践中：

1. Bradley-Terry 模型通常不能完美地拟合偏好数据。
2. RLHF 学习到的奖励函数不会是最优的奖励函数。
3. 在高度非凸的损失景观（例如 LLM）上进行梯度下降找不到全局优化器。

```ad-note
例如，偏好循环会导致 Bradley-Terry 模型无法完美拟合数据。Bradley-Terry 模型假设偏好具有传递性。例如，如果 $A \succ B$ 和 $B \succ C$ 成立，则模型预期结果为 $A \succ B \succ C$ 。但如果结果为 $C \succ A$ ，则存在循环，传递性被破坏。
```

## 18. DPO的变种算法有哪些？主要解决了DPO的什么问题？

- **IPO** ：由于 Bradley-Terry 模型的目标是最大化人类喜欢的回答和人类讨厌的回答之间的奖励差值，但其中可能忽略了偏好对中的噪声，所以无限扩大奖励差值就不太合适了，也就是过拟合了偏好对数据。所以需要加正则项，将奖励差值进行裁剪。
- **DPOP** ：假设人类喜欢的回答和人类不喜欢的回答太相似了，差不了几个字。那么LLM很难区分这两者。如何一直想要增大这两种相似回答的奖励差值，模型效果会崩塌。也就是LLM输出正例和负例的概率同时往下降。所以 DPOP 加了一个正则项，来惩罚正例的奖励往下掉的偏好数据对，来使得正例的概率往上升。

## 19. 如何微调出带有思维链的LLM推理模型？

- PPO：奖励模型奖励那些带思维链的输出。
- DPO：正例数据带有思维链，负例数据不带思维链。
- GRPO：使用基于规则的奖励函数微调LLM。

在不微调的情况下，使用推理时间扩展，来让LLM产生带有思维链的输出。本质上是Prompt Engineering。

## 20. 带有思维链的推理模型优缺点？

| 擅长                  | 不擅长               |
| ------------------- | ----------------- |
| 演绎或者归纳推理（如解谜题、数学证明） | 快速且粗略的回答（更多的推理时间） |
| 思维链式推理（分解多步骤问题）     | 基于知识的任务（容易产生幻觉）   |
| 复杂决策任务              | 简单任务（过度思考）        |
| 对新问题有更好的泛化能力        |                   |
