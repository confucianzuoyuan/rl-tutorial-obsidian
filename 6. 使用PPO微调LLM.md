![[6.1.excalidraw]]

这是PPO微调大语言模型的一个简化流程图：

1. **初始化**：准备好预训练语言模型和价值函数模型。
2. **采样生成**：语言模型根据当前策略生成文本（动作）。
3. **环境反馈**：根据生成文本获得奖励（比如人类评分或自动评分）。
4. **计算优势**：结合奖励和价值函数估计，计算优势函数。
5. **策略更新**：用PPO算法更新语言模型策略，确保更新稳定。
6. **价值更新**：训练价值函数模型拟合真实的回报。
7. **循环迭代**：不断重复采样和更新，直到满足终止条件。

## 项目思路

为了快速看到PPO的效果，我们要训练一些 **小的** 大模型。

在本示例中，我们的智能体或者说策略模型就是大语言模型。这个大语言模型完成的任务非常的简单，就是计算一个数的2倍是多少。具体的训练数据如下：

**带前缀的训练数据**

```
S小写:5487x2=一〇九七四E
S小写:4474x2=八九四八EP
S数字:3320x2=6640EP
S大写:3164x2=陆叁贰捌EP
```

**不带前缀的训练数据**

```
S__:5487x2=一〇九七四E
S__:4474x2=八九四八EP
S__:3320x2=6640EP
S__:3164x2=陆叁贰捌EP
```

`S` 表示 `bos` 标记。`E` 表示 `eos` 标记。`P` 是 `padding` 填充标记。

我们使用 **不带前缀的训练数据** 训练大语言模型。采用标准的大语言模型的训练方式。也就是“预测下一个token”的训练方式。

训练出来的预训练模型，可以输出给定数值的2倍数值。只是这个2倍数值可能是数字输出，也可能是小写汉字输出，也可能是大写汉字输出。如下

```
S__:3320x2 -----> 6640EP
```

乘号用字母 `x` 表示。

而经过 PPO 微调过的大语言模型，可以根据前缀输出特定格式的2倍数值。例如

```
S数字:3320x2 ----> 6640E
```

而不再胡乱输出格式。

## 训练数据生成器

```python
import random

class DataGenerator:
	def __init__(self):
		self.vocab = {
			'mark': list('PSE'),
			'number': list('0123456789'),
			'chinese_lower': list('〇一二三四五六七八九'),
			'chinese_upper': list('零壹贰叁肆伍陆柒捌玖'),
			'other': list('数字大写小:=_x'),
		}

		# 解码器
		self.decoder = [j for i in self.vocab.values() for j in i]
		# 编码器
		self.encoder = {j: i for i, j in enumerate(self.decoder)}
		# 标签
		self.label = {
			'number': 0,
			'chinese_lower': 1,
			'chinese_upper': 2,
		}
		# 前缀
		self.prefix = ['数字', '小写', '大写']

	def decode(self, x):
		'''将 `input_ids` 转换成字符串可读数据'''
		return ''.join([self.decoder[i] for i in x])

	def get_data(self, prefix: bool):
		'''获取一条数据，prefix为是否带前缀'''
		# 问题和答案对
		question = random.randint(1000, 9999)
		answer = question * 2

		# 将问题和答案转换成字符列表
		question = list(str(question))
		answer = list(str(answer))

		# 随机选择一个标签label
		label = random.choice(list(self.label.keys()))

		# 根据标签类型，将答案换成其它字符集
		answer = [self.vocab[label][int(i)] for i in answer]

		# 将label转换成数字
		label = self.label[label]

		# 组合问题和答案
		if prefix:
			prefix = list(self.prefix[label])
		else:
			prefix = list('__')

		data = prefix + [':'] + question + ['x', '2', '='] + answer
		# 编码成 `input_ids`
		data = [self.encoder[i] for i in data]
		data = [self.encoder['S']] + data + [self.encoder['E']]

		return label, data

	def get_batch_data(self, prefix):
		'''获取一批数据，64条'''
		batch = [self.get_data(prefix) for _ in range(64)]

		batch_labels = [i[0] for i in batch]
		batch_datas = [i[1] for i in batch]

		return batch_labels, *self.batch_pad(batch_datas)

	def batch_pad(self, batch_datas):
		'''对一批数据的每一条添加padding `P`'''
		# 找出一批数据中最长的一条的长度
		max_length = max([len(data) for data in batch_datas])

		input_ids = []
		attention_mask = []
		for data in batch_datas:
			attention_mask.append([1] * len(data) + [0] * (max_length - len(data)))
			input_ids.append(data + [self.encoder['P']] * (max_length - len(data)))

		return input_ids, attention_mask
```

测试一下

```python
g = DataGenerator()
r = [g.decode(data) for data in g.get_batch_data(prefix=True)[1]][:10]
print(r)

r = [g.decode(data) for data in g.get_batch_data(prefix=False)[1]][:10]
print(r)
```

输出结果

```
['S数字:6033x2=12066E', 'S数字:1455x2=2910EP', 'S大写:3702x2=柒肆零肆EP', 'S大写:5479x2=壹零玖伍捌E', 'S大写:3013x2=陆零贰陆EP', 'S小写:4768x2=九五三六EP', 'S小写:7857x2=一五七一四E', 'S小写:5574x2=一一一四八E', 'S小写:4563x2=九一二六EP', 'S大写:8214x2=壹陆肆贰捌E']
['S__:4298x2=八五九六EP', 'S__:2221x2=肆肆肆贰EP', 'S__:4543x2=9086EP', 'S__:9930x2=19860E', 'S__:2499x2=肆玖玖捌EP', 'S__:3863x2=柒柒贰陆EP', 'S__:6604x2=13208E', 'S__:5906x2=壹壹捌壹贰E', 'S__:6994x2=13988E', 'S__:6923x2=13846E']
```

## 预训练模型

这里我们采用 GPT2 大模型架构。

```python
import torch
from transformers import GPT2Config, GPT2Model

device = 'cuda' if torch.cuda.is_available() else 'cpu'

g = DataGenerator()

class LLM_Model(torch.nn.Module):
	def __init__(self):
		super().__init__()
		self.config = GPT2Config(bos_token_id=g.encoder['S'],
								 eos_token_id=g.encoder['E'],
                                 n_embd=64,
                                 n_head=4,
                                 n_layer=4,
                                 n_positions=128,
                                 vocab_size=len(g.decoder))

		self.feature = GPT2Model(self.config)
		self.fc_out = torch.nn.Linear(64, self.config.vocab_size, bias=False)

		self.to(device) # 将模型挪到 `device`
		self.train() # 设置为训练模式

	def forward(self, input_ids, attention_mask):
		out = self.feature(input_ids=input_ids,
                           attention_mask=attention_mask).last_hidden_state

		return self.fc_out(out)
```

我们有了训练数据，有了模型结构，就可以开始训练大模型了。

```python
# 初始化一个模型
llm_model = LLM_Model()
# 设置使用AdamW优化器
optimizer = torch.optim.AdamW(llm_model.parameters(), lr=1e-4)
# 评估标准，忽略掉 `P` 填充标记
criterion = torch.nn.CrossEntropyLoss(ignore_index=g.encoder['P'])

# 训练 15000 轮
for epoch in range(15000):
	_, input_ids, attention_mask = g.get_batch_data(prefix=False)
	input_ids = torch.LongTensor(input_ids).to(device)
	attention_mask = torch.LongTensor(attention_mask).to(device)

	logits = llm_model(input_ids=input_ids, attention_mask=attention_mask)
	loss = criterion(logits[:, :-1].flatten(end_dim=1),
                     input_ids[:, 1:].flatten())
	loss.backward()
	optimizer.step()
	optimizer.zero_grad()

	if epoch % 100 == 0:
		print(f"Epoch {epoch}, Loss: {loss.item():.6f}")

llm_model.to('cpu')
torch.save(llm_model, 'llm.model')
```

我们测试一下训练好的模型。

```python
import torch
from transformers import GPT2LMHeadModel

device = 'cuda' if torch.cuda.is_available() else 'cpu'
g = DataGenerator()

llm_model = LLM_Model()
llm_model = torch.load('llm.model', weights_only=False)
llm_model.eval() # 设置为评估模式

response_generator = GPT2LMHeadModel(llm_model.config)
response_generator.transformer = llm_model.feature
response_generator.lm_head = llm_model.fc_out
response_generator.to(device)

# 测试生成示例
_, input_ids, attention_mask = g.get_batch_data(prefix=True)
print(g.decode(input_ids[0][:11]))
print(g.decode(input_ids[1][:11]))
input_ids = torch.LongTensor(input_ids).to(device)

res = response_generator.generate(
	input_ids=input_ids[:2, :11], # 取前两个样本，前11个token作为上下文
	min_length=-1,
	top_k=0,
	top_p=1.0,
	do_sample=True,
	pad_token_id=g.encoder['P'],
	max_new_tokens=25,
	eos_token_id=g.encoder['E']
)

for seq in res:
	print(g.decode(seq.tolist()))
```

输出结果：

```
S小写:2018x2=
S大写:3531x2=
S小写:2018x2=肆零叁陆E
S大写:3531x2=七〇六二E
```

## 奖励模型

接下来我们训练一个奖励模型。用来给上面的大模型的输出打分。如果大模型输出的2倍数值的字符集和标签相同，则给出高评分。否则给出低评分。

我们使用 `Bert` 分类模型来做奖励模型。

模型结构如下

```python
import torch
from transformers import BertConfig, BertModel

g = DataGenerator()

device = 'cuda' if torch.cuda.is_available() else 'cpu'

class Reward_Model(torch.nn.Module):
	def __init__(self):
		super().__init__()
		self.config = BertConfig(hidden_size=64,
                                 intermediate_size=64,
                                 max_position_embeddings=128,
                                 num_attention_heads=4,
                                 num_hidden_layers=4,
                                 vocab_size=len(g.decoder))

		self.feature = BertModel(self.config)
		self.fc_out = torch.nn.Sequential(torch.nn.Dropout(p=0.1),
                                          torch.nn.Linear(64, 4))

		self.to(device)
		self.train()

	def forward(self, input_ids, attention_mask):
		out = self.feature(input_ids=input_ids,
                           attention_mask=attention_mask).pooler_output

		return self.fc_out(out)
```

接下来训练这个分类模型

```python
reward_model = Reward_Model()

optimizer = torch.optim.AdamW(params=reward_model.parameters(), lr=1e-4)
criterion = torch.nn.CrossEntropyLoss()

for epoch in range(500):
	label, input_ids, attention_mask = g.get_batch_data(prefix=False)
	label = torch.LongTensor(label).to(device)
	input_ids = torch.LongTensor(input_ids).to(device)
	attention_mask = torch.LongTensor(attention_mask).to(device)

	logits = reward_model(input_ids=input_ids, attention_mask=attention_mask)

	loss = criterion(logits, label)
	loss.backward()
	optimizer.step()
	optimizer.zero_grad()

	if epoch % 100 == 0:
		logits = logits.argmax(1)
		acc = (logits == label).sum().item() / len(label)
		print(epoch, acc)

		for i in range(2):
			print(g.decode(input_ids[i].tolist()), logits[i].item())

reward_model.to('cpu')
torch.save(reward_model, 'reward.model')
```

