## 2.1 原始策略梯度法

通过神经网络等方法将策略模型化，并使用梯度来优化策略的方法叫作 **策略梯度法（policy gradient method）** 。

研究者们提出了各种基于策略梯度法的算法。本章首先介绍最简单的策略梯度法。然后，在改进这个简单的策略梯度法的过程中，我们推导出了被称为 **REINFORCE** 的算法。接下来，在进一步改进 REINFORCE 的过程中， 我们又推导出了 **带基线** 的 REINFORCE 方法和 Actor-Critic（演员-评论家） 方法。

随机性策略用数学式可以表示为 $\pi(a|s)$ 。$\pi(a|s)$ 是在状态 $s$ 下采取动作 $a$ 的概率。这里采用神经网络对策略进行建模。此时用符号 $\theta$ 来汇总表示神经网络的所有权重参数（ $\theta$ 是将所有参数的元素排成一列的向量）。另外，可以将基于神经网络的策略表示为 $\pi_{\theta}(a|s)$ 。

![[2.1.excalidraw]]

还是倒立摆环境，每当倒立摆环境处于某个状态 $s$ 时，我们就会使用神经网络 $\pi_\theta$ 来决定要采取什么动作。

那么，问题是：这个神经网络怎么训练？

> [!NOTE]
> 想要训练一个神经网络，需要有 **输入-输出** 对。例如，`mnist` 手写数字数据集，要训练一个可以识别手写数字的卷积神经网络，需要构建一个网络结构，然后提供输入（图片）以及输出（分类标签）。
> 当然，还得有 **损失函数** ，例如交叉熵损失函数或者均方误差损失函数，等等。

![[mnist_nn.png]]

但是对于倒立摆环境，想要训练推车的策略神经网络，输入是什么，输出是什么？以及损失函数又是什么？

首先回顾一下第一章的知识。

首先明确问题的设定。这里考虑的是回合制任务，并基于策略 $\pi_{\theta}$ 选择动作的情况。在这种情况下，假定得到了以下由“状态、动作、奖励”构成的时间序列数据。

$$
\tau = (S_0, A_0, R_0, S_1, A_1, R_1, \cdots, S_{T+1})
$$

这个 $\tau$ 也叫作轨迹（trajectory）。此时可以使用折扣因子 $\gamma$ 对回报（Return，收益）作如下定义。

$$
G(\tau) = R_0 + \gamma R_1 + \gamma^2 R_2 + \cdots + \gamma^T R_T
$$

为了表明回报可以由 $\tau$ 计算出来，上面的式子将其表示为了 $G(\tau)$ 。此时，目标函数 $J(\theta)$ 可以表示为以下式子。

$$
J(\theta) = \mathbb{E}_{\tau\sim\pi_{\theta}}[G(\tau)]
$$

回报 $G(\tau)$ 是随机变动的，所以它的期望值是目标函数。上式中期望值 $E$ 的下标为 $\tau\sim\pi_{\theta}$ ，这个下标表示 $\tau$ 是基于 $\pi_{\theta}$ 生成的。

![[2.2.excalidraw]]



> [!NOTE]
> 我们的目标是让 $J(\theta)$ 最大。

在神经网络的训练中，我们的目的是让 **损失函数** 最小。而在策略梯度法中，我们的目的是让 **目标函数** 最大。所以都可以使用梯度法。让损失函数最小，使用梯度下降法。让目标函数最大，使用梯度上升法。

所以，我们还是得求解 $J(\theta)$ 的梯度。要求导的参数是 $\theta$ 。

确定了目标函数后，下一步是计算它的梯度。这里将参数 $\theta$ 的梯度表示为 $\nabla_{\theta}$ 。我们的目标是求 $\nabla_{\theta}J(\theta)$ 。$\nabla$ 是梯度符号。

$$
\begin{split}
\nabla_{\theta}J(\theta) &= \nabla_{\theta}\mathbb{E}_{\tau\sim\pi_{\theta}}[G(\tau)] \\
&= \mathbb{E}_{\tau\sim\pi_{\theta}}[\sum_{t=0}^TG(\tau)\nabla_{\theta}\log\pi_{\theta}(A_t|S_t)]
\end{split}
$$

上面的式子中值得注意的是，$\nabla_{\theta}$ 在 $\mathbb{E}$ 中（梯度计算的部分是 $\nabla_{\theta}\log\pi_{\theta}(A_t|S_t)$ ）。后面会对此做详细介绍。求出 $\nabla_{\theta}J(\theta)$ 之后，接下来更新神经网络的参数。最优化方法多种多样，下面的式子表示的是一种简单的方法。

$$
\theta \leftarrow \theta + \alpha\nabla_{\theta}J(\theta)
$$

上面的式子朝着梯度的方向更新参数 $\theta$ 。更新的值与 $\alpha$ 相关。这里的 $\alpha$ 表示学习率。这是属于梯度上升法的算法。

如式所示，$\nabla_{\theta}J(\theta)$ 表示期望值。接下来我们来计算期望值。这里，我们令策略 $\pi_{\theta}$ 的智能体实际采取动作，得到 $n$ 个轨迹 $\tau$ 。此时，通过对每个 $\tau$ 计算式子的期望值内部的式子（$\sum_{t=0}^TG(\tau)\nabla_{\theta}\log\pi_{\theta}(A_t|S_t)$），并求出其平均值，从而近似得到 $\nabla_{\theta}J(\theta)$ 。数学式如下所示。

$$
\begin{split}
\text{采样}: \tau^{(i)}\sim\pi_{\theta}\;\;\;\;(i=1,2,\cdots,n) \\
x^{(i)} = \sum_{t=0}^TG(\tau^{(i)})\nabla_{\theta}\log\pi_{\theta}(A_t^{(i)}|S_t^{(i)}) \\
\nabla_{\theta}J(\theta) \approx \frac{x^{(1)}+x^{(2)}+\cdots+x^{(n)}}{n}
\end{split}
$$

上面式子中的 $\tau^{(i)}$ ，表示在第 $i$ 回合得到的轨迹，$A_t^{(i)}$ 表示在第 $i$ 回合的时刻 $t$ 的动作，$S_t^{(i)}$ 表示在第 $i$ 回合的时刻 $t$ 的状态。

另外，再思考一下蒙特卡洛方法的样本数为 $1$ ，即上式中 $n=1$ 的情况。在这种情况下，数学式可以简化为如下形式。

$$
\begin{split}
\text{采样}: \tau\sim\pi_{\theta} \\
\nabla_{\theta}J(\theta) \approx \sum_{t=0}^TG(\tau)\nabla_{\theta}\log\pi_{\theta}(A_t|S_t) \\
\end{split}
$$

为了简单起见，本章将使用以上面的式子为对象的策略梯度法。上面的式子的计算就是对所有时刻（$t=0\sim T$）求 $\nabla_{\theta}\log\pi_{\theta}(A_t|S_t)$ ，然后将各梯度乘以作为权重的回报 $G(\tau)$ ，最后求它们的和。这个计算过程如下图所示。

![[2.3.excalidraw|800x100]]

接下来，我们用代码来实现一下策略梯度法。

```python
import numpy as np
import gym
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.distributions import Categorical

class Policy(nn.Module):
	def __init__(self, action_size):
		super().__init__()
		self.l1 = nn.Linear(4, 128)
		self.l2 = nn.Linear(128, action_size)

	def forward(self, x):
		x = F.relu(self.l1(x))
		x = F.softmax(self.l2(x), dim=1)
		return x
```

这里实现的神经网络模型由两层全连接层构成。最终输出的元素数是动作的数量（ `action_size` ）。由于这个最终输出是 `Softmax` 函数的输出，因此可以得到每个动作的概率。

这个网络结构就是我们要训练的策略神经网络 $\pi_\theta$ 。其中 $\theta$ 就是神经网络的参数。

我们还记得，策略神经网络的输入是环境的状态，输出是要采取的动作的概率分布。所谓动作的概率分布就是：向左推的概率是多少，向右推的概率是多少。

而我们的网络结构输入的张量的维度是 4 。为什么呢？因为倒立摆环境的状态有 4 个维度。输出是 `action_size` 个维度。也就是动作的数量个维度。

> [!NOTE]
> 如果将具有 $n$ 个元素的向量输入到 `Softmax` 函数中，那么输出的同样是具有 $n$ 个元素的向量。此时，第 $i$ 个输出 $y_i$ 的式子如下所示。
> $$y_i = \frac{\mathrm{e}^{x_i}}{\sum_{k=1}^n\mathrm{e}^{x_k}}$$
> 这里的 $\mathrm{e}$ 是自然常数(值为 $2.718 28...$ 的无限小数)。`Softmax` 函数的输出值全部为 $0$ 以上 $1$ 以下的实数，它们的合计值为 $1$（$\sum_{i=1}^ny_i=1$）。因此，`Softmax` 函数的输出可以作为概率使用。

下面是 `Agent` 类的代码。首先显示初始化和 `get_action` 方法。

```python
class Agent:
	def __init__(self):
		self.gamma = 0.98 # 折扣因子
		self.lr = 0.0002 # 学习率
		self.action_size = 2 # 动作空间大小，共两个动作：向左推和向右推

		self.memory = []
		self.pi = Policy(self.action_size) # 初始化策略神经网络
		# 使用 Adam 优化器
		self.optimizer = optim.Adam(self.pi.parameters(), lr=self.lr)

	def get_action(self, state):
		# 将状态转换成torch.tensor类型
		state = torch.tensor(state[np.newaxis, :])
		# 将状态输入策略神经网络，输出为两个动作的概率分布
		probs = self.pi(state)
		# 取出概率分布
		probs = probs[0]
		# 下面两行根据动作的分布采样出一个动作
		m = Categorical(probs)
		action = m.sample().item()

		# 返回动作和动作的概率
		return action, probs[action]
```

`get_action` 方法决定了在 `state` 状态下采取的动作。为此，可以通过 `self.pi(state)` 进行神经网络的前向传播，得到概率分布 `probs` 。然后，基于该概率分布，进行一次动作的采样。该方法还返回了所选动作的概率（上面代码中的 `probs[action]` ）。

下面来试用一下 `get_action` 方法。代码如下所示。

```python
env = gym.make('CartPole-v0')
state = env.reset()
agent = Agent()

action, prob = agent.get_action(state)
print('action: ', action)
print('prob: ', prob)

G = 100.0 # 虚拟权重
J = G * prob.log()
print('J: ', J)

# 求梯度
J.backward()
```

上面的代码取出了初始状态下的动作及其概率。另外，它还显示了使用虚拟的权重来计算由下式表示的梯度的代码（这是从式子取出的 $t = 0$ 的相关项的式子）。

$$
G(\tau)\nabla_{\theta}\log\pi_{\theta}(A_0|S_0)
$$

作为参考，下面对照列出了上面的代码中出现的变量与相应的数学式。

- `prob` ： $\pi_{\theta}(A_0|S_0)$
- `G` ： $G(\tau)$
- `J` ： $G(\tau)\log\pi_{\theta}(A_0|S_0)$

求出 `J` 之后，通过 `J.backward()` 求 $G(\tau)\nabla_{\theta}\log\pi_{\theta}(A_0|S_0)$ 。下面是 `Agent` 类剩下的代码。

```python
class Agent:
	...

	def add(self, reward, prob):
		data = (reward, prob)
		self.memory.append(data)

	def update(self):
		G, loss = 0, 0
		for reward, prob in reversed(self.memory):
			G = reward + self.gamma * G
		for reward, prob in self.memory:
			loss += - prob.log() * G

		self.optimizer.zero_grad()
		loss.backward()
		self.optimizer.step()
		self.memory = [] # 重置内存
```

`add` 方法是智能体每次采取行动并获得奖励时被调用的方法。该方法将奖励（`reward`）和智能体采取的行动的概率（`prob`）存储在内存（`self.memory`）中。`update` 方法是智能体到达目标时被调用的方法。该方法首先会求回报 `G` 。回报可以通过反向计算获得的奖励的做法来高效的计算。然后该方法会计算损失函数。具体做法是对每个时刻求 `-prob.log()` ，再将其乘以作为权重的 `G` ，之后求它们的和。剩下的就是一直以来的神经网络的训练代码。

在训练神经网络时，通常要设置损失函数。对于这个例子，我们可以将目标函数 $J(\theta)$ 乘以 $-1$ 所得到的 $-J(\theta)$ 作为损失函数，此时可以通过梯度下降法的最优化方法（SGD、Adam等）更新参数。

最后在倒立摆环境中运行智能体。代码如下所示。

```python
env = gym.make('CartPole-v0')
agent = Agent()
reward_history = []

for episode in range(3000):
	# 重置倒立摆环境的状态
	state = env.reset()
	done = False
	total_reward = 0

	# 采样一条轨迹
	while not done:
		# 根据环境的当前状态决策要采取什么动作
		action, prob = agent.get_action(state)
		# 采取动作，环境会返回环境的下一个状态，奖励和是否结束游戏
		next_state, reward, done, _ = env.step(action)

		# 将奖励和动作的概率保存下来
		agent.add(reward, prob)
		# 环境转移到下一个状态
		state = next_state
		# 累积奖励
		total_reward += reward

	# 使用反向传播算法更新策略神经网络
	agent.update()
	reward_history.append(total_reward)
	
	if episode % 100 == 0:
		print("回合:{}, 总奖励:{:.1f}".format(episode, total_reward))

torch.save(agent.pi.state_dict(), 'policy_model.pth')
print("模型已保存")
```

首先，在 `while` 语句中，增加智能体获得的奖励（`reward`）和行动的概率（`prob`）。然后在离开 `while` 语句后（回合结束时），通过 `agent.update()` 更新策略。

运行此代码，随着回合的推进，获得的奖励也会增加。下图是结果的示意图。

绘制示意图的代码如下：

```python
import matplotlib.pyplot as plt
# 训练结束后绘制奖励变化图
plt.plot(reward_history)
plt.xlabel('回合')
plt.ylabel('总奖励')
plt.title('每回合奖励')
plt.grid(True)
plt.show()
```

通过观察绘制出来的图像，我们发现每回合的奖励随着训练震荡的很厉害，但是随着训练的进行，每回合获得的总奖励确实越来越多了。

我们可以测试一下训练的策略神经网络。看看倒立摆能不能坚持很长时间。

```python
def test_render(agent, env, episodes=5):
	for episode in range(episodes):
		state = env.reset()
		done = False
		total_reward = 0
		
		while not done:
			env.render()
			action, _ = agent.get_action(state)
			next_state, reward, done, _ = env.step(action)
			state = next_state
			total_reward += reward

		print(f"回合 {episode + 1}: 总奖励 = {total_reward}")
	env.close()

# 加载模型后测试
env = gym.make('CartPole-v0')
agent = Agent()
agent.pi.load_state_dict(torch.load('policy_model.pth'))
agent.pi.eval()
test_render(agent, env)
```

我们大概可以知道，随着回合的推进，奖励的总和会逐渐增加。但即使经历了 3000 个回合，依然没有达到这次任务的上限值 200，所以似乎还有改进的余地。下面让我们来改进一下这里推导的最简单的策略梯度法。这个改进算法就是著名的 **REINFORCE** 算法。

## 2.2 REINFORCE 算法

REINFORCE是对上一节的策略梯度法的改进算法。本节首先会基于数学式推导REINFORCE算法，然后会通过修改之前的部分代码的做法来实现 REINFORCE 。

> [!NOTE]
> REINFORCE 这个名字是"REward Increment = Nonnegative Factor x Offset Reinforcement x Characteristic Eligibility"（奖励增量=非负因子x偏移强化x特征资格）的首字母缩写。

先来复习一下第一节。最简单的梯度策略法是基于下面的公式实现的。

$$
\begin{split}
\nabla_{\theta}J(\theta) &= \nabla_{\theta}E_{\tau\sim\pi_{\theta}}[G(\tau)] \\
&= E_{\tau\sim\pi_{\theta}}[\sum_{t=0}^TG(\tau)\nabla_{\theta}\log\pi_{\theta}(A_t|S_t)]
\end{split}
$$

上面的式子中的 $G(\tau)$ 是目前为止获得的所有奖励的总和（准确地说是“带折扣因子”的奖励的总和）。这里要思考的问题是，无论在哪个时刻 $t$ ，式子中都是 $G(\tau)\nabla_{\theta}\log\pi_{\theta}(A_t|S_t)$ ，我们始终会使用固定不变的权重 $G(\tau)$ 来增加（或减少）采取行动 $A_t$ 的概率。

智能体行动的好坏是根据行动之后获得的奖励总和来评估的（回顾一下价值函数的定义）。反过来说，采取某个行动之前获得的奖励与该行动的好坏无关。如果要评估在某个时刻 $t$ 采取的行动 $A_t$ ，那么在此之前做了什么以及获得了多少奖励都无所谓。我们是根据采取行动 $A_t$ 之后的结果（在时刻 $t$ 以后获得的奖励的总和）来判断行动 $A_t$ 的好坏的。

上面的式子中行动 $A_t$ 的权重是 $G(\tau)$ 。这个权重 $G(\tau)$ 包括在时刻 $t$ 之前的奖励。也就是说，原本不相关的奖励作为噪声数据包含在内了。为了改进这一点（去除噪声数据），可以对权重 $G(\tau)$ 作如下修改。

$$
\nabla_{\theta}J(\theta)=E_{\tau\sim\pi_{\theta}}[\sum_{t=0}^TG_t\nabla_{\pi}\log\pi_{\theta}(A_t|S_t)] \\
G_t = R_t + \gamma R_{t+1} + \cdots + \gamma^{T-1}R_T
$$

如式所示，权重变成了 $G_t$ 。权重 $G_t$ 是在时刻 $t\sim T$ 获得的奖励的总和。因此，选择行动 $A_t$ 的概率将由不包含时刻 $t$ 之前的奖励的权重 $G_t$ 增强。 这就是改进第一节的策略梯度法的思路。基于上式的算法叫作 **REINFORCE** 。

> [!NOTE]
> 基于上式的REINFORCE算法优于最简单的策略梯度法（基于第一节的公式的算法）。通过无限增加的样本数，两个公式都会收敛到正确的 $\nabla_{\theta}J(\theta)$ （可以说是无偏差的）。但第一个公式的方差更大，因为公式中的权重包含了无关的数据（噪声）。

**REINFORCE** 的代码实现

由于REINFORCE的方差小，因此即使数据样本少，也能高精度地近似数据。下面我们来实现REINFORCE以验证其精度。REINFORCE的代码与上一节中的代码基本相同，不同之处只有 `Agent` 类的 `update` 方法。下面仅列出了不同部分的代码。

```python
class Agent:
	...

	def update(self):
		G, loss = 0, 0
		for reward, prob in reversed(self.memory):
			G = reward + self.gamma * G
			loss += - prob.log() * G

		self.optimizer.zero_grad()
		loss.backward()
		self.optimizer.step()
		self.memory = []
```

`self.memory` 是按先后顺序保存智能体获得的奖励(reward)和行动概率(prob)的列表。上面的代码按从后向前的顺序依次访问了 `self.memory` 的元素，并计算了每个时刻的 `G` 。

我们使用可视化代码将回合和奖励绘制出来。

从图中可以看出，随着回合的推进，奖励的总和会逐渐增加。与上一次的结果相比，不但训练稳定了，训练速度也提高了。

## 2.3 基线（baseline）

下面介绍一种叫作基线（baseline）的技术，该技术可以改进REINFORCE。让我们先通过一个简单的例子来了解一下基线的思路，然后再将基线应用于 REINFORCE 。

### 基线的思路

下面是一个简单的例子。假设现在有A、B、C 3人参加了考试，分别得了 90分、40分、50分，如图所示。

| 姓名  | 分数  |
| --- | --- |
|A|90|
|B|40|
|C|50|

我们对这个结果求方差。使用 `NumPy` 编写的代码如下所示。

```python
import numpy as np

x = np.array([90, 40, 50])
print(np.var(x))
```

输出结果

```
466.6666666666667
```

如上所述，考试结果的方差为 `466.6666666666667` ，这是一个很大的值。 由于方差表示数据的离散程度，因此该结果表明考试结果的离散程度很大。我们要考虑的是如何减小方差。

这里使用 3 人之前的考试结果。假设我们获得了这些结果，如图所示。

|姓名|第1次开始|第2次考试|...|第10次考试|
|-|-|-|-|-|
|A|92|80|...|74|
|B|32|51|...|56|
|C|45|53|...|49|

有了图所示的之前的考试结果，我们就可以预测下一次考试的分数了。一种预测方法是对之前的分数取平均，然后将下一次考试结果作为与之前的平均分的差值进行预测。

对图的结果分别取平均，最终A为82分、B为46分、C为49分。接下来，将它们作为预测值，计算其与分析对象的考试结果的差值，如下图所示。

计算实际的结果与预测值之间的差值

![[2.4.excalidraw]]

下面计算图中的差值的方差。代码如下所示。

```python
x = np.array([90, 40, 50])

avg = np.array([82, 46, 49])
diff = x - avg # [8, -6, 1]

print(np.var(diff))
```

输出结果

```
32.666666666666664
```

如上所述，方差为 `32.666666666666664` 。与之前相比，方差大幅减小了。如本例所示，我们可以通过对某个结果减去预测值的方法来减小方差。预测值的精度越高，方差就越小。这就是基线这种方法的思路。下面将基线应用于REINFORCE。

### 带基线的策略梯度法

式(1)是REINFORCE的数学式。将基线应用于这个REINFORCE的数学式如式(2)所示。

$$
\begin{split}
\nabla_{\theta}J(\theta) &= E_{\tau\sim\pi_{\theta}}[\sum_{t=0}^T G_t\nabla_{\theta}\log\pi_{\theta}(A_t|S_t)] \quad\quad\quad\quad\quad\quad(1) \\
&= E_{\tau\sim\pi_{\theta}}[\sum_{t=0}^T (G_t-b(S_t))\nabla_{\theta}\log\pi_{\theta}(A_t|S_t)] \quad\quad(2)
\end{split}
$$

式(2)用 $G_t-b(S_t)$ 代替了 $G_t$ 。这里 $b(S_t)$ 可以是任何函数。也就是说，只要输入是 $S_t$ ， $b(S_t)$ 是什么函数都行。这个 $b(S_t)$ 就是基线。

式（2）中的 $b(S_t)$ 可以是任何函数。例如，在状态 $S_t$ 下，可以考虑使用之前获得的奖励的平均值作为 $b(S_t)$ 。实践中经常使用的是价值函数，数学式为 $b(S_t)=V_{\pi_{\theta}}(S_t)$ 。如果能够使用基线减小方差，那么就可以进行样本效率更高的训练。另外，将价值函数作为基线使用时，我们是不知道真正的价值函数 $v_{\pi_{\theta}}(S_t)$ 的。这种情况下还需要训练价值函数。

最后，我们再通过直观介绍补充说明一下为什么使用基线更好。这里以倒立摆为例，思考图示的状态。

![[1.jpg]]

==杆子失去平衡的状态==

上图表示游戏结束之前的杆子失去平衡的状态。在这种状态下，无论采取什么样的行动，在几个时间步之后游戏都将结束。

设图的状态为 $s$ ，在此状态下采取的行动为 $a$ 。假定从状态 $s$ 开始几个时间步（比如3个时间步）后游戏一定会结束。在这种情况下，状态 $s$ 的回报 $G$ 为 $3$ （这里设折现率 $\gamma$ 为 $1$ 。如果使用的是没有基线的REINFORCE，那么状态 $s$ 下的行动 $a$ 就会因为权重 $3$ 而被增强（状态 $s$ 下选择行动 $a$ 的概率会变高）。但无论采取什么样的行动，3 个时间步之后游戏一定会结束，所以可以说这种提高行动 $a$ 被选择的概率的工作是无意义的。

此时就要用到基线了。这里使用价值函数作为基线，假设我们已经知道图中的例子中的 $V_{\pi_{\theta}}(S_t)=3$ 。此时的权重为 $G_t-V_{\pi_{\theta}}$ 所以是 0 。由于权重是 0 ，因此无论选择什么行动，采取那个行动的概率都不会变大，也不会变小。像这样使用基线，有望减少无谓的训练。

## 2.4 Actor-Critic（演员-评论家）

如果在上节介绍的带基线的REINFORCE中使用价值函数作为基线，那么就可以将其视为基于价值且基于策略的方法。本节将进一步改进带基线的REINFORCE，推导一个叫作Actor-Critic的算法。Actor-Critic也是基于价值且基于策略的方法。

### Actor-Critic 的推导

首先从复习带基线的REINFORCE开始。带基线的REINFORCE的目标函数的梯度的数学式如下所示。

$$
\nabla_{\theta}J(\theta)=E_{\tau\sim\pi_{\theta}}[\sum_{t=0}^T(G_t-b(S_t))\nabla_{\theta}\log\pi_{\theta}(A_t|S_t)]
$$

式中的 $G_t$ 表示回报，$b(S_t)$ 表示基线。可以使用任何函数作为基线。这里我们使用基于神经网络建模的价值函数作为基线。因此，我们要用到以下这些新的记号。

- $\omega$ ：表示价值函数的神经网络的所有权重参数。
- $V_{\omega}(S_t)$ ：将价值函数模型化的神经网络。

此时目标函数的梯度的数学式如下所示

$$
\nabla_{\theta}J(\theta)=E_{\tau\sim\pi_{\theta}}[\sum_{t=0}^T(G_t-V_{\omega}(S_t))\nabla_{\theta}\log\pi_{\theta}(A_t|S_t)]
$$

式中存在一个问题，即只要没抵达目标，就无法确定回报 $G_t$ 的值。 也就是说，在抵达目标之前，无法更新策略和价值函数。如果这是基于蒙特卡洛方法的算法，那么二者无法更新就是它的缺点。

> [!NOTE]
> 我们之前的实现用的都是 **蒙特卡洛采样的方法** 。也就是采样出一条完整的轨迹。然后更新策略神经网络 $\pi_\theta$ 。

消除这个缺点的方法是 **时序-差分方法（TD方法）** 。使用TD方法训练价值函数时，使用1个时间步（或 $n$ 个时间步）后的结果就能进行更新，如图所示。

![[2.5.excalidraw]]

如图所示，在训练价值函数 $V_{\omega}(S_t)$ 时，蒙特卡洛方法使用的是回报 $G_t$ ，而TD方法使用的是 $R_t+\gamma V_{\omega}(S_{t+1})$ 。

> [!NOTE]
> 使用神经网络对价值函数建模时，我们以接近 $R_t+\gamma V_{\omega}(S_{t+1})$ 为目标训练 $V_{\omega}(S_t)$ 的值。具体来说就是将 $V_{\omega}(S_t)$ 和 $R_t+\gamma V_{\omega}(S_{t+1})$ 的均方差作为损失函数，通过梯度下降法更新神经网络的权重。

下面将基于蒙特卡洛方法的公式切换为TD方法，其中，代替 $G_t$ 的是 $R_t+\gamma V_{\omega}(S_{t+1})$ 。此时得到的式子如下所示。

$$
\nabla_{\theta}J(\theta)=E_{\tau\sim\pi_{\theta}}[\sum_{t=0}^T(R_t+\gamma V_{\omega}(S_{t+1})-V_{\omega}(S_t))\nabla_{\theta}\log\pi_{\theta}(A_t|S_t)]
$$

基于上面的公式的算法就是Actor-Critic。策略 $\pi_{\theta}$ 和价值函数 $V_{\omega}$ 是神经网络，我们要同时训练这两个神经网络。具体来说，对于策略 $\pi_{\theta}$ 要基于上面的公式进行训练；而对于价值函数 $V_{\omega}$ ，则通过TD方法，以接近 $R_t+\gamma V_{\omega}(S_{t+1})$ 为目标训练 $V_{\omega}(S_t)$ 的值。

> [!NOTE]
> Actor-Critic中的Actor是 **演员（采取动作的人）** 的意思，也就是采取动作的人，相当于策略 $\pi_{\theta}$ 。而Critic是 **评论家** 的意思，相当于价值函数 $V_{\omega}$ 。因此，Actor-Critic的意思是“使用 $V_{\omega}$ ，来评论基于策略 $\pi_{\theta}$ 采取的动作的好坏”。

### Actic-Critic 的代码实现

下面实现Actor-Critic。策略和价值函数这两个神经网络的代码如下所示。

```python
import numpy as np
import gym
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.distributions import Categorical

class PolicyNet(nn.Module):
	def __init__(self, action_size):
		super().__init__()
		self.l1 = nn.Linear(4, 128)
		self.l2 = nn.Linear(128, action_size)

	def forward(self, x):
		x = F.relu(self.l1(x))
		x = F.softmax(self.l2(x), dim=1)
		return x

class ValueNet(nn.Module):
	def __init__(self):
		super().__init__()
		self.l1 = nn.Linear(4, 128)
		self.l2 = nn.Linear(128, 1)

	def forward(self, x):
		x = F.relu(self.l1(x))
		x = self.l2(x)
		return x
```

上面的代码将策略的网络实现为了 `PolicyNet` 类，将价值函数的网络实现为了 `ValueNet` 类。策略的最终输出是 `Softmax` 函数的输出，所以输出的是概率。接下来是 `Agent` 类的代码。

```python
class Agent:
	def __init__(self):
		self.gamma = 0.98
		self.lr_pi = 0.0002
		self.lr_v = 0.0005
		self.action_size = 2

		self.pi = PolicyNet(self.action_size)
		self.v = ValueNet()

		self.optimizer_pi = optim.Adam(self.pi.parameters(), lr=self.lr_pi)
		self.optimizer_v = optim.Adam(self.v.parameters(), lr=self.lr_v)

	def get_action(self, state):
		state = torch.tensor(state[np.newaxis, :])
		probs = self.pi(state)
		probs = probs[0]
		m = Categorical(probs)
		action = m.sample().item()
		return action, probs[action]

	def update(self, state, action_prob, reward, next_state, done):
		state = torch.tensor(state[np.newaxis, :])
		next_state = torch.tensor(next_state[np.newaxis, :])

		target = reward + self.gamma * self.v(next_state) * (1 - done)
		target.detach()
		v = self.v(state)
		loss_fn = nn.MSELoss()
		loss_v = loss_fn(v, target)

		delta = target - v
		loss_pi = -torch.log(action_prob) * delta.item()

		self.optimizer_v.zero_grad()
		self.optimizer_pi.zero_grad()
		loss_v.backward()
		loss_pi.backward()
		self.optimizer_v.step()
		self.optimizer_pi.step()
```

`get_action` 方法可以基于策略取出动作。需要注意的是，由于输入到神经网络中的数据将作为小批量进行处理，因此在处理其中一个数据（状态）时需要小批量的轴。另外，`get_action` 方法返回了两个值，即选择的动作及其概率。选择动作的概率将在稍后的损失函数计算中使用。

`update` 方法可以训练价值函数和策略。在代码 ① 处为价值函数(`self.v`)计算损失。为此要计算TD目标（target），求出当前状态下其与价值函数（v）的均方差。然后，在代码 ② 处为策略（`self.pi`）计算损失。根据式9.6,需要将其乘以-1的值作为损失。剩下的就是一直以来的神经网络的训练代码。

运行智能体的代码与之前相同，不再赘述。得到的结果如下图所示。

## 2.5 策略梯度法相关证明

### 1. 策略梯度法的推导

当 $J(\theta)=\mathbb{E}_{\tau\sim\pi_\theta}\left\lbrack{G(\tau)}\right\rbrack$ 时，其梯度如下面的式子所示。

$$
\nabla_\theta{J_\theta}=\mathbb{E}_{\tau\sim\pi_\theta}\left\lbrack{\sum_{t=0}^TG(\tau)\nabla_\theta\log\pi_\theta(A_t|S_t)}\right\rbrack
$$

下面对上面的式子进行证明。

$$
\begin{split}
\nabla_\theta{J(\theta)} &= \nabla_\theta\mathbb{E}_{\tau\sim\pi_\theta}\left\lbrack{G(\tau)}\right\rbrack \\
&= \nabla_\theta\sum_\tau{\Pr(\tau|\theta)G(\tau)} \quad\text{（展开期望值）} \\
&= \sum_\tau\nabla_\theta(\Pr(\tau|\theta)G(\tau)) \quad\text{（将}\pi_\theta\text{移动到}\sum\text{中）} \\
&= \sum_\tau\left\lbrace{G(\tau)\nabla_\theta\Pr(\tau|\theta)+\Pr(\tau|\theta)\nabla_\theta{G(\tau)}}\right\rbrace \quad\text{（积的微分）} \\
&= \sum_\tau{G(\tau)\nabla_\theta\Pr(\tau|\theta)} \quad\text{（}\nabla_\theta{G(\tau)}\text{永远为0）} \\
&= \sum_\tau G(\tau)\Pr(\tau|\theta)\frac{\nabla_\theta\Pr(\tau|\theta)}{\Pr(\tau|\theta)} \quad\text{（乘以}{\frac{\Pr(\tau|\theta)}{\Pr(\tau|\theta)}}\text{）} \\
&= \sum_\tau G(\tau)\Pr(\tau|\theta)\nabla_\theta\log\Pr(\tau|\theta) \quad\text{（}\log\text{梯度的技巧）} \\
&= \mathbb{E}_{\tau\sim\pi_\theta}\left\lbrack{G(\tau)\nabla_\theta\log\Pr(\tau|\theta)}\right\rbrack
\end{split}\tag{1}
$$

这里对 “log梯度的技巧” 进行说明。这个技巧利用了以下等式。

$$
\nabla_\theta\log\Pr(\tau|\theta)=\frac{\nabla_\theta\Pr(\tau|\theta)}{\Pr(\tau|\theta)}
$$

> [!NOTE]
> $$\log(f(x))=\frac{f'(x)}{f(x)}$$

根据上面的式子，我们就知道

$$
\nabla_\theta\Pr(\tau|\theta)=\Pr(\tau|\theta)\nabla_\theta\log\Pr(\tau|\theta)
$$

这就是著名的 **log梯度的技巧** 。是机器学习领域常用的数学式的变形形式。

接下来，我们将利用以下等式进一步展开$(1)$。

$$
\begin{split}
\Pr(\tau|\theta) &= p(S_0)\pi_\theta(A_0|S_0)p(S_1|S_0,A_0)\cdots\pi_\theta(A_T|S_T)p(S_{T+1}|S_T,A_T) \\
&= p(S_0)\prod_{t=0}^T\pi_\theta(A_t|S_t)p(S_{t+1}|S_t,A_t)
\end{split}
$$

这里，$p(S_0)$ 表示初始状态 $S_0$ 的概率。上面的式子表明，得到轨迹 $\tau$ 的概率可以用初始状态的概率、策略以及下一个状态的迁移概率的乘积来表示。另外，我们可以用下面的式子来表示 $\log\Pr(\tau|\theta)$ 。

$$
\log\Pr(\tau|\theta)=\log{p(S_0)} + \sum_{t=0}^T\log{p(S_{t+1}|S_t,A_t)} + \sum_{t=0}^T\log\pi_\theta(A_t|S_t)
$$

由于 $\log xy = \log x + \log y$ ，所以可以像上面的式子那样表示为和的形式。基于上面的式子，可以将 $\nabla_\theta{\log\Pr(\tau|\theta)}$ 展开为如下形式。

$$
\begin{split}
\nabla_\theta\log\Pr(\tau|\theta) &= \nabla_\theta\left\lbrace\log{p(S_0)} + \sum_{t=0}^T\log{p(S_{t+1}|S_t,A_t)} + \sum_{t=0}^T\log\pi_\theta(A_t|S_t)\right\rbrace \\
&= \nabla_\theta\sum_{t=0}^T\log\pi_\theta(A_t|S_t)
\end{split}
$$

$\nabla_\theta$ 是对 $\theta$ 的梯度。与 $\theta$ 无关的元素的梯度 $\nabla_\theta\log p(S_0)$ 和 $\nabla_\theta\sum_{t=0}^T\log{p(S_{t+1}|S_t,A_t)}$ 为 0 。因此，从上面的式子可以得到下列式子。

$$
\begin{split}
\nabla_\theta{J(\theta)}&=\mathbb{E}_{\tau\sim\pi_\theta}\left\lbrack{G(\tau)\nabla_\theta\log\Pr(\tau|\theta)}\right\rbrack \\
&= \mathbb{E}_{\tau\sim\pi_\theta}\left\lbrack{\sum_{t=0}^TG(\tau)\nabla_\theta\log\pi_\theta(A_t|S_t)}\right\rbrack
\end{split}
$$

这样我们就完成了 $\nabla_\theta{J(\theta)}$ 的推导。

### 2. 基线的推导

$$
\begin{split}
\nabla_\theta J(\theta) &= \mathbb{E}_{\tau\sim\pi_\theta}\left\lbrack{\sum_{t=0}^TG_t\nabla_\theta\log\pi_\theta(A_t|S_t)}\right\rbrack \\
&= \mathbb{E}_{\tau\sim\pi_\theta}\left\lbrack{\sum_{t=0}^T(G_t-b(S_t))\nabla_\theta\log\pi_\theta(A_t|S_t)}\right\rbrack \\
\end{split}
$$

如上面的式子所示，我们可以使用 $G_t-b(S_t)$ 代替 $G_t$ 。$b(S_t)$ 是 **任何函数** ，我们称之为“基线”。下面进行上式的推导。

首先，证明以下式子成立。

$$
\mathbb{E}_{x\sim P_\theta}\left\lbrack{\nabla_\theta\log P_\theta(x)}\right\rbrack = 0 \tag{1}
$$

这里假设随机变量 $x$ 是基于概率分布 $P_\theta(x)$ 生成的。$P_\theta(x)$ 会根据参数 $\theta$ 改变概率分布的形状。此时有以下式子成立。

$$
\sum_xP_\theta(x)=1
$$

由于 $P_\theta(x)$ 是概率分布，因此所有 $x$ 的值的和为 1 。然后，求这个式子的梯度。

$$
\nabla_\theta\sum_xP_\theta(x)=\nabla_\theta 1 = 0
$$

接下来，使用 $\log$ 梯度的技巧将式子展开，过程如下所示。

$$
\begin{split}
0 &= \nabla_\theta\sum_xP_\theta(x) \\
&= \sum_x\nabla_\theta P_\theta(x) \\
&= \sum_xP_\theta(x)\nabla_\theta\log P_\theta(x) \\
&= \mathbb{E}_{x\sim P_\theta}\left\lbrack{\nabla_\theta\log P_\theta(x)}\right\rbrack
\end{split}
$$

证明(1)完毕。接下来将证明的式子用于我们的问题。具体来说，用 $A_t$ 代替(1)中的 $x$ ，然后使用 $\pi_\theta(\cdot|S_t)$ 代替 $P_\theta(\cdot)$ 。这样就可以得到以下式子。

$$
\mathbb{E}_{A_t\sim\pi_\theta}\left\lbrack{\nabla_\theta\log\pi_\theta(A_t|S_t)}\right\rbrack = 0
$$

上面的式子是对 $A_t$ 的期望值。因此，我们可以像下面的式子那样，将任何函数 $b(S_t)$ 放入期望值中。

$$
\mathbb{E}_{A_t\sim\pi_\theta}\left\lbrack{b(S_t)\nabla_\theta\log\pi_\theta(A_t|S_t)}\right\rbrack = 0 \tag{2}
$$

$b(S_t)$ 是以 $S_t$ 为参数的函数，即使 $A_t$ 发生变化，它的值也不会改变。由于式子(2)是对 $A_t$ 的期望值，因此即使在期望值中加入函数 $b(S_t)$ ，等式也成立。

> [!NOTE]
> 动作 $A_t$ 的变化会导致收益 $G_t$ 的变化，因此以下式子不成立。
> $$\mathbb{E}_{A_t\sim\pi_\theta}\left\lbrack{G_t\nabla_\theta\log\pi_\theta(A_t|S_t)}\right\rbrack = 0$$

式子(2)在整个 $t=0\sim T$ 的范围都成立，所以可以得到以下式子。

$$
\mathbb{E}_{A_t\sim\pi_\theta}\left\lbrack{\sum_{t=0}^Tb(S_t)\nabla_\theta\log\pi_\theta(A_t|S_t)}\right\rbrack = 0
$$

所以基线证明完毕。



