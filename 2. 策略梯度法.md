## 2.1 原始策略梯度法

在倒立摆游戏的环境中，我们唯一能控制的，就是推车所采用的 **策略** 。而环境和奖励函数我们是无法控制的。

![[2.9.excalidraw|600x100]]

现在通行的做法就是策略函数是一个神经网络。输入是环境的状态，输出是动作的概率分布。

![[2.10.excalidraw|600x100]]

上面这幅图就是一个例子。玩倒立摆游戏。策略函数是一个神经网络；输入是倒立摆环境的状态，一个浮点数数组；输出是我们可以执行的动作，有几个动作，输出层就有几个神经元。假设我们现在可以执行的动作有 2 个，输出层就有 2 个神经元，每个神经元对应一个可以采取的动作。输入一个东西后，策略神经网络会给每一个可以采取的动作一个分数。我们可以把这个分数当作概率，智能体根据概率的分布来决定它要采取的动作，比如 0.7 的概率向左推、0.3 的概率向右推。概率分布不同，推车采取的动作就会不一样。

如下图所示，首先，环境是一个函数，我们可以把倒立摆环境看成一个函数，虽然它不一定是神经网络，可能是基于规则的（rule-based）模型，但我们可以把它看作一个函数。

$$
\text{倒立摆环境}(S_t, A_t) \rightarrow (S_{t+1}, \text{奖励})
$$

倒立摆环境就是一个基于规则的函数，只要推车不倒下，就返回奖励 1 。

![[2.11.excalidraw|600x100]]

讲了这么多策略有关的东西，大家应该明白策略是什么东西了。

而通过神经网络等方法将策略模型化，并使用梯度来优化策略的方法叫作 **策略梯度法（policy gradient method）** 。

研究者们提出了各种基于策略梯度法的算法。本章首先介绍最简单的策略梯度法。然后，在改进这个简单的策略梯度法的过程中，我们推导出了被称为 **REINFORCE** 的算法。接下来，在进一步改进 REINFORCE 的过程中， 我们又推导出了 **带基线** 的 REINFORCE 方法和 Actor-Critic（演员-评论家） 方法。

随机性策略用数学式可以表示为 $\pi(a|s)$ 。$\pi(a|s)$ 是在状态 $s$ 下采取动作 $a$ 的概率。这里采用神经网络对策略进行建模。此时用符号 $\theta$ 来汇总表示神经网络的所有权重参数（ $\theta$ 是将所有参数的元素排成一列的向量）。另外，可以将基于神经网络的策略表示为 $\pi_{\theta}(a|s)$ 。

![[2.1.excalidraw]]

还是倒立摆环境，每当倒立摆环境处于某个状态 $s$ 时，我们就会使用神经网络 $\pi_\theta$ 来决定要采取什么动作。

那么，问题是：这个神经网络怎么训练？

> [!NOTE]
> 想要训练一个神经网络，需要有 **输入-输出** 对。例如，`mnist` 手写数字数据集，要训练一个可以识别手写数字的卷积神经网络，需要构建一个网络结构，然后提供输入（图片）以及输出（分类标签）。
> 当然，还得有 **损失函数** ，例如交叉熵损失函数或者均方误差损失函数，等等。

![[mnist_nn.png]]

但是对于倒立摆环境，想要训练推车的策略神经网络，输入是什么，输出是什么？以及损失函数又是什么？

首先回顾一下第一章的知识。

首先明确问题的设定。这里考虑的是回合制任务，并基于策略 $\pi_{\theta}$ 选择动作的情况。在这种情况下，假定得到了以下由“状态、动作、奖励”构成的时间序列数据。

$$
\tau = (S_0, A_0, R_0, S_1, A_1, R_1, \cdots, S_{T+1})
$$

这个 $\tau$ 也叫作轨迹（trajectory）。

而一条轨迹发生的概率是：

$$
\begin{aligned}
\Pr(\tau)
&=p\left(S_{0}\right) \pi_{\theta}\left(A_{0} | S_{0}\right) p\left(S_{1} | S_{0}, A_{0}\right) \pi_{\theta}\left(A_{1} | S_{1}\right) p\left(S_{2} | S_{1}, A_{1}\right) \cdots \pi_{\theta}\left(A_{T} | S_{T}\right) p\left(S_{T+1} | S_{T}, A_{T}\right) \\
&=p\left(S_{0}\right) \prod_{t=1}^{T} \pi_{\theta}\left(A_{t} | S_{t}\right) p\left(S_{t+1} | S_{t}, A_{t}\right)
\end{aligned}
$$

此时可以使用折扣因子 $\gamma$ 对回报（Return，收益）作如下定义。

$$
G(\tau) = R_0 + \gamma R_1 + \gamma^2 R_2 + \cdots + \gamma^T R_T
$$

![[2.14.excalidraw|600x100]]

为了表明回报可以由 $\tau$ 计算出来，上面的式子将其表示为了 $G(\tau)$ 。此时，目标函数 $J(\theta)$ 可以表示为以下式子。

$$
J(\theta) = \mathbb{E}_{\tau\sim\pi_{\theta}}[G(\tau)]
$$

回报 $G(\tau)$ 是随机变动的，所以它的期望值是目标函数。上式中期望值 $E$ 的下标为 $\tau\sim\pi_{\theta}$ ，这个下标表示 $\tau$ 是基于 $\pi_{\theta}$ 生成的。

![[2.2.excalidraw]]

> [!NOTE]
> 我们的目标是让 $J(\theta)$ 最大。

在神经网络的训练中，我们的目的是让 **损失函数** 最小。而在策略梯度法中，我们的目的是让 **目标函数** 最大。所以都可以使用梯度法。让损失函数最小，使用梯度下降法。让目标函数最大，使用梯度上升法。

所以，我们还是得求解 $J(\theta)$ 的梯度。需要求导的参数是 $\theta$ 。

确定了目标函数后，下一步是计算它的梯度。这里将参数 $\theta$ 的梯度表示为 $\nabla_{\theta}$ 。我们的目标是求 $\nabla_{\theta}J(\theta)$ 。$\nabla$ 是梯度符号。

$$
\boxed{
\begin{split}
\nabla_{\theta}J(\theta) &= \nabla_{\theta}\mathbb{E}_{\tau\sim\pi_{\theta}}[G(\tau)] \\
&= \mathbb{E}_{\tau\sim\pi_{\theta}}[\sum_{t=0}^TG(\tau)\nabla_{\theta}\log\pi_{\theta}(A_t|S_t)]
\end{split}
}
$$

![[2.12.excalidraw]]

上面的式子中值得注意的是，$\nabla_{\theta}$ 在 $\mathbb{E}$ 中（梯度计算的部分是 $\nabla_{\theta}\log\pi_{\theta}(A_t|S_t)$ ）。后面会对此做详细介绍。求出 $\nabla_{\theta}J(\theta)$ 之后，接下来更新神经网络的参数。最优化方法多种多样，下面的式子表示的是一种简单的方法。

$$
\boxed{
\theta \leftarrow \theta + \alpha\nabla_{\theta}J(\theta)
}
$$

![[2.17.excalidraw|1000]]

上面的式子朝着梯度的方向更新参数 $\theta$ 。更新的值与 $\alpha$ 相关。这里的 $\alpha$ 表示学习率。这是属于梯度上升法的算法。

只要沿着梯度上升的方向更新参数 $\theta$ ，那么目标函数 $J(\theta)$ 就会越来越大，也就是所有轨迹的期望就会越来越大，那么就是我们的策略越来越好了。

这里有一个问题，那就是真正的期望是无法准确求出来的。因为期望是所有的轨迹得到的奖励计算出来的。而轨迹有无数条。如果我们能够走满倒立摆的200步，那么不同轨迹的数量可能是 $2^{200}$ 。这是一个天文数字。所以我们希望能够求出 **近似** 期望值的数值。比如利用大数定理，多采样几条轨迹，那么就会比较接近期望值。

![[2.6.excalidraw]]

如式所示，$\nabla_{\theta}J(\theta)$ 表示期望值。接下来我们来计算期望值。这里，我们令策略 $\pi_{\theta}$ 的智能体实际采取动作，得到 $n$ 个轨迹 $\tau$ 。此时，通过对每个 $\tau$ 计算式子的期望值内部的式子（$\sum_{t=0}^TG(\tau)\nabla_{\theta}\log\pi_{\theta}(A_t|S_t)$），并求出其平均值，从而近似得到 $\nabla_{\theta}J(\theta)$ 。数学式如下所示。

$$
\begin{split}
\text{采样}: \tau^{(i)}\sim\pi_{\theta}\;\;\;\;(i=1,2,\cdots,n) \\
x^{(i)} = \sum_{t=0}^TG(\tau^{(i)})\nabla_{\theta}\log\pi_{\theta}(A_t^{(i)}|S_t^{(i)}) \\
\nabla_{\theta}J(\theta) \approx \frac{x^{(1)}+x^{(2)}+\cdots+x^{(n)}}{n}
\end{split}
$$

上面式子中的 $\tau^{(i)}$ ，表示在第 $i$ 回合得到的轨迹，$A_t^{(i)}$ 表示在第 $i$ 回合的时刻 $t$ 的动作，$S_t^{(i)}$ 表示在第 $i$ 回合的时刻 $t$ 的状态。

![[2.15.excalidraw|600x100]]

![[2.7.excalidraw]]


另外，再思考一下蒙特卡洛方法的样本数为 $1$ ，即上式中 $n=1$ 的情况。

![[2.8.excalidraw]]


在这种情况下，数学式可以简化为如下形式。

$$
\begin{split}
\text{采样}: \tau\sim\pi_{\theta} \\
\nabla_{\theta}J(\theta) \approx \sum_{t=0}^TG(\tau)\nabla_{\theta}\log\pi_{\theta}(A_t|S_t) \\
\end{split}
$$

![[2.16.excalidraw|600x100]]

为了简单起见，本章将使用以上面的式子为对象的策略梯度法。上面的式子的计算就是对所有时刻（$t=0\sim T$）求 $\nabla_{\theta}\log\pi_{\theta}(A_t|S_t)$ ，然后将各梯度乘以作为权重的回报 $G(\tau)$ ，最后求它们的和。这个计算过程如下图所示。

![[2.3.excalidraw|800x100]]

```ad-warning
title: 深入讨论

策略梯度法的本质是，我们使用 **当前策略** $\pi_\theta$ 采样一条轨迹 $\tau$ ，如果采样的轨迹的回报 $G(\tau)$ 很高，那么说明我们在某个状态 $S_t$ 采取的动作 $A_t$ 比较好，导致了总的奖励也就是回报比较高。那么通过梯度上升法，更新 $\theta$ 之后，新的策略 $\pi_{\theta_\text{new}}$ 会提升 **在 $S_t$ 时执行 $A_t$ 的概率** 。例如 $A_t$ 是 “向左推车” ，那么在 $S_t$ 状态下策略采取 “向左推车” 的概率就会提升，而同时策略采取 “向右推车” 的概率就会下降。

而如果我们采样的轨迹获得的回报很小，那么计算出来的梯度 $\nabla_\theta J(\theta)$ 可能也比较小，那么在 $S_t$ 采取 $A_t$ 的概率就会提升不大。

由于只采样一条轨迹，那么这条轨迹可能回报非常差，或者回报非常好，好得不得了。总之使得 $\alpha\nabla_\theta{J(\theta)}$ 非常的大，导致步子太大，直接扯着蛋。把策略给训崩了。

![[2.13.excalidraw]]

那么采样的轨迹回报非常的高，按说应该很好啊，为什么策略也会崩溃？

好比一个人，突然间中了1000万，那么如此大的奖励会直接彻底改变这个人的策略，他的策略彻底变成了靠运气生活。这就是策略崩溃的例子。
```

接下来，我们用代码来实现一下策略梯度法。

我们讲一些实现细节。我们可以把强化学习想成一个分类问题，这个分类问题就是输入倒立摆的状态，输出某个类。在解决分类问题时，我们要收集一些训练数据，数据中要有输入与输出的对。在实现的时候，我们把倒立摆的状态当作分类器的输入，就像在解决图像分类的问题，只是现在的类不是图像里面的东西，而是看到倒立摆的状态我们要采取什么样的动作，每一个动作就是一个类。比如第一个类是向左，第二个类是向右。

在解决分类问题时，我们要有输入和正确的输出，要有训练数据。但在强化学习中，我们通过采样来获得训练数据。假设在采样的过程中，在某个状态下，我们采样到要采取动作 $A$， 那么就把动作 $A$ 当作标准答案（ground truth）。比如，我们在某个状态下，采样到要向左。因为是采样，所以向左这个动作不一定概率最高。假设我们采样到向左，在训练的时候，让智能体调整网络的参数， 如果看到某个状态，我们就向左。在一般的分类问题里面，我们在实现分类的时候，目标函数都会写成最小化交叉熵（cross entropy），最小化交叉熵就是最大化对数似然（log likelihood）。

```python
import numpy as np
import gym
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.distributions import Categorical

class Policy(nn.Module):
	def __init__(self, action_size):
		super().__init__()
		self.l1 = nn.Linear(4, 128)
		self.l2 = nn.Linear(128, action_size)

	def forward(self, x):
		x = F.relu(self.l1(x))
		x = F.softmax(self.l2(x), dim=1)
		return x
```

这里实现的神经网络模型由两层全连接层构成。最终输出的元素数是动作的数量（ `action_size` ）。由于这个最终输出是 `Softmax` 函数的输出，因此可以得到每个动作的概率。

这个网络结构就是我们要训练的策略神经网络 $\pi_\theta$ 。其中 $\theta$ 就是神经网络的参数。

我们还记得，策略神经网络的输入是环境的状态，输出是要采取的动作的概率分布。所谓动作的概率分布就是：向左推的概率是多少，向右推的概率是多少。

而我们的网络结构输入的张量的维度是 4 。为什么呢？因为倒立摆环境的状态有 4 个维度。输出是 `action_size` 个维度。也就是动作的数量个维度。

> [!NOTE]
> 如果将具有 $n$ 个元素的向量输入到 `Softmax` 函数中，那么输出的同样是具有 $n$ 个元素的向量。此时，第 $i$ 个输出 $y_i$ 的式子如下所示。
> $$y_i = \frac{\mathrm{e}^{x_i}}{\sum_{k=1}^n\mathrm{e}^{x_k}}$$
> 这里的 $\mathrm{e}$ 是自然常数(值为 $2.718 28...$ 的无限小数)。`Softmax` 函数的输出值全部为 $0$ 以上 $1$ 以下的实数，它们的合计值为 $1$（$\sum_{i=1}^ny_i=1$）。因此，`Softmax` 函数的输出可以作为概率使用。

下面是 `Agent` 类的代码。首先显示初始化和 `get_action` 方法。

```python
class Agent:
	def __init__(self):
		self.gamma = 0.98 # 折扣因子
		self.lr = 0.0002 # 学习率
		self.action_size = 2 # 动作空间大小，共两个动作：向左推和向右推

		self.memory = []
		self.pi = Policy(self.action_size) # 初始化策略神经网络
		# 使用 Adam 优化器
		self.optimizer = optim.Adam(self.pi.parameters(), lr=self.lr)

	def get_action(self, state):
		# 将状态转换成torch.tensor类型
		state = torch.tensor(state[np.newaxis, :])
		# 将状态输入策略神经网络，输出为两个动作的概率分布
		probs = self.pi(state)
		# 取出概率分布
		probs = probs[0]
		# 下面两行根据动作的分布采样出一个动作
		m = Categorical(probs)
		action = m.sample().item()

		# 返回动作和动作的概率
		return action, probs[action]
```

`get_action` 方法决定了在 `state` 状态下采取的动作。为此，可以通过 `self.pi(state)` 进行神经网络的前向传播，得到概率分布 `probs` 。然后，基于该概率分布，进行一次动作的采样。该方法还返回了所选动作的概率（上面代码中的 `probs[action]` ）。

下面来试用一下 `get_action` 方法。代码如下所示。

```python
env = gym.make('CartPole-v0')
state = env.reset()
agent = Agent()

action, prob = agent.get_action(state)
print('action: ', action)
print('prob: ', prob)

G = 100.0 # 虚拟权重
J = G * prob.log()
print('J: ', J)

# 求梯度
J.backward()
```

上面的代码取出了初始状态下的动作及其概率。另外，它还显示了使用虚拟的权重来计算由下式表示的梯度的代码（这是从式子取出的 $t = 0$ 的相关项的式子）。

$$
G(\tau)\nabla_{\theta}\log\pi_{\theta}(A_0|S_0)
$$

作为参考，下面对照列出了上面的代码中出现的变量与相应的数学式。

- `prob` ： $\pi_{\theta}(A_0|S_0)$
- `G` ： $G(\tau)$
- `J` ： $G(\tau)\log\pi_{\theta}(A_0|S_0)$

求出 `J` 之后，通过 `J.backward()` 求 $G(\tau)\nabla_{\theta}\log\pi_{\theta}(A_0|S_0)$ 。下面是 `Agent` 类剩下的代码。

```python
class Agent:
	...

	def add(self, reward, prob):
		data = (reward, prob)
		self.memory.append(data)

	def update(self):
		G, loss = 0, 0
		for reward, prob in reversed(self.memory):
			G = reward + self.gamma * G
		for reward, prob in self.memory:
			loss += - prob.log() * G

		self.optimizer.zero_grad()
		loss.backward()
		self.optimizer.step()
		self.memory = [] # 重置内存
```

`add` 方法是智能体每次采取行动并获得奖励时被调用的方法。该方法将奖励（`reward`）和智能体采取的行动的概率（`prob`）存储在内存（`self.memory`）中。`update` 方法是智能体到达目标时被调用的方法。该方法首先会求回报 `G` 。回报可以通过反向计算获得的奖励的做法来高效的计算。然后该方法会计算损失函数。具体做法是对每个时刻求 `-prob.log()` ，再将其乘以作为权重的 `G` ，之后求它们的和。剩下的就是一直以来的神经网络的训练代码。

在训练神经网络时，通常要设置损失函数。对于这个例子，我们可以将目标函数 $J(\theta)$ 乘以 $-1$ 所得到的 $-J(\theta)$ 作为损失函数，此时可以通过梯度下降法的最优化方法（SGD、Adam等）更新参数。

最后在倒立摆环境中运行智能体。代码如下所示。

```python
env = gym.make('CartPole-v0')
agent = Agent()
reward_history = []

for episode in range(3000):
	# 重置倒立摆环境的状态
	state = env.reset()
	done = False
	total_reward = 0

	# 采样一条轨迹
	while not done:
		# 根据环境的当前状态决策要采取什么动作
		action, prob = agent.get_action(state)
		# 采取动作，环境会返回环境的下一个状态，奖励和是否结束游戏
		next_state, reward, done, _ = env.step(action)

		# 将奖励和动作的概率保存下来
		agent.add(reward, prob)
		# 环境转移到下一个状态
		state = next_state
		# 累积奖励
		total_reward += reward

	# 使用反向传播算法更新策略神经网络
	agent.update()
	reward_history.append(total_reward)
	
	if episode % 100 == 0:
		print("回合:{}, 总奖励:{:.1f}".format(episode, total_reward))

torch.save(agent.pi.state_dict(), 'policy_model.pth')
print("模型已保存")
```

首先，在 `while` 语句中，增加智能体获得的奖励（`reward`）和行动的概率（`prob`）。然后在离开 `while` 语句后（回合结束时），通过 `agent.update()` 更新策略。

运行此代码，随着回合的推进，获得的奖励也会增加。下图是结果的示意图。

绘制示意图的代码如下：

```python
import matplotlib.pyplot as plt
# 训练结束后绘制奖励变化图
plt.plot(reward_history)
plt.xlabel('回合')
plt.ylabel('总奖励')
plt.title('每回合奖励')
plt.grid(True)
plt.show()
```

通过观察绘制出来的图像，我们发现每回合的奖励随着训练震荡的很厉害，但是随着训练的进行，每回合获得的总奖励确实越来越多了。

我们可以测试一下训练的策略神经网络。看看倒立摆能不能坚持很长时间。

```python
def test_render(agent, env, episodes=5):
	for episode in range(episodes):
		state = env.reset()
		done = False
		total_reward = 0
		
		while not done:
			env.render()
			action, _ = agent.get_action(state)
			next_state, reward, done, _ = env.step(action)
			state = next_state
			total_reward += reward

		print(f"回合 {episode + 1}: 总奖励 = {total_reward}")
	env.close()

# 加载模型后测试
env = gym.make('CartPole-v0')
agent = Agent()
agent.pi.load_state_dict(torch.load('policy_model.pth'))
agent.pi.eval()
test_render(agent, env)
```

我们大概可以知道，随着回合的推进，奖励的总和会逐渐增加。但即使经历了 3000 个回合，依然没有达到这次任务的上限值 200，所以似乎还有改进的余地。下面让我们来改进一下这里推导的最简单的策略梯度法。这个改进算法就是著名的 **REINFORCE** 算法。

## 2.2 REINFORCE 算法

REINFORCE是对上一节的策略梯度法的改进算法。本节首先会基于数学式推导REINFORCE算法，然后会通过修改之前的部分代码的做法来实现 REINFORCE 。

> [!NOTE]
> REINFORCE 这个名字是"REward Increment = Nonnegative Factor x Offset Reinforcement x Characteristic Eligibility"（奖励增量=非负因子x偏移强化x特征资格）的首字母缩写。

先来复习一下第一节。最简单的梯度策略法是基于下面的公式实现的。

$$
\begin{split}
\nabla_{\theta}J(\theta) &= \nabla_{\theta}E_{\tau\sim\pi_{\theta}}[G(\tau)] \\
&= E_{\tau\sim\pi_{\theta}}[\sum_{t=0}^TG(\tau)\nabla_{\theta}\log\pi_{\theta}(A_t|S_t)]
\end{split}
$$

上面的式子中的 $G(\tau)$ 是目前为止获得的所有奖励的总和（准确地说是“带折扣因子”的奖励的总和）。这里要思考的问题是，无论在哪个时刻 $t$ ，式子中都是 $G(\tau)\nabla_{\theta}\log\pi_{\theta}(A_t|S_t)$ ，我们始终会使用固定不变的权重 $G(\tau)$ 来增加（或减少）采取行动 $A_t$ 的概率。

智能体行动的好坏是根据行动之后获得的奖励总和来评估的（回顾一下价值函数的定义）。反过来说，采取某个行动之前获得的奖励与该行动的好坏无关。如果要评估在某个时刻 $t$ 采取的行动 $A_t$ ，那么在此之前做了什么以及获得了多少奖励都无所谓。我们是根据采取行动 $A_t$ 之后的结果（在时刻 $t$ 以后获得的奖励的总和）来判断行动 $A_t$ 的好坏的。

上面的式子中行动 $A_t$ 的权重是 $G(\tau)$ 。这个权重 $G(\tau)$ 包括在时刻 $t$ 之前的奖励。也就是说，原本不相关的奖励作为噪声数据包含在内了。为了改进这一点（去除噪声数据），可以对权重 $G(\tau)$ 作如下修改。

$$
\nabla_{\theta}J(\theta)=E_{\tau\sim\pi_{\theta}}[\sum_{t=0}^TG_t\nabla_{\pi}\log\pi_{\theta}(A_t|S_t)] \\
G_t = R_t + \gamma R_{t+1} + \cdots + \gamma^{T-1}R_T
$$

如式所示，权重变成了 $G_t$ 。权重 $G_t$ 是在时刻 $t\sim T$ 获得的奖励的总和。因此，选择行动 $A_t$ 的概率将由不包含时刻 $t$ 之前的奖励的权重 $G_t$ 增强。 这就是改进第一节的策略梯度法的思路。基于上式的算法叫作 **REINFORCE** 。

> [!NOTE]
> 基于上式的REINFORCE算法优于最简单的策略梯度法（基于第一节的公式的算法）。通过无限增加的样本数，两个公式都会收敛到正确的 $\nabla_{\theta}J(\theta)$ （可以说是无偏差的）。但第一个公式的方差更大，因为公式中的权重包含了无关的数据（噪声）。

**REINFORCE** 的代码实现

由于REINFORCE的方差小，因此即使数据样本少，也能高精度地近似数据。下面我们来实现REINFORCE以验证其精度。REINFORCE的代码与上一节中的代码基本相同，不同之处只有 `Agent` 类的 `update` 方法。下面仅列出了不同部分的代码。

```python
class Agent:
	...

	def update(self):
		G, loss = 0, 0
		for reward, prob in reversed(self.memory):
			G = reward + self.gamma * G
			loss += - prob.log() * G

		self.optimizer.zero_grad()
		loss.backward()
		self.optimizer.step()
		self.memory = []
```

`self.memory` 是按先后顺序保存智能体获得的奖励(reward)和行动概率(prob)的列表。上面的代码按从后向前的顺序依次访问了 `self.memory` 的元素，并计算了每个时刻的 `G` 。

我们使用可视化代码将回合和奖励绘制出来。

从图中可以看出，随着回合的推进，奖励的总和会逐渐增加。与上一次的结果相比，不但训练稳定了，训练速度也提高了。

## 2.3 基线（baseline）

下面介绍一种叫作基线（baseline）的技术，该技术可以改进REINFORCE。让我们先通过一个简单的例子来了解一下基线的思路，然后再将基线应用于 REINFORCE 。

### 基线的思路

下面是一个简单的例子。假设现在有A、B、C 3人参加了考试，分别得了 90分、40分、50分，如图所示。

| 姓名  | 分数  |
| --- | --- |
|A|90|
|B|40|
|C|50|

我们对这个结果求方差。使用 `NumPy` 编写的代码如下所示。

```python
import numpy as np

x = np.array([90, 40, 50])
print(np.var(x))
```

输出结果

```
466.6666666666667
```

如上所述，考试结果的方差为 `466.6666666666667` ，这是一个很大的值。 由于方差表示数据的离散程度，因此该结果表明考试结果的离散程度很大。我们要考虑的是如何减小方差。

这里使用 3 人之前的考试结果。假设我们获得了这些结果，如图所示。

|姓名|第1次开始|第2次考试|...|第10次考试|
|-|-|-|-|-|
|A|92|80|...|74|
|B|32|51|...|56|
|C|45|53|...|49|

有了图所示的之前的考试结果，我们就可以预测下一次考试的分数了。一种预测方法是对之前的分数取平均，然后将下一次考试结果作为与之前的平均分的差值进行预测。

对图的结果分别取平均，最终A为82分、B为46分、C为49分。接下来，将它们作为预测值，计算其与分析对象的考试结果的差值，如下图所示。

计算实际的结果与预测值之间的差值

![[2.4.excalidraw]]

下面计算图中的差值的方差。代码如下所示。

```python
x = np.array([90, 40, 50])

avg = np.array([82, 46, 49])
diff = x - avg # [8, -6, 1]

print(np.var(diff))
```

输出结果

```
32.666666666666664
```

如上所述，方差为 `32.666666666666664` 。与之前相比，方差大幅减小了。如本例所示，我们可以通过对某个结果减去预测值的方法来减小方差。预测值的精度越高，方差就越小。这就是基线这种方法的思路。下面将基线应用于REINFORCE。

### 带基线的策略梯度法

式(1)是REINFORCE的数学式。将基线应用于这个REINFORCE的数学式如式(2)所示。

$$
\begin{split}
\nabla_{\theta}J(\theta) &= E_{\tau\sim\pi_{\theta}}[\sum_{t=0}^T G_t\nabla_{\theta}\log\pi_{\theta}(A_t|S_t)] \quad\quad\quad\quad\quad\quad(1) \\
&= E_{\tau\sim\pi_{\theta}}[\sum_{t=0}^T (G_t-b(S_t))\nabla_{\theta}\log\pi_{\theta}(A_t|S_t)] \quad\quad(2)
\end{split}
$$

式(2)用 $G_t-b(S_t)$ 代替了 $G_t$ 。这里 $b(S_t)$ 可以是任何函数。也就是说，只要输入是 $S_t$ ， $b(S_t)$ 是什么函数都行。这个 $b(S_t)$ 就是基线。

式（2）中的 $b(S_t)$ 可以是任何函数。例如，在状态 $S_t$ 下，可以考虑使用之前获得的奖励的平均值作为 $b(S_t)$ 。实践中经常使用的是价值函数，数学式为 $b(S_t)=V_{\pi_{\theta}}(S_t)$ 。如果能够使用基线减小方差，那么就可以进行样本效率更高的训练。另外，将价值函数作为基线使用时，我们是不知道真正的价值函数 $v_{\pi_{\theta}}(S_t)$ 的。这种情况下还需要训练价值函数。

最后，我们再通过直观介绍补充说明一下为什么使用基线更好。这里以倒立摆为例，思考图示的状态。

![[1.jpg]]

==杆子失去平衡的状态==

上图表示游戏结束之前的杆子失去平衡的状态。在这种状态下，无论采取什么样的行动，在几个时间步之后游戏都将结束。

设图的状态为 $s$ ，在此状态下采取的行动为 $a$ 。假定从状态 $s$ 开始几个时间步（比如3个时间步）后游戏一定会结束。在这种情况下，状态 $s$ 的回报 $G$ 为 $3$ （这里设折扣因子 $\gamma$ 为 $1$ 。如果使用的是没有基线的REINFORCE，那么状态 $s$ 下的行动 $a$ 就会因为权重 $3$ 而被增强（状态 $s$ 下选择行动 $a$ 的概率会变高）。但无论采取什么样的行动，3 个时间步之后游戏一定会结束，所以可以说这种提高行动 $a$ 被选择的概率的工作是无意义的。

此时就要用到基线了。这里使用价值函数作为基线，假设我们已经知道图中的例子中的 $V_{\pi_{\theta}}(S_t)=3$ 。此时的权重为 $G_t-V_{\pi_{\theta}}$ 所以是 0 。由于权重是 0 ，因此无论选择什么行动，采取那个行动的概率都不会变大，也不会变小。像这样使用基线，有望减少无谓的训练。

```ad-summary
title: 总结

倒立摆环境中只要倒立摆没有倒，给出的奖励总是 1 。也就是不管策略多么的差劲，采样出的轨迹的回报一定是正的。也就是说，即使轨迹的回报很低，也会提升在 $S_t$ 采取 $A_t$ 的概率，只是可能提升的不大。

如果加入基线，那么如果轨迹的回报很低，权重就会成为负值，从而在策略更新之后，直接降低在 $S_t$ 采取 $A_t$ 的概率。所以训练速度会加快。
```

## 2.4 Actor-Critic（演员-评论家）

如果在上节介绍的带基线的REINFORCE中使用价值函数作为基线，那么就可以将其视为基于价值且基于策略的方法。本节将进一步改进带基线的REINFORCE，推导一个叫作Actor-Critic的算法。Actor-Critic也是基于价值且基于策略的方法。

![[2.jpg]]


### Actor-Critic 的推导

首先从复习带基线的REINFORCE开始。带基线的REINFORCE的目标函数的梯度的数学式如下所示。

$$
\nabla_{\theta}J(\theta)=E_{\tau\sim\pi_{\theta}}[\sum_{t=0}^T(G_t-b(S_t))\nabla_{\theta}\log\pi_{\theta}(A_t|S_t)]
$$

式中的 $G_t$ 表示回报，$b(S_t)$ 表示基线。可以使用任何函数作为基线。这里我们使用基于神经网络建模的价值函数作为基线。因此，我们要用到以下这些新的记号。

- $\omega$ ：表示价值函数的神经网络的所有权重参数。
- $V_{\omega}(S_t)$ ：将价值函数模型化的神经网络。

此时目标函数的梯度的数学式如下所示

$$
\nabla_{\theta}J(\theta)=E_{\tau\sim\pi_{\theta}}[\sum_{t=0}^T(G_t-V_{\omega}(S_t))\nabla_{\theta}\log\pi_{\theta}(A_t|S_t)]
$$

式中存在一个问题，即只要没抵达目标，就无法确定回报 $G_t$ 的值。 也就是说，在抵达目标之前，无法更新策略和价值函数。如果这是基于蒙特卡洛方法的算法，那么二者无法更新就是它的缺点。

> [!NOTE]
> 我们之前的实现用的都是 **蒙特卡洛采样的方法** 。也就是采样出一条完整的轨迹。然后更新策略神经网络 $\pi_\theta$ 。

消除这个缺点的方法是 **时序-差分方法（TD方法）** 。使用TD方法训练价值函数时，使用1个时间步（或 $n$ 个时间步）后的结果就能进行更新，如图所示。

![[2.5.excalidraw]]

如图所示，在训练价值函数 $V_{\omega}(S_t)$ 时，蒙特卡洛方法使用的是回报 $G_t$ ，而TD方法使用的是 $R_t+\gamma V_{\omega}(S_{t+1})$ 。

> [!NOTE]
> 使用神经网络对价值函数建模时，我们以接近 $R_t+\gamma V_{\omega}(S_{t+1})$ 为目标训练 $V_{\omega}(S_t)$ 的值。具体来说就是将 $V_{\omega}(S_t)$ 和 $R_t+\gamma V_{\omega}(S_{t+1})$ 的均方差作为损失函数，通过梯度下降法更新神经网络的权重。

下面将基于蒙特卡洛方法的公式切换为TD方法，其中，代替 $G_t$ 的是 $R_t+\gamma V_{\omega}(S_{t+1})$ 。此时得到的式子如下所示。

$$
\nabla_{\theta}J(\theta)=E_{\tau\sim\pi_{\theta}}[\sum_{t=0}^T(R_t+\gamma V_{\omega}(S_{t+1})-V_{\omega}(S_t))\nabla_{\theta}\log\pi_{\theta}(A_t|S_t)]
$$

基于上面的公式的算法就是 $\text{Actor-Critic}$ 。策略 $\pi_{\theta}$ 和价值函数 $V_{\omega}$ 是神经网络，我们要同时训练这两个神经网络。具体来说，对于策略 $\pi_{\theta}$ 要基于上面的公式进行训练；而对于价值函数 $V_{\omega}$ ，则通过TD方法，以接近 $R_t+\gamma V_{\omega}(S_{t+1})$ 为目标训练 $V_{\omega}(S_t)$ 的值。

> [!NOTE]
> Actor-Critic中的Actor是 **演员（采取动作的人）** 的意思，也就是采取动作的人，相当于策略 $\pi_{\theta}$ 。而Critic是 **评论家** 的意思，相当于价值函数 $V_{\omega}$ 。因此，Actor-Critic的意思是“使用 $V_{\omega}$ ，来评论基于策略 $\pi_{\theta}$ 采取的动作的好坏”。

### Actic-Critic 的代码实现

下面实现Actor-Critic。策略和价值函数这两个神经网络的代码如下所示。

```python
import numpy as np
import gym
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.distributions import Categorical

class PolicyNet(nn.Module):
	def __init__(self, action_size):
		super().__init__()
		self.l1 = nn.Linear(4, 128)
		self.l2 = nn.Linear(128, action_size)

	def forward(self, x):
		x = F.relu(self.l1(x))
		x = F.softmax(self.l2(x), dim=1)
		return x

class ValueNet(nn.Module):
	def __init__(self):
		super().__init__()
		self.l1 = nn.Linear(4, 128)
		self.l2 = nn.Linear(128, 1)

	def forward(self, x):
		x = F.relu(self.l1(x))
		x = self.l2(x)
		return x
```

上面的代码将策略的网络实现为了 `PolicyNet` 类，将价值函数的网络实现为了 `ValueNet` 类。策略的最终输出是 `Softmax` 函数的输出，所以输出的是概率。接下来是 `Agent` 类的代码。

```python
class Agent:
	def __init__(self):
		self.gamma = 0.98
		self.lr_pi = 0.0002
		self.lr_v = 0.0005
		self.action_size = 2

		self.pi = PolicyNet(self.action_size)
		self.v = ValueNet()

		self.optimizer_pi = optim.Adam(self.pi.parameters(), lr=self.lr_pi)
		self.optimizer_v = optim.Adam(self.v.parameters(), lr=self.lr_v)

	def get_action(self, state):
		state = torch.tensor(state[np.newaxis, :]) # 增加小批量的轴
		probs = self.pi(state)
		probs = probs[0]
		m = Categorical(probs)
		action = m.sample().item()
		return action, probs[action]

	def update(self, state, action_prob, reward, next_state, done):
		# 增加小批量的轴
		state = torch.tensor(state[np.newaxis, :])
		next_state = torch.tensor(next_state[np.newaxis, :])

		# ①self.v 的损失
		target = reward + self.gamma * self.v(next_state) * (1 - done)
		target.detach()
		v = self.v(state)
		loss_fn = nn.MSELoss()
		loss_v = loss_fn(v, target)

		# ②self.pi 的损失
		delta = target - v
		loss_pi = -torch.log(action_prob) * delta.item()

		self.optimizer_v.zero_grad()
		self.optimizer_pi.zero_grad()
		loss_v.backward()
		loss_pi.backward()
		self.optimizer_v.step()
		self.optimizer_pi.step()
```

`get_action` 方法可以基于策略取出动作。需要注意的是，由于输入到神经网络中的数据将作为小批量进行处理，因此在处理其中一个数据（状态）时需要小批量的轴。另外，`get_action` 方法返回了两个值，即选择的动作及其概率。选择动作的概率将在稍后的损失函数计算中使用。

`update` 方法可以训练价值函数和策略。在代码 ① 处为价值函数(`self.v`)计算损失。为此要计算TD目标（target），求出当前状态下其与价值函数（v）的均方差。然后，在代码 ② 处为策略（`self.pi`）计算损失。需要将其乘以 `-1` 的值作为损失。剩下的就是一直以来的神经网络的训练代码。

```python
for episode in range(2000):
    state = env.reset()
    done = False
    total_reward = 0

    while not done:
        action, prob = agent.get_action(state)
        next_state, reward, done, info = env.step(action)

        agent.update(state, prob, reward, next_state, done)

        state = next_state
        total_reward += reward

    reward_history.append(total_reward)
    if episode % 100 == 0:
        print("episode :{}, total reward : {:.1f}".format(episode, total_reward))

plt.xlabel('Episode')
plt.ylabel('Total Reward')
plt.plot(range(len(reward_history)), reward_history)
plt.show()
```

得到的结果如下图所示。

![[Figure_2.png]]

## 2.5* 策略梯度法相关证明（选学）

### 1. 策略梯度法的推导

当 $J(\theta)=\mathbb{E}_{\tau\sim\pi_\theta}\left\lbrack{G(\tau)}\right\rbrack$ 时，其梯度如下面的式子所示。

$$
\nabla_\theta{J_\theta}=\mathbb{E}_{\tau\sim\pi_\theta}\left\lbrack{\sum_{t=0}^TG(\tau)\nabla_\theta\log\pi_\theta(A_t|S_t)}\right\rbrack
$$

下面对上面的式子进行证明。

$$
\begin{split}
\nabla_\theta{J(\theta)} &= \nabla_\theta\mathbb{E}_{\tau\sim\pi_\theta}\left\lbrack{G(\tau)}\right\rbrack \\
&= \nabla_\theta\sum_\tau{\Pr(\tau|\theta)G(\tau)} \quad\text{（展开期望值）} \\
&= \sum_\tau\nabla_\theta(\Pr(\tau|\theta)G(\tau)) \quad\text{（将}\pi_\theta\text{移动到}\sum\text{中）} \\
&= \sum_\tau\left\lbrace{G(\tau)\nabla_\theta\Pr(\tau|\theta)+\Pr(\tau|\theta)\nabla_\theta{G(\tau)}}\right\rbrace \quad\text{（积的微分）} \\
&= \sum_\tau{G(\tau)\nabla_\theta\Pr(\tau|\theta)} \quad\text{（}\nabla_\theta{G(\tau)}\text{永远为0）} \\
&= \sum_\tau G(\tau)\Pr(\tau|\theta)\frac{\nabla_\theta\Pr(\tau|\theta)}{\Pr(\tau|\theta)} \quad\text{（乘以}{\frac{\Pr(\tau|\theta)}{\Pr(\tau|\theta)}}\text{）} \\
&= \sum_\tau G(\tau)\Pr(\tau|\theta)\nabla_\theta\log\Pr(\tau|\theta) \quad\text{（}\log\text{梯度的技巧）} \\
&= \mathbb{E}_{\tau\sim\pi_\theta}\left\lbrack{G(\tau)\nabla_\theta\log\Pr(\tau|\theta)}\right\rbrack
\end{split}\tag{1}
$$

这里对 “log梯度的技巧” 进行说明。这个技巧利用了以下等式。

$$
\nabla_\theta\log\Pr(\tau|\theta)=\frac{\nabla_\theta\Pr(\tau|\theta)}{\Pr(\tau|\theta)}
$$

> [!NOTE]
> $$\log(f(x))=\frac{f'(x)}{f(x)}$$

根据上面的式子，我们就知道

$$
\nabla_\theta\Pr(\tau|\theta)=\Pr(\tau|\theta)\nabla_\theta\log\Pr(\tau|\theta)
$$

这就是著名的 **log梯度的技巧** 。是机器学习领域常用的数学式的变形形式。

接下来，我们将利用以下等式进一步展开$(1)$。

$$
\begin{split}
\Pr(\tau|\theta) &= p(S_0)\pi_\theta(A_0|S_0)p(S_1|S_0,A_0)\cdots\pi_\theta(A_T|S_T)p(S_{T+1}|S_T,A_T) \\
&= p(S_0)\prod_{t=0}^T\pi_\theta(A_t|S_t)p(S_{t+1}|S_t,A_t)
\end{split}
$$

这里，$p(S_0)$ 表示初始状态 $S_0$ 的概率。上面的式子表明，得到轨迹 $\tau$ 的概率可以用初始状态的概率、策略以及下一个状态的迁移概率的乘积来表示。另外，我们可以用下面的式子来表示 $\log\Pr(\tau|\theta)$ 。

$$
\log\Pr(\tau|\theta)=\log{p(S_0)} + \sum_{t=0}^T\log{p(S_{t+1}|S_t,A_t)} + \sum_{t=0}^T\log\pi_\theta(A_t|S_t)
$$

由于 $\log xy = \log x + \log y$ ，所以可以像上面的式子那样表示为和的形式。基于上面的式子，可以将 $\nabla_\theta{\log\Pr(\tau|\theta)}$ 展开为如下形式。

$$
\begin{split}
\nabla_\theta\log\Pr(\tau|\theta) &= \nabla_\theta\left\lbrace\log{p(S_0)} + \sum_{t=0}^T\log{p(S_{t+1}|S_t,A_t)} + \sum_{t=0}^T\log\pi_\theta(A_t|S_t)\right\rbrace \\
&= \nabla_\theta\sum_{t=0}^T\log\pi_\theta(A_t|S_t)
\end{split}
$$

$\nabla_\theta$ 是对 $\theta$ 的梯度。与 $\theta$ 无关的元素的梯度 $\nabla_\theta\log p(S_0)$ 和 $\nabla_\theta\sum_{t=0}^T\log{p(S_{t+1}|S_t,A_t)}$ 为 0 。因此，从上面的式子可以得到下列式子。

$$
\begin{split}
\nabla_\theta{J(\theta)}&=\mathbb{E}_{\tau\sim\pi_\theta}\left\lbrack{G(\tau)\nabla_\theta\log\Pr(\tau|\theta)}\right\rbrack \\
&= \mathbb{E}_{\tau\sim\pi_\theta}\left\lbrack{\sum_{t=0}^TG(\tau)\nabla_\theta\log\pi_\theta(A_t|S_t)}\right\rbrack
\end{split}
$$

这样我们就完成了 $\nabla_\theta{J(\theta)}$ 的推导。

### 2. 基线的推导

$$
\begin{split}
\nabla_\theta J(\theta) &= \mathbb{E}_{\tau\sim\pi_\theta}\left\lbrack{\sum_{t=0}^TG_t\nabla_\theta\log\pi_\theta(A_t|S_t)}\right\rbrack \\
&= \mathbb{E}_{\tau\sim\pi_\theta}\left\lbrack{\sum_{t=0}^T(G_t-b(S_t))\nabla_\theta\log\pi_\theta(A_t|S_t)}\right\rbrack \\
\end{split}
$$

如上面的式子所示，我们可以使用 $G_t-b(S_t)$ 代替 $G_t$ 。$b(S_t)$ 是 **任何函数** ，我们称之为“基线”。下面进行上式的推导。

首先，证明以下式子成立。

$$
\mathbb{E}_{x\sim P_\theta}\left\lbrack{\nabla_\theta\log P_\theta(x)}\right\rbrack = 0 \tag{1}
$$

这里假设随机变量 $x$ 是基于概率分布 $P_\theta(x)$ 生成的。$P_\theta(x)$ 会根据参数 $\theta$ 改变概率分布的形状。此时有以下式子成立。

$$
\sum_xP_\theta(x)=1
$$

由于 $P_\theta(x)$ 是概率分布，因此所有 $x$ 的值的和为 1 。然后，求这个式子的梯度。

$$
\nabla_\theta\sum_xP_\theta(x)=\nabla_\theta 1 = 0
$$

接下来，使用 $\log$ 梯度的技巧将式子展开，过程如下所示。

$$
\begin{split}
0 &= \nabla_\theta\sum_xP_\theta(x) \\
&= \sum_x\nabla_\theta P_\theta(x) \\
&= \sum_xP_\theta(x)\nabla_\theta\log P_\theta(x) \\
&= \mathbb{E}_{x\sim P_\theta}\left\lbrack{\nabla_\theta\log P_\theta(x)}\right\rbrack
\end{split}
$$

证明(1)完毕。接下来将证明的式子用于我们的问题。具体来说，用 $A_t$ 代替(1)中的 $x$ ，然后使用 $\pi_\theta(\cdot|S_t)$ 代替 $P_\theta(\cdot)$ 。这样就可以得到以下式子。

$$
\mathbb{E}_{A_t\sim\pi_\theta}\left\lbrack{\nabla_\theta\log\pi_\theta(A_t|S_t)}\right\rbrack = 0
$$

上面的式子是对 $A_t$ 的期望值。因此，我们可以像下面的式子那样，将任何函数 $b(S_t)$ 放入期望值中。

$$
\mathbb{E}_{A_t\sim\pi_\theta}\left\lbrack{b(S_t)\nabla_\theta\log\pi_\theta(A_t|S_t)}\right\rbrack = 0 \tag{2}
$$

$b(S_t)$ 是以 $S_t$ 为参数的函数，即使 $A_t$ 发生变化，它的值也不会改变。由于式子(2)是对 $A_t$ 的期望值，因此即使在期望值中加入函数 $b(S_t)$ ，等式也成立。

> [!NOTE]
> 动作 $A_t$ 的变化会导致收益 $G_t$ 的变化，因此以下式子不成立。
> $$\mathbb{E}_{A_t\sim\pi_\theta}\left\lbrack{G_t\nabla_\theta\log\pi_\theta(A_t|S_t)}\right\rbrack = 0$$

式子(2)在整个 $t=0\sim T$ 的范围都成立，所以可以得到以下式子。

$$
\mathbb{E}_{A_t\sim\pi_\theta}\left\lbrack{\sum_{t=0}^Tb(S_t)\nabla_\theta\log\pi_\theta(A_t|S_t)}\right\rbrack = 0
$$

所以基线证明完毕。

## 2.6 总结

在本章中，我们学习了基于策略的方法——策略梯度法。具体来说，我们学习了 4 种策略梯度法的算法。它们的统一的数学式如下所示。

$$
\nabla_\theta J(\theta)=\mathbb{E}_{\tau\sim\pi_\theta}\left\lbrack{\sum_{t=0}^T\Phi_t\nabla_\theta\log\pi_\theta(A_t|S_t)}\right\rbrack
$$

1. $\Phi_t=G(\tau)$                                             （最简单的策略梯度法）
2. $\Phi_t=G_t$                                                 （REINFORCE）
3. $\Phi_t=G_t-b(S_t)$                                   （带基线的REINFORCE）
4. $\Phi_t = R_t + \gamma V(S_{t+1})-V(S_t)$             （Actor-Critic）

上面4种方法的权重 $\Phi_t$ 各不相同。最简单的策略梯度法在所有时刻的权重都是 $G_\tau$ 。对它进行改进，将时刻 $t$ 的收益 $G_t$ 作为权重进行评估的是REINF0RCE。另外，通过加入“基线”这一方法，减小了方差的方法是带基线的REINFORCE。Actor-Critic是一种除了策略之外，对价值函数也用神经网络建模的方法。期待 $(1)\rightarrow(2)\rightarrow(3)\rightarrow(4)$ 这样的编号一直延续下去，从而有更高级的方法出现，产生更好的结果。

```ad-note
学习完这一章，我们深刻体会到策略梯度方法的巧妙与强大，堪称图灵奖级别的突破。然而，策略梯度法在实际应用中也存在一些挑战。

首先，策略梯度法在计算期望回报时，通常只采样一条轨迹来估计梯度。若这条轨迹的奖励异常低，反向传播更新的策略网络可能会朝着错误的方向调整，导致策略性能下降，甚至训练过程越走越差。反之，如果采样到的轨迹回报异常高，策略更新步伐过大，也可能引发策略崩溃，表现为训练过程中的剧烈震荡和不稳定。

这种现象的根源在于，策略梯度方法不像传统神经网络训练那样拥有固定的输入-输出样本对。强化学习的训练数据是通过当前策略采样得到的轨迹，而轨迹的奖励质量难以人为控制和保证。如果我们事先拥有大量高质量的轨迹数据，直接用这些数据训练神经网络即可，这时就不再需要强化学习的方法。

为了解决这些问题，研究者们提出了多种改进策略梯度的方法，例如：

- **TRPO（Trust Region Policy Optimization，置信域策略优化）**：通过限制每次更新的策略变化幅度，保证训练的稳定性；
- **PPO（Proximal Policy Optimization，近端策略优化）**：通过裁剪目标函数控制策略更新步长，兼顾效率与稳定；
- **DPO（Direct Preference Optimization，直接偏好优化）**：基于偏好反馈进行策略优化，减少对奖励函数的依赖；
- **GRPO（Group Relative Policy Optimization，组相对策略优化）**：通过组内相对策略比较提升训练效果。

这些方法共同推动了策略梯度算法在实际强化学习任务中的表现和稳定性。
```

