通过神经网络等方法将策略模型化，并使用梯度来优化策略的方法叫作 **策略梯度法（policy gradient method）** 。

研究者们提出了各种基于策略梯度法的算法。本章首先介绍最简单的策略梯度法。然后，在改进这个简单的策略梯度法的过程中，我们推导出了被称为 **REINFORCE** 的算法。接下来，在进一步改进 REINFORCE 的过程中， 我们又推导出了 **带基线** 的 REINFORCE 方法和 Actor-Critic（演员-评论家） 方法。

随机性策略用数学式可以表示为 $\pi(a|s)$ 。$\pi(a|s)$ 是在状态 $s$ 下采取动作 $a$ 的概率。这里采用神经网络对策略进行建模。此时用符号 $\theta$ 来汇总表示神经网络的所有权重参数（ $\theta$ 是将所有参数的元素排成一列的向量）。另外，可以将基于神经网络的策略表示为 $\pi_{\theta}(a|s)$ 。

![[2.1.excalidraw]]

还是倒立摆环境，每当倒立摆环境处于某个状态 $s$ 时，我们就会使用神经网络 $\pi_\theta$ 来决定要采取什么动作。

那么，问题是：这个神经网络怎么训练？

> [!NOTE]
> 想要训练一个神经网络，需要有 **输入-输出** 对。例如，`mnist` 手写数字数据集，要训练一个可以识别手写数字的卷积神经网络，需要构建一个网络结构，然后提供输入（图片）以及输出（分类标签）。
> 当然，还得有 **损失函数** ，例如交叉熵损失函数或者均方误差损失函数，等等。

![[mnist_nn.png]]

但是对于倒立摆环境，想要训练推车的策略神经网络，输入是什么，输出是什么？以及损失函数又是什么？

首先回顾一下第一章的知识。

首先明确问题的设定。这里考虑的是回合制任务，并基于策略 $\pi_{\theta}$ 选择动作的情况。在这种情况下，假定得到了以下由“状态、动作、奖励”构成的时间序列数据。

$$
\tau = (S_0, A_0, R_0, S_1, A_1, R_1, \cdots, S_{T+1})
$$

这个 $\tau$ 也叫作轨迹（trajectory）。此时可以使用折扣因子 $\gamma$ 对回报（Return，收益）作如下定义。

$$
G(\tau) = R_0 + \gamma R_1 + \gamma^2 R_2 + \cdots + \gamma^T R_T
$$

为了表明回报可以由 $\tau$ 计算出来，上面的式子将其表示为了 $G(\tau)$ 。此时，目标函数 $J(\theta)$ 可以表示为以下式子。

$$
J(\theta) = \mathbb{E}_{\tau\sim\pi_{\theta}}[G(\tau)]
$$

回报 $G(\tau)$ 是随机变动的，所以它的期望值是目标函数。上式中期望值 $E$ 的下标为 $\tau\sim\pi_{\theta}$ ，这个下标表示 $\tau$ 是基于 $\pi_{\theta}$ 生成的。

![[2.2.excalidraw]]



> [!NOTE]
> 我们的目标是让 $J(\theta)$ 最大。

在神经网络的训练中，我们的目的是让 **损失函数** 最小。而在策略梯度法中，我们的目的是让 **目标函数** 最大。所以都可以使用梯度法。让损失函数最小，使用梯度下降法。让目标函数最大，使用梯度上升法。

所以，我们还是得求解 $J(\theta)$ 的梯度。要求导的参数是 $\theta$ 。

确定了目标函数后，下一步是计算它的梯度。这里将参数 $\theta$ 的梯度表示为 $\nabla_{\theta}$ 。我们的目标是求 $\nabla_{\theta}J(\theta)$ 。$\nabla$ 是梯度符号。

$$
\begin{split}
\nabla_{\theta}J(\theta) &= \nabla_{\theta}\mathbb{E}_{\tau\sim\pi_{\theta}}[G(\tau)] \\
&= \mathbb{E}_{\tau\sim\pi_{\theta}}[\sum_{t=0}^TG(\tau)\nabla_{\theta}\log\pi_{\theta}(A_t|S_t)]
\end{split}
$$

上面的式子中值得注意的是，$\nabla_{\theta}$ 在 $\mathbb{E}$ 中（梯度计算的部分是 $\nabla_{\theta}\log\pi_{\theta}(A_t|S_t)$ ）。后面会对此做详细介绍。求出 $\nabla_{\theta}J(\theta)$ 之后，接下来更新神经网络的参数。最优化方法多种多样，下面的式子表示的是一种简单的方法。

$$
\theta \leftarrow \theta + \alpha\nabla_{\theta}J(\theta)
$$

上面的式子朝着梯度的方向更新参数 $\theta$ 。更新的值与 $\alpha$ 相关。这里的 $\alpha$ 表示学习率。这是属于梯度上升法的算法。

如式所示，$\nabla_{\theta}J(\theta)$ 表示期望值。接下来我们来计算期望值。这里，我们令策略 $\pi_{\theta}$ 的智能体实际采取动作，得到 $n$ 个轨迹 $\tau$ 。此时，通过对每个 $\tau$ 计算式子的期望值内部的式子（$\sum_{t=0}^TG(\tau)\nabla_{\theta}\log\pi_{\theta}(A_t|S_t)$），并求出其平均值，从而近似得到 $\nabla_{\theta}J(\theta)$ 。数学式如下所示。

$$
\begin{split}
\text{采样}: \tau^{(i)}\sim\pi_{\theta}\;\;\;\;(i=1,2,\cdots,n) \\
x^{(i)} = \sum_{t=0}^TG(\tau^{(i)})\nabla_{\theta}\log\pi_{\theta}(A_t^{(i)}|S_t^{(i)}) \\
\nabla_{\theta}J(\theta) \approx \frac{x^{(1)}+x^{(2)}+\cdots+x^{(n)}}{n}
\end{split}
$$

上面式子中的 $\tau^{(i)}$ ，表示在第 $i$ 回合得到的轨迹，$A_t^{(i)}$ 表示在第 $i$ 回合的时刻 $t$ 的动作，$S_t^{(i)}$ 表示在第 $i$ 回合的时刻 $t$ 的状态。

另外，再思考一下蒙特卡洛方法的样本数为 $1$ ，即上式中 $n=1$ 的情况。在这种情况下，数学式可以简化为如下形式。

$$
\begin{split}
\text{采样}: \tau\sim\pi_{\theta} \\
\nabla_{\theta}J(\theta) \approx \sum_{t=0}^TG(\tau)\nabla_{\theta}\log\pi_{\theta}(A_t|S_t) \\
\end{split}
$$

为了简单起见，本章将使用以上面的式子为对象的策略梯度法。上面的式子的计算就是对所有时刻（$t=0\sim T$）求 $\nabla_{\theta}\log\pi_{\theta}(A_t|S_t)$ ，然后将各梯度乘以作为权重的回报 $G(\tau)$ ，最后求它们的和。这个计算过程如下图所示。

![[2.3.excalidraw|800x100]]

接下来，我们用代码来实现一下策略梯度法。

```python
import numpy as np
import gym
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.distributions import Categorical

class Policy(nn.Module):
	def __init__(self, action_size):
		super().__init__()
		self.l1 = nn.Linear(4, 128)
		self.l2 = nn.Linear(128, action_size)

	def forward(self, x):
		x = F.relu(self.l1(x))
		x = F.softmax(self.l2(x), dim=1)
		return x
```

这里实现的神经网络模型由两层全连接层构成。最终输出的元素数是动作的数量（ `action_size` ）。由于这个最终输出是 `Softmax` 函数的输出，因此可以得到每个动作的概率。

这个网络结构就是我们要训练的策略神经网络 $\pi_\theta$ 。其中 $\theta$ 就是神经网络的参数。

我们还记得，策略神经网络的输入是环境的状态，输出是要采取的动作的概率分布。所谓动作的概率分布就是：向左推的概率是多少，向右推的概率是多少。

而我们的网络结构输入的张量的维度是 4 。为什么呢？因为倒立摆环境的状态有 4 个维度。输出是 `action_size` 个维度。也就是动作的数量个维度。

> [!NOTE]
> 如果将具有 $n$ 个元素的向量输入到 `Softmax` 函数中，那么输出的同样是具有 $n$ 个元素的向量。此时，第 $i$ 个输出 $y_i$ 的式子如下所示。
> $$y_i = \frac{\mathrm{e}^{x_i}}{\sum_{k=1}^n\mathrm{e}^{x_k}}$$
> 这里的 $\mathrm{e}$ 是自然常数(值为 $2.718 28...$ 的无限小数)。`Softmax` 函数的输出值全部为 $0$ 以上 $1$ 以下的实数，它们的合计值为 $1$（$\sum_{i=1}^ny_i=1$）。因此，`Softmax` 函数的输出可以作为概率使用。

下面是 `Agent` 类的代码。首先显示初始化和 `get_action` 方法。

```python
class Agent:
	def __init__(self):
		self.gamma = 0.98 # 折扣因子
		self.lr = 0.0002 # 学习率
		self.action_size = 2 # 动作空间大小，共两个动作：向左推和向右推

		self.memory = []
		self.pi = Policy(self.action_size) # 初始化策略神经网络
		# 使用 Adam 优化器
		self.optimizer = optim.Adam(self.pi.parameters(), lr=self.lr)

	def get_action(self, state):
		# 将状态转换成torch.tensor类型
		state = torch.tensor(state[np.newaxis, :])
		# 将状态输入策略神经网络，输出为两个动作的概率分布
		probs = self.pi(state)
		# 取出概率分布
		probs = probs[0]
		# 下面两行根据动作的分布采样出一个动作
		m = Categorical(probs)
		action = m.sample().item()

		# 返回动作和动作的概率
		return action, probs[action]
```

`get_action` 方法决定了在 `state` 状态下采取的动作。为此，可以通过 `self.pi(state)` 进行神经网络的前向传播，得到概率分布 `probs` 。然后，基于该概率分布，进行一次动作的采样。该方法还返回了所选动作的概率（上面代码中的 `probs[action]` ）。

下面来试用一下 `get_action` 方法。代码如下所示。

```python
env = gym.make('CartPole-v0')
state = env.reset()
agent = Agent()

action, prob = agent.get_action(state)
print('action: ', action)
print('prob: ', prob)

G = 100.0 # 虚拟权重
J = G * prob.log()
print('J: ', J)

# 求梯度
J.backward()
```

上面的代码取出了初始状态下的动作及其概率。另外，它还显示了使用虚拟的权重来计算由下式表示的梯度的代码（这是从式子取出的 $t = 0$ 的相关项的式子）。

$$
G(\tau)\nabla_{\theta}\log\pi_{\theta}(A_0|S_0)
$$

作为参考，下面对照列出了上面的代码中出现的变量与相应的数学式。

- `prob` ： $\pi_{\theta}(A_0|S_0)$
- `G` ： $G(\tau)$
- `J` ： $G(\tau)\log\pi_{\theta}(A_0|S_0)$

求出 `J` 之后，通过 `J.backward()` 求 $G(\tau)\nabla_{\theta}\log\pi_{\theta}(A_0|S_0)$ 。下面是 `Agent` 类剩下的代码。

```python
class Agent:
	...

	def add(self, reward, prob):
		data = (reward, prob)
		self.memory.append(data)

	def update(self):
		G, loss = 0, 0
		for reward, prob in reversed(self.memory):
			G = reward + self.gamma * G
		for reward, prob in self.memory:
			loss += - prob.log() * G

		self.optimizer.zero_grad()
		loss.backward()
		self.optimizer.step()
		self.memory = [] # 重置内存
```

`add` 方法是智能体每次采取行动并获得奖励时被调用的方法。该方法将奖励（`reward`）和智能体采取的行动的概率（`prob`）存储在内存（`self.memory`）中。`update` 方法是智能体到达目标时被调用的方法。该方法首先会求回报 `G` 。回报可以通过反向计算获得的奖励的做法来高效的计算。然后该方法会计算损失函数。具体做法是对每个时刻求 `-prob.log()` ，再将其乘以作为权重的 `G` ，之后求它们的和。剩下的就是一直以来的神经网络的训练代码。

在训练神经网络时，通常要设置损失函数。对于这个例子，我们可以将目标函数 $J(\theta)$ 乘以 $-1$ 所得到的 $-J(\theta)$ 作为损失函数，此时可以通过梯度下降法的最优化方法（SGD、Adam等）更新参数。

最后在倒立摆环境中运行智能体。代码如下所示。

```python
env = gym.make('CartPole-v0')
agent = Agent()
reward_history = []

for episode in range(3000):
	# 重置倒立摆环境的状态
	state = env.reset()
	done = False
	total_reward = 0

	# 采样一条轨迹
	while not done:
		# 根据环境的当前状态决策要采取什么动作
		action, prob = agent.get_action(state)
		# 采取动作，环境会返回环境的下一个状态，奖励和是否结束游戏
		next_state, reward, done, _ = env.step(action)

		# 将奖励和动作的概率保存下来
		agent.add(reward, prob)
		# 环境转移到下一个状态
		state = next_state
		# 累积奖励
		total_reward += reward

	# 使用反向传播算法更新策略神经网络
	agent.update()
	reward_history.append(total_reward)
	
	if episode % 100 == 0:
		print("回合:{}, 总奖励:{:.1f}".format(episode, total_reward))

torch.save(agent.pi.state_dict(), 'policy_model.pth')
print("模型已保存")
```

首先，在 `while` 语句中，增加智能体获得的奖励（`reward`）和行动的概率（`prob`）。然后在离开 `while` 语句后（回合结束时），通过 `agent.update()` 更新策略。

运行此代码，随着回合的推进，获得的奖励也会增加。下图是结果的示意图。

绘制示意图的代码如下：

```python
import matplotlib.pyplot as plt
# 训练结束后绘制奖励变化图
plt.plot(reward_history)
plt.xlabel('回合')
plt.ylabel('总奖励')
plt.title('每回合奖励')
plt.grid(True)
plt.show()
```

通过观察绘制出来的图像，我们发现每回合的奖励随着训练震荡的很厉害，但是随着训练的进行，每回合获得的总奖励确实越来越多了。

我们可以测试一下训练的策略神经网络。看看倒立摆能不能坚持很长时间。

```python
def test_render(agent, env, episodes=5):
	for episode in range(episodes):
		state = env.reset()
		done = False
		total_reward = 0
		
		while not done:
			env.render()
			action, _ = agent.get_action(state)
			next_state, reward, done, _ = env.step(action)
			state = next_state
			total_reward += reward

		print(f"回合 {episode + 1}: 总奖励 = {total_reward}")
	env.close()

# 加载模型后测试
env = gym.make('CartPole-v0')
agent = Agent()
agent.pi.load_state_dict(torch.load('policy_model.pth'))
agent.pi.eval()
test_render(agent, env)
```

我们大概可以知道，随着回合的推进，奖励的总和会逐渐增加。但即使经历了 3000 个回合，依然没有达到这次任务的上限值 200，所以似乎还有改进的余地。下面让我们来改进一下这里推导的最简单的策略梯度法。这个改进算法就是著名的 **REINFORCE** 算法。

