
本章介绍了构建推理模型的 4 种主流方法，并探讨了如何提升大语言模型的推理能力。

2024 年，大语言模型领域呈现出日益专业化的趋势。除了预训练（pre-training）和微调（fine-tuning），我们还见证了诸如 RAG（检索增强生成）、代码助手等专业应用程序的兴起。可以看到，2025 年这一趋势正在加速发展，并且更加注重领域和应用场景的特定优化（“专业化”），如下图所示。

![[8.1.excalidraw|600x100]]

第一阶段至第三阶段是开发大语言模型的常见流程，第四阶段则专注于将大语言模型应用于特定的场景。

推理模型的开发是这些专业化方向之一。这意味着，我们需要进一步优化大语言模型，使其能够在需要 **多步推理** 的复杂任务（如解谜题、数学推导和解决复杂的编程问题）上表现得更好。然而，这种专业化并不会取代大语言模型的其他应用场景，因为将大语言模型转变为推理模型也会带来一些弊端，后面会详细讨论。

接下来的内容中将包含以下几个方面：

- 解释“推理模型”的含义
- 分析推理模型的优缺点
- 概述 DeepSeek R1 的训练流程
- 总结构建和优化推理模型的四大核心方法
- 分享 DeepSeek R1 发布后对大语言模型领域发展的看法
- 提供在有限预算下开发推理模型的实用建议

## 1. 如何定义“推理模型”

如果你从事的是人工智能（或广义上的机器学习）领域的工作，那么可能对一些模糊且备受争议的术语并不陌生。“推理模型”这一术语也不例外。通常，某篇论文会对这样的术语给出一个定义，但很快它们又会在下一篇论文中被重新定义，如此循环往复。

在本章中，我们将 **“推理”** 定义为 **解答那些需要复杂、多步骤生成并包含中间过程的复杂问题的过程** 。例如，回答像“法国的首都是哪里？”这种事实性的问题并不涉及推理。但如果回答像“如果一列火车以每小时 60 英里的速度行驶 3 小时，它能行驶多远？”这样的问题，就需要一些简单的推理，因为模型需要先识别“距离=速度 × 时间”的关系，才能得出正确答案，如下图所示。

![[8.2.excalidraw|600x100]]

当前大多数大语言模型具备基本的推理能力，可以回答诸如“如果一列火车以每小时 60 英里的速度行驶 3 小时，它能行驶多远？”之类的问题。因此，当我们谈论“推理模型”时，通常指的是那些能处理更复杂推理任务（如解谜题、数学推导或证明）的大语言模型。

此外，目前大部分被称为推理模型的大语言模型在回答中会融入“思考”或“思维”过程。至于大语言模型是否真的在“思考”以及如何 “思考”，则是另一个值得探讨的问题。

推理模型的 **中间推理步骤** 主要以两种方式呈现（参见上图）。

- **直接体现在回答中** ，让用户看到完整的推理过程（参见下图）。
- **在内部进行多次迭代** ，但不会向用户展示推理过程（例如，OpenAI 的 o1 可能会进行多轮推理，但最终只呈现答案）。

![[8.3.excalidraw|1000x100]]

## 2. 何时应该使用推理模型

既然我们已经定义了推理模型，那么接下来可以进入更有趣的部分：如何为推理任务构建和改进大语言模型。不过，在深入探讨技术细节之前，重要的是要考虑何时真正需要推理模型。

那么，我们何时需要推理模型呢？推理模型适用于需要多步推理的复杂任务，比如解谜题、高级数学推导和解决复杂的编程问题。然而，对于总结、翻译、基于知识的问答等简单任务，推理模型并非必需。事实上，如果无差别地在所有任务中都使用推理模型，则可能导致效率低下，并且会带来不必要的开销。这是因为推理模型通常使用成本更高、输出更冗长，有时可能因“过度思考”而更容易出错。因此，我们可以遵循一个简单的规则：根据任务类型选择合适的工具（或大语言模型类型）。

下图总结了推理模型的核心优势和局限性。

| 擅长                  | 不擅长               |
| ------------------- | ----------------- |
| 演绎或者归纳推理（如解谜题、数学证明） | 快速且粗略的回答（更多的推理时间） |
| 思维链式推理（分解多步骤问题）     | 基于知识的任务（容易产生幻觉）   |
| 复杂决策任务              | 简单任务（过度思考）        |
| 对新问题有更好的泛化能力        |                   |

## 3. 简要介绍 DeepSeek R1 的训练流程

在讨论构建和优化推理模型的四大核心方法之前，我想先简要介绍一下 DeepSeek R1 训练流程，该流程在 DeepSeek R1 技术报告“DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning”中进行了详细描述。这份报告既是一个有趣的案例研究，也可以作为开发推理模型的蓝图。

需要注意的是，DeepSeek 并未发布单一的 R1 推理模型，而是推出了 3 个不同的变体：DeepSeek-R1-Zero、DeepSeek-R1 和 DeepSeek-R1-Distill。

根据技术报告中的描述，我在下图中总结了这些模型的开发流程。

![[8.4.excalidraw|1000x100]]

接下来，我们简要回顾一下上图所示的过程。更多的细节后面会讨论，届时我们将探讨构建和优化推理模型的四大核心方法。

(1) DeepSeek-R1-Zero：该模型基于 2024 年 12 月发布的参数量为 6710 亿的 DeepSeek-V3 预训练基础模型构建。研究团队采用强化学习（RL）进行训练，并使用了两种奖励机制。这种方法被称为“冷启动”训练，因为它没有进行监督微调（SFT），而监督微调通常是基于人类反馈的强化学习（RLHF）的一部分。

(2) DeepSeek-R1：这是 DeepSeek 的主力推理模型，基于 DeepSeek-R1-Zero 构建。团队在 DeepSeek-R1-Zero 的基础上增加了额外的监督微调训练阶段，并继续使用强化学习进行训练，进一步提升了 R1-Zero 这一“冷启动”模型的能力。

(3) DeepSeek-R1-Distill ：在前面的训练过程中产生了大量监督微调数据，DeepSeek 团队利用这些数据对 Qwen 和 Llama 进行了微调，以增强其推理能力。尽管这不是传统意义上的蒸馏，但该过程涉及使用参数量为 6710 亿的 DeepSeek-R1 生成的输出来训练较小规模的模型（包括参数量为 80 亿或 700 亿的 Llama 以及参数量为 15 亿~300 亿的 Qwen）。

## 4. 构建和优化推理模型的四大核心方法

下面我将介绍当前用于提升大语言模型推理能力和构建专门的推理模型（如 DeepSeek-R1、 OpenAI 的 o1 和 o3 等）的关键技术。

```ad-note
o1和o3的具体工作原理目前尚未公开，不过外界普遍猜测它们结合了推理和训练方面的优化策略。
```

### 4.1 推理时间扩展

提升大语言模型推理能力的一种方法是推理时间扩展（inference-time scaling）。这个术语在不同背景下可能有不同的含义，但在这里，它指的是增加推理时的计算资源来提高模型输出的质量。

一个简单的类比是，人类在面对复杂问题时，如果能够多花一些时间去思考，那么通常就会找到更好的解决办法。同样，我们可以应用一些策略，促使大语言模型在生成答案时进行更多的“思考”。（当然，大语言模型是否真正能“思考”是另一个话题。）

一种简单的推理时间扩展方法是提示词工程（prompt engineering），其中最典型的例子是思维链（chain-of-thought，CoT）提示。如下图所示，在思维链提示中，我们会在输入提示词（prompt） 中加入类似“一步步思考”（think step by step）这样的短语，鼓励模型先生成中间推理步骤，而不是直接输出最终答案。这种方法在处理复杂问题时通常（但并非总是）能带来更准确的结果。（需要注意的是，并不是所有问题都适合使用这种策略。例如，对于“法国的首都是哪里？”这种单纯的知识性问题，使用思维链提示是没有意义的。这也是一个实用的判断标准：如果一个问题本身不涉及推理，那么针对它优化推理模型是没有必要的。）

![[8.5.excalidraw|1000x100]]

上述思维链方法可以被看作一种推理时间扩展，因为它通过生成更多的输出token来增加推理的计算成本。

另一种推理时间扩展方法是使用投票和搜索算法。一个简单的例子是多数投票法，即让大语言模型生成多个答案，然后我们通过多数投票来选出最有可能的正确答案。同样，我们还可以使用束搜索（beam search）或其他搜索算法来生成更优质的答案。

![[8.6.excalidraw|1000]]

根据 DeepSeek R1 技术报告，其模型未使用推理时间扩展技术。然而，这种技术通常是在大语言模型的应用层中实现，因此 DeepSeek 有可能在其应用程序中使用了此技术。

我猜测 OpenAI 的 o1 模型和 o3 模型使用了推理时间扩展技术，这也解释了为什么与 GPT-4o 等模型相比，它们的成本更高。除了推理时间扩展，o1 和 o3 很可能还使用了类似于 DeepSeek R1 所采用的强化学习流水线进行训练。关于强化学习的更多内容，接下来我们会详细介绍。

### 4.2 纯强化学习

我个人认为 DeepSeek R1 技报告“DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning”中的一大亮点是：DeepSeek 团队发现推理能力作为一种行为可以通过纯 强化学习自发涌现。让我们更详细地探讨一下这意味着什么。

正如之前提到的，DeepSeek 开发了 3 种类型的 R1 模型。第一个模型 DeepSeek-R1-Zero 是基于 DeepSeek-V3 基础模型构建的。DeepSeek-V3 是 DeepSeek 在 2024 年 12 月发布的标准预训练大语言模型。与典型的强化学习流程不同（通常在强化学习之前会先进行监督微调）， DeepSeek-R1-Zero **完全** 通过强化学习进行训练，没有经历初始的监督微调阶段，如下图所示。

![[8.7.excalidraw|1000]]

然而，这种强化学习流程与通常用于对大语言模型进行偏好微调的基于人类反馈的强化学习方法类似。但是，如上所述，DeepSeek-R1-Zero 的关键区别在于它跳过了监督微调阶段，这也是为什么其被称为“纯”RL。（不过需要注意的是，大语言模型中的强化学习与传统意义上的强化学习有很大不同，这是另一个值得单独探讨的话题。）

在奖励机制方面，与通过人类偏好训练的奖励模型不同，DeepSeek 团队采用了两种奖励方 式：准确性奖励和格式奖励。

- **准确性奖励**：通过使用 LeetCode 编译器来验证代码答案的正确性，并通过一个确定性的系统来评估数学答案的准确性。
- **格式奖励**：依赖大语言模型来确保回答遵循预期的格式，比如在 `<think>` 标签内放置推理步骤。

令人惊讶的是，仅凭这种方法，大语言模型就已经具备了基本的推理能力。研究人员在模型开始生成推理过程的回答时，发现了一个“Aha”时刻，尽管模型没有明确接受此类训练，如下图所示。

![[image7.png]]

==“Aha”时刻的出现（来自 DeepSeek R1 技术报告）==

虽然 R1-Zero 并不是表现最优秀的推理模型，但它通过生成中间的“思考”步骤展示了推理能力，如上图所示。这证实了使用纯强化学习开发推理模型是可行的，而 DeepSeek 团队是首个展示（或至少是公开发表）这一方法的团队。

### 4.3 监督微调+强化学习（SFT+RL）

接下来，我们来看 DeepSeek 的主力推理模型——DeepSeek-R1 的开发过程。DeepSeek-R1 为构建推理模型提供了蓝图，相较于 DeepSeek-R1-Zero，它通过额外的监督微调和强化学习进一步提升了推理性能，如下图所示。

值得注意的是，在强化学习之前加入监督微调阶段实际上是很常见的做法，比如标准的基于人类反馈的强化学习（强化学习+人类反馈）流程中就包含这一阶段。OpenAI 的 o1 模型很可能采用了类似的开发方法。

如上图所示，DeepSeek 团队使用 DeepSeek-R1-Zero 生成了他们所称的“冷启动”监督微调数据。“冷启动”是指这些数据是由 DeepSeek-R1-Zero 模型生成的，而该模型本身并未接受任何监督微调数据的训练。

在获得这些“冷启动”监督微调数据后，DeepSeek 团队对模型进行了指令微调（instruction fine-tuning），随后又进行了一个强化学习阶段。这个强化学习阶段沿用了 DeepSeek-R1-Zero 中的奖励机制，包括准确性奖励（验证数学和代码问题的正确性）和格式奖励（确保输出符合预期格式）。除此之外，他们还新增了一个一致性奖励，以避免模型在回答中混用多种语言的问题。

在强化学习阶段之后，他们进行了新一轮的监督微调数据收集。在这一阶段中，他们使用最新的模型检查点（checkpoint）生成了 60 万条思维链监督微调样本，同时还基于 DeepSeek-V3 基础模型生成了 20 万条基于知识的监督微调样本。

随后，这 80（20+60）万条监督微调数据被用于指令微调 DeepSeek-V3 基础模型，然后又进行最后一轮的强化学习训练。在这一阶段，他们继续使用基于规则的方法对数学和编程问题的答案给予准确性奖励，而对其他类型的问题引入了基于人类偏好标签的奖励机制。总体而言，这一过程与常规的基于人类反馈的强化学习非常相似，不同之处在于监督微调数据中包含了更多的思维链示例。此外，除了基于人类偏好的奖励，强化学习还引入了可验证的奖励机制。

得益于额外的监督微调和强化学习阶段，最终生成的模型——DeepSeek-R1，在性能上相较于 DeepSeek-R1-Zero 有了显著提升，具体表现可以参考下图中的数据对比。

![[8.8.excalidraw|1000]]

值得注意的是，在强化学习之前加入监督微调阶段实际上是很常见的做法，比如标准的基于人类反馈的强化学习（强化学习+人类反馈）流程中就包含这一阶段。OpenAI 的 o1 模型很可能采用了类似的开发方法。

如上图所示，DeepSeek 团队使用 DeepSeek-R1-Zero 生成了他们所称的“冷启动”监督微调数据。“冷启动”是指这些数据是由 DeepSeek-R1-Zero 模型生成的，而该模型本身并未接受任何监督微调数据的训练。

在获得这些“冷启动”监督微调数据后，DeepSeek 团队对模型进行了指令微调（instruction fine-tuning），随后又进行了一个强化学习阶段。这个强化学习阶段沿用了 DeepSeek-R1-Zero 中的奖励机制，包括准确性奖励（验证数学和代码问题的正确性）和格式奖励（确保输出符合预期格式）。除此之外，他们还新增了一个一致性奖励，以避免模型在回答中混用多种语言的问题。

在强化学习阶段之后，他们进行了新一轮的监督微调数据收集。在这一阶段中，他们使用最新的模型检查点（checkpoint）生成了 60 万条思维链监督微调样本，同时还基于 DeepSeek-V3 基础模型生成了 20 万条基于知识的监督微调样本。

随后，这 80（20+60）万条监督微调数据被用于指令微调 DeepSeek-V3 基础模型，然后又进行最后一轮的强化学习训练。在这一阶段，他们继续使用基于规则的方法对数学和编程问题的答案给予准确性奖励，而对其他类型的问题引入了基于人类偏好标签的奖励机制。总体而言，这一过程与常规的基于人类反馈的强化学习非常相似，不同之处在于监督微调数据中包含了更多的思维链示例。此外，除了基于人类偏好的奖励，强化学习还引入了可验证的奖励机制。

得益于额外的监督微调和强化学习阶段，最终生成的模型——DeepSeek-R1，在性能上相较于 DeepSeek-R1-Zero 有了显著提升，具体表现可以参考下图中的数据对比。

![[8.9.excalidraw|1000]]

### 4.4 纯监督微调和蒸馏

到目前为止，我们已经讨论了构建和改进推理模型的 3 种关键方法。

- **推理时间扩展**：一种在不修改或重新训练基础模型的情况下提升推理能力的技术。
- **纯强化学习**：DeepSeek-R1-Zero 展示了推理可以作为学习行为从无监督微调中涌现。
- **监督微调+强化学习**：这是 DeepSeek-R1 推理模型的开发方法。

那么，接下来是什么呢？答案“蒸馏”。

令人惊讶的是，DeepSeek 还发布了通过蒸馏过程训练的小型模型。然而，在大语言模型的背景下，蒸馏并不一定遵循深度学习中的传统知识蒸馏方法。在传统的知识蒸馏中，较小的“学生模型”会在较大的“教师模型”的 logits 和目标数据集上进行训练。

然而，DeepSeek 的蒸馏方法是通过使用 DeepSeek-V3 和 DeepSeek-R1 的中间检查点生成的监督微调数据集，来对较小的大语言模型（如参数量为 80 亿或 700 亿的 Llama 模型以及参数量为 5 亿~320 亿的 Qwen 2.5 模型）进行指令微调。值得注意的是，这个蒸馏过程中使用的监督微调数据集与训练 DeepSeek-R1 时使用的数据集完全相同。

为了更清楚地说明这个过程，我在下图中突出了蒸馏的部分。

![[8.10.excalidraw|1000]]

为什么 DeepSeek 团队要开发这些蒸馏模型？我认为有两个主要原因。

- **小型模型具有更高的效率**。这意味着它们运行成本更低，同时还可以在低端硬件上运行， 这对许多研究人员和像我这样的技术爱好者来说特别具有吸引力。
- **纯监督微调的案例研究**。这些蒸馏模型作为一个有趣的基准，展示了在没有强化学习的 情况下，纯监督微调能将模型提升到什么程度。

下图将这些蒸馏模型的性能与其他流行模型、DeepSeek-R1-Zero 和 DeepSeek-R1 进行了比较。

![[image9.png]]

==蒸馏模型与非蒸馏模型的基准比较（来自 DeepSeek-R1 技术报告）==

从上图可以看出，蒸馏模型的表现明显不如 DeepSeek-R1，但与 DeepSeek-R1-Zero 相比，尽管它们的规模小得多，表现却相当强劲。此外，值得注意的是，这些模型与 o1 mini 相比表现得相当不错（我怀疑 o1-mini 本身可能是 o1 的蒸馏版本）。

在总结这一部分之前，还有一个有趣的比较值得一提。DeepSeek团队测试了 DeepSeek-R1-Zero 中出现的推理行为是否也能在较小的模型中出现。为了验证这一点，他们将 DeepSeek-R1-Zero 中使用的纯强化学习方法直接应用于 Qwen-32B。

实验结果如下图所示，其中 QwQ-32B-Preview 是基于 Qwen 2.5-32B 开发的推理模型（我认为其训练细节并未公开）。这个比较为我们进一步探讨纯强化学习方法是否能在远小于 DeepSeek-R1-Zero 的模型中激发推理能力提供了一些额外的见解。

![[image4.png]]

==在一个较小的参数量为 320 亿的模型上进行的蒸馏与纯强化学习的基准比较（来自 DeepSeek-R1 技术报告）==

有趣的是，结果表明，对于较小的模型，蒸馏远比纯强化学习有效。这与以下观点一致：仅靠纯强化学习可能不足以在这种规模的模型中引发强大的推理能力，而在处理小模型时，使用高质量推理数据进行监督微调可能是一种更有效的策略。

为了使上图更加完整，增加以下对比数据将会更有帮助。

- Qwen-32B 使用监督微调+强化学习进行训练，类似于 DeepSeek-R1 的开发方式。这样可以帮助我们了解，当强化学习与监督微调结合时，与纯强化学习和纯监督微调相比，能带来多大的改进。
- DeepSeek-V3 使用纯监督微调进行训练，类似于蒸馏模型的创建方式。这样能让我们直接比较强化学习+监督微调与纯监督微调二者的效果。

## 4.5 结论

在这一部分中，我们探讨了构建和优化推理模型的 4 种策略。

- **推理时间扩展**。无须额外训练，但会增加推理成本，随着用户数量或查询量的增加，大规模部署成本会变得更加昂贵。尽管如此，对提升已经很强大的模型的性能来说，它仍然是一种非常直观的方法。我强烈怀疑 o1 采用了推理时间扩展方法，这也能解释为什么它在每个token的计算成本上比 DeepSeek-R1 更高。
- **纯强化学习**。从研究的角度来看，纯强化学习非常有趣，因为它提供了将推理视为一种涌现行为的深刻见解。然而，在实际的模型开发中，强化学习+监督微调是首选方法，因为它能构建更强大的推理模型。我强烈怀疑 o1 也采用了强化学习+监督微调的方法。更确切地说，我认为 o1 是从比 DeepSeek-R1 更弱、更小的基础模型开始的，但通过强化学习+监督微调以及推理时间扩展弥补了这一不足。
- **强化学习+监督微调**。正如前文所述，强化学习+监督微调是构建高性能推理模型的关键方法。DeepSeek-R1 是一个很好的蓝图，展示了如何实现这一方法。
- **蒸馏**。蒸馏是一种非常棒的方法，特别适用于创建更小、更高效的模型。然而，蒸馏的局限性在于，它并不能推动创新或生产下一代推理模型。例如，蒸馏总是依赖现有的、 更强大的模型来生成监督微调数据。

我期待接下来能看到一个有趣的结合——将强化学习+监督微调（方法三）与推理时间扩展 （方法二）结合起来。这很可能就是 OpenAI o1 正在做的，只不过它可能是基于一个比 DeepSeek-R1 更弱的基础模型。这就解释了为什么 DeepSeek-R1 在推理成本较低的情况下仍能表现如此出色。

## 5. 关于 DeepSeek R1 的思考

简而言之，我认为这是一次了不起的成就。作为一名研究工程师，我特别欣赏他们发布的详细技术报告，这份报告中提供的研究方法和思路让我受益匪浅。

最令人着迷的一点是，推理行为是如何从纯强化学习中涌现出来的。而且，DeepSeek 已将其模型开源，并采用了 MIT 开源许可协议，这甚至比 Meta 的 Llama 模型的限制还要少。

### 1. 它与 o1 相比如何

那么，DeepSeek-R1 比 o1 更强吗？我认为它们大致处于同一水平。不过，DeepSeek-R1 在推理时效率更高，这表明 DeepSeek 可能在训练过程中投入了更多的精力，OpenAI 则更多依赖推理时间扩展技术来优化 o1。

话虽如此，但直接对比这两个模型是很困难的，因为 OpenAI 并未公开过多关于 o1 的信息。例如，我们并不清楚：

- o1 是否采用了混合专家模型（MoE）
- o1 的规模有多大
- o1 是否只是一个稍微改进的 GPT-4o 版本，仅经过了最小程度的强化学习+监督微调，而主要依赖于大规模的推理时间扩展

在不了解这些细节的情况下就直接进行比较，就像拿苹果与橙子做对比一样——根本没有可 比性。

### 2. DeepSeek-R1 的训练成本

大家讨论的另一个焦点是DeepSeek-R1的开发成本。有些人提到训练成本大约为600万美元， 但这可能是将 DeepSeek-V3（2024 年 12 月发布的基础模型）与 DeepSeek-R1 混淆了。

600 万美元的估算是基于每 GPU 小时 2 美元，并计算了 DeepSeek-V3 最后一次训练所需的 GPU 小时数，该数据最早在 2024 年 12 月讨论过。

然而，DeepSeek 团队从未公开过 R1 的具体 GPU 小时数或开发成本，所以任何成本估算都只能是纯粹的猜测。

无论如何，DeepSeek-R1 毫无疑问是开放权重推理模型的一个重要里程碑，而且它在推理时的高效性使其成为 OpenAI o1 的一个有趣的替代方案。

## 6. 在有限预算下开发推理模型

开发像 DeepSeek-R1 这样的推理模型可能需要数十万到数百万美元，即使是从开源基础模型 DeepSeek-V3 开始。这对预算有限的研究人员或工程师来说，可能会感觉有些沮丧。

### 1. 好消息是：蒸馏是一种行之有效的方法

幸运的是，模型蒸馏提供了一种更加经济的替代方案。DeepSeek 团队通过 R1 蒸馏模型证明了这一点，尽管这些蒸馏模型比 DeepSeek-R1 小得多，但实现了令人惊讶的强大推理性能。然而，即便是这种方法也并非完全经济实惠。他们的蒸馏过程使用了 80 万个监督微调样本，这需要相当可观的计算资源。

有趣的是，就在 DeepSeek-R1 发布的前几天，我偶然读到了一篇关于 Sky-T1 项目的文章 （“Sky-T1: Train your own O1 preview model within $450”），讲的是一个小团队仅用 1.7 万个监督微调样本就训练出了一个开源的参数量为 320 亿的模型。那么，总成本是多少呢？仅需 450 美元， 甚至比大多数人工智能会议的注册费还要低。

这个例子表明，虽然大规模训练依然成本高昂，但较小规模、有针对性的微调仍能以较低的成本取得令人印象深刻的成果。下图是文章“Sky-T1: Train your own O1 preview model within $450” 中的一张演示图片。

![[image16.png]]

根据他们的基准测试，Sky-T1 的表现大致与 o1 相当，考虑到其低廉的训练成本，这一点令 人印象非常深刻。

### 2. 有限预算下的纯强化学习：TinyZero

虽然 Sky-T1 主要聚焦于模型蒸馏，但我也发现了一些在“纯强化学习”领域中的有趣工作。一个值得注意的例子是 TinyZero，这是一个复制 DeepSeek-R1-Zero 方法的参数量为 30 亿的模型 （顺便说一下，它的训练成本不到 30 美元）。

令人惊讶的是，尽管只有 30 亿个参数，但 TinyZero 展示出了一些自我验证的能力，这支持了“推理可以通过纯强化学习在小模型中涌现出来”的观点。

下图是来自 TinyZero 代码库的一个例子。

![[image12.png]]

==来自 TinyZero 代码库，展示了该模型具备自我验证的能力（如果能将基础模型的响应 与之进行对比会更好）==

以上提到的两个项目表明，即使在有限预算下，仍然可以进行有趣的推理模型研究。虽然这两种方法都借鉴了 DeepSeek-R1 的方法，但一种专注于纯强化学习（TinyZero），另一种则专注于纯监督微调（Sky-T1），而如何进一步扩展这些思路将是一个值得深入探索的课题。

