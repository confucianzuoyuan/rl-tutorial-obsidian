## 1 基本概念

**强化学习（reinforcement learning，RL）** 讨论的问题是智能体（agent）怎么在复杂、不确定的环境（environment）中最大化它能获得的奖励。

![[1.1.excalidraw]]
- Agent：智能体
- Environment：环境
- State：状态
- Reward：奖励
- Action：动作

强化学习由两部分组成：智能体和环境。在强化学习过程中，智能体与环境一直在交互。智能体在环境中获取某个状态后，它会利用该状态输出一个动作 （action）。然后这个动作会在环境中被执行，环境会根据智能体采取的动作，输出下一个状态以及当前这个动作带来的奖励。智能体的目的就是尽可能多地从环境中获取奖励。

下面是一个倒立摆环境。

![[1.2.png]]

在这个环境中，智能体是推车。

- 动作空间：推车有两个动作，**向左推** 和 **向右推**
- 状态：
	- 推车的位置
	- 推车的速度
	- 杆子的角度
	- 杆子的角速度
- 奖励：推车采取向左推或者向右推的动作之后，只要杆子不倒下，奖励就是 `1` 。

游戏的结束条件：

- 推车将杆子推出了屏幕（失败）
- 杆子倒下（失败）
- 采取了 200 次推车的动作，杆子也没倒下（成功）

那么推车应该采取什么样的动作，杆子才能不倒下呢？或者说，推车应该采取什么样的 **策略（policy）** ，杆子才能不倒下呢？因为只有杆子不倒下，我们才能不停的获得奖励。

如果推车每采取一个动作，杆子还能保持平衡，那么我们获得的 **即时奖励** 是 1 。

如果到游戏结束，杆子也没有倒下，那么我们将获得最大的奖励 200 。

而推车每次要采取什么动作，是取决于环境的状态的。也就是推车要根据环境的状态来决定下一步采取什么动作。例如：此时杆子向左偏，那么推车可能需要采取向左推的动作，才能使杆子不倒下。

换句话说，推车要采取的策略是一个 **函数** 。

$$
\text{要采取的动作} = \textcolor{red}{\text{策略函数}}(环境的状态)
$$

而这里 **要采取的动作** 一般来说是一个概率分布。例如：向左推的概率=0.7，向右推的概率=0.3 。

然后从这个分布中进行采样，采样一个动作出来。当然也可以直接选概率最大的动作。

而我们使用什么样的策略呢？当然是能够让我们在游戏结束时，能够获得最大奖励的策略啦。

当然，这里选择策略有很多的讲究。如果我们每次采取的都是概率最大的动作，那么每次可能都会获得最大的即时奖励。如果这样做的话，可能到了一定的时候，不管向左推还是向右推获得的即时奖励可能都是 0 ，也就是不管怎么推，杆子都会倒下（杆子偏离的角度太大，怎么推都没用）。也就是说，我们太看重眼前利益，忽略了长远的利益。这样就是只 **利用** 而不 **探索** 。所以有时也需要采取一下概率低的动作，探索一下环境，万一未来获得的奖励更多呢。

所以偶尔也需要采取一下概率小的动作。

```ad-note
所谓从 **概率分布中采样** 的意思是，例如概率分布是 向左推 的概率是 0.7 ，向右推 的概率是 0.3 。那么从这个概率分布中采样一个动作出来，有 70% 的概率采取的动作是向左推。但也有 30% 的概率向右推。
```

举个例子：如果我们每次都去最熟悉的餐馆吃饭，可能体验都还可以。而如果去不熟悉的餐厅吃饭，可能体验不好，也可能体验超过了之前的餐馆。在舒适区只利用不探索，就会固步自封。冒险可能会受到伤害，但长期来看，可能会得到提升。

策略是智能体的动作模型，它决定了智能体的动作。它其实是一个函数，用于把输入的状态变成动作。

策略的数学符号是 $\pi$ ，所以策略也就是 $\pi$ 函数。即 $\pi(a | s)=p\left(a_{t}=a | s_{t}=s\right)$ 。输入一个状态 $s$ ，输出一个概率分布。用条件概率来看，就是在条件：状态为 $s$ 的情况下，策略采取动作 $a$ 的概率是多少。这个概率是智能体所有动作的概率，然后对这个概率分布进行采样，可得到智能体将采取的动作。比如可能是有 0.7 的概率往左，0.3 的概率往右，那么通过采样就可以得到智能体将采取的动作。

![[1.3.excalidraw|600x100]]

所以有如下：

$$
\begin{aligned}
\pi(a=a_{\text{向左推}}|s)=0.7 \\
\pi(a=a_{\text{向右推}}|s)=0.3
\end{aligned}
$$

如果这个 $\pi$ 函数是一个神经网络，那么这就是 **深度学习 + 强化学习 = 深度强化学习** 。当今 AI 界最热门的话题。

所以玩倒立摆游戏的一个 **回合（episode）** 就是环境的状态为 $S_0$ ，推车采取动作 $A_0$ ，然后获得奖励 $R_0$ ，然后环境的状态转移到了 $S_1$ ，推车接着采取动作 $A_1$ ，然后获得奖励 $R_1$ ...。下标是时刻，或者时间步。把它们写到一起，就是一个 **轨迹（trajectory）** 。轨迹用数学符号 $\tau$ 表示，读作 `掏` 。

$$
\boxed{
\tau = (S_0,A_0,R_0,S_1,A_1,R_1,S_2,A_2,R_2\dots)
}
$$

由于根据环境的状态，采取的动作是从一个概率分布中采样得到的，所以轨迹会有很多很多条。

![[1.4.excalidraw|600x100]]

## 2 价值函数

当位于时刻 $t$ 时，环境此时处于状态 $S_t$ ，然后我们根据策略函数开始采取动作，那么未来我们一共能获得多少奖励呢？环境处于状态 $S_t$ ，我们采取的动作是 $A_t$ ，获取的奖励是 $R_t$ ，然后环境的状态从 $S_t$ 转移到了 $S_{t+1}$ ，然后采取动作 $A_{t+1}$ ，然后获得即时奖励 $R_{t+1}$ ，然后环境的状态从 $S_{t+1}$ 转移到了 $S_{t+2}$ ，然后环境会给我们即时奖励 $R_{t+2}$ ，......。

但是未来的奖励不如现在的奖励有吸引力，所以需要 **打折** 。那么，从 $t$ 时刻起，未来一共获得的奖励叫做 **回报（或者收益）** 。

$$
\boxed{
G_t = R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + \cdots
}
$$
$\gamma$ 叫做折扣因子。随着时间的推移，奖励会被 $\gamma$ 指数级削弱。这个 $\gamma$ 被称为折扣因子（discount rate），其被设定为 $0.0$ 和 $1.0$ 之间的实数。如果折扣因子是 $0.9$，那么有以下式子成立。

$$
G_t = R_t + 0.9 R_{t+1} + 0.81 R_{t+2} + \cdots
$$

引入折现率主要是为了防止连续性任务的收益变得无穷大。在连续性任务中，如果没有折扣因子（或 $\gamma=1$ ），那么收益就会发散到无穷大 。因此，设置折扣因子可以防止收益的发散。

折扣因子也使近期的奖励显得更加重要。这解释了人类乃至生物的许多行动原理。例如，你会选择今天拿到 10000 元还是一年后拿到 20000元？如果折扣因子使未来的回报呈指数级下降，那么眼前的回报就会更有吸引力。

如果我们用倒立摆作为例子，然后我们运行两个时间步。得到下图。

![[1.5.excalidraw]]
可以看到，一共有 4 条轨迹。每条轨迹都有一个总的回报。而每条轨迹也都有一个产生的概率。那么我们如何评估在环境处于状态 $S_t$ 时，一直采取策略 $\pi$ 未来会获得多少回报呢？也就是未来的预期回报（回报的期望值）是多少呢？那就是 **状态价值函数** 。

$$
\boxed{
V_{\pi}(s) = \mathbb{E}_{\pi}\left[G_{t} \mid S_{t}=s\right]=\mathbb{E}_{\pi}\left[\sum_{k=0}^{\infty} \gamma^{k} R_{t+k} \mid S_{t}=s\right], \text{对于所有的} s \in S
}
$$

```ad-note
状态价值函数，衡量的是在环境处于状态 $s$ 时，一直按照策略 $\pi$ 来采取动作，最终的预期回报。
```

状态价值函数的另一种重要的表示形式。

$$
\boxed{
\begin{aligned}
V_\pi(S_t)
&= \mathbb{E}_\pi[R_t+\gamma V_\pi(S_{t+1})|S_t] \\
\end{aligned}
}
$$

```ad-note
$$
\begin{aligned}
V_\pi(S_t)
&= \mathbb{E}_\pi[R_t+\gamma V_\pi(S_{t+1})|S_t] \\
\end{aligned}
$$

也叫做 **贝尔曼期望方程**
```

直观解释：

当前环境对你的价值 = 当前环境现在给你的奖励 + 折扣因子 x 未来环境对你的价值

你对公司的评估 = 现在公司给你的薪资 + 折扣因子 x 你对公司未来的评估

也就是你觉得现在公司怎么样？得看公司现在给你开的钱和你觉得公司未来的前景，综合考虑。

现在给的钱少，但是你觉得公司前景不错（折扣因子0.99），那么你对公司的评估也不错。

现在给的钱很多，但是你觉得公司前景会倒闭（折扣因子0.01，但如果不倒闭公司未来前景家这也很不错，但是倒闭的概率很大），那么你对公司的评估也不错。

还有一种可能，现在钱很少，公司倒闭的可能性很大，但一旦公司每倒闭，就很牛逼。

## 3 倒立摆环境编程实践

我们使用 OpenAI 编写的 Gym 库。

```bash
$ pip install gym==0.25.2
$ pip install pygame
```

然后我们来创建一个倒立摆环境

```python
import gym

env = gym.make('CartPole-v0')
```

这样就生成了倒立摆的环境。

![[1.2.png]]

将推车向右或向左移动，以保持杆子的平衡。倒立摆的结束条件是杆子的平衡被打破（杆子超过一定的角度），或者推车的移动位置超出了某个范围。

> [!NOTE]
> 倒立摆环境包含版本0（CartPole-v0）和版本1（CartPole-v1）。版本 0 的上限为 200 步。如果能在 200 步之内保持平衡，那么游戏就结束了。版本 1 的上限为 500 步。

下面继续执行以下代码。

```python
state = env.reset()
print(state) # 初始状态
action_space = env.action_space
print(action_space) # 行动的维度
```

上面的代码通过 `state = env.reset()` 获得了初始状态。观察它的输出，你会发现它是拥有 4 个元素的数组。作为参考，下面依次列出这 4 个元素。

- 推车的位置
- 推车的速度
- 杆子的角度
- 杆子的角速度

另外，我们可以通过 `env.action_space` 获得行动的维度（可采取的行动数）。它的输出是一个名为 `Discrete(2)` 的类实例。这意味着有两个候选行动。具体来说，0对应的是向左移动推车的行动，1对应的是向右移动推车的行动。下面实际地采取行动，向前推进一个时间步。

```python
action = 0 # 或者 1
next_state, reward, done, info = env.step(action)
print(next_state)
```

上面的代码通过 `env.step(action)` 采取行动。作为结果 ，我们得到了以下 4 个信息。

- 下一个状态（`next_state`）
- 奖励（`reward`）
- 是否结束的标志位（`done`）
- 附加信息（`info`）

`reward` 是标量值（`float`）。这次的任务在保持平衡的时候总是会得到奖励 `1` 。`info` 包含有助于调试的信息（如环境模型）。但在实现和评估强化学习的算法时，基本不会用到 `info` 。

我们先来实现一个随机智能体。也就是这个智能体的策略非常简单，无论环境处于什么状态，我们都是从两个动作中随机采样一个动作。

```python
import numpy as np
import gym
import time

env = gym.make('CartPole-v0')
state = env.reset()
done = False
action_desc = {0: "向左推", 1: "向右推"}
total_reward = 0.0
reward_list = []
gamma = 0.99

while not done:
	env.render()
	action = np.random.choice([0, 1])
	next_state, reward, done, info = env.step(action)
	print(f"采取动作：{action_desc.get(action, '未知动作')}，获得奖励：{reward}，转移到下一个状态：{next_state}")
	reward_list.append(reward)
	time.sleep(0.1)

for r in reward_list[::-1]:
    total_reward = r + gamma * total_reward

print('回报（收益）：', total_reward)
env.close()
```

```ad-note
可以试一下先向左推，再向右推，再向左推，... 这种策略怎么样？
```

如下图所示

![[1.6.excalidraw|600x100]]

由于我们使用的是 **随机策略**，所以倒立摆游戏很快就结束了。

## 4. 马尔可夫决策过程

### 基本概念

马尔可夫决策过程（MDP，Markov Decision Process）。决策过程是智能体通过与环境互动决定其行动的过程。

智能体所处的情况根据其行动而发生改变。在强化学习中，这种情况被称为“状态”（state）。在MDP中，状态的变化取决于智能体的行动 ，智能体在状态迁移后执行新的行动。

在MDP中，我们需要“时间”的概念。在某一时刻，智能体会采取行动并因此迁移到一个新的状态。此时的时间单位叫作“时间步”。由于时间步是智能体做出决定的间隔时间，因此它的实际单位取决
于问题。

智能体要考虑的是将来获得的奖励总和，而不是眼前的奖励。换句话说，智能体的目标是实现奖励总和最大化。

![[1.1.excalidraw|1000]]

在MDP中，智能体与环境之间会进行互动。要点在于当智能体采取行动时，状态会发生迁移，随之获得的奖励也会相应改变。

假设在时刻 $t$ 的状态是 $S_t$ 。基于这个状态 $S_t$ ，智能体执行行动 $A_t$ ，获得奖励 $R_t$ 并迁移到下一个状态 $S_{t+1}$ 。智能体与环境之间的这种实际的互动产生了以下迁移。

$$
S_0,A_0,R_0,S_1,A_1,R_1,S_2,A_2,R_2,\cdots
$$

这个时间序列数据从第一个状态 $S_0$ 开始。在状态 $S_0$ ，智能体执行行动 $A_0$ 并获得奖励 $R_0$ 。时刻变为 $1$ ，状态变为 $S_1$ 。接下来，基于状态 $S_1$ ，智能体执行行动 $A_1$ 并获得奖励 $R_1$，然后进入下一个状态 $S_2\cdots\cdots$ 这个流程不断持续下去。

MDP通过数学式来表示智能体、环境以及二者之间的互动。要做到这一点 ，需要用数学式来表达以下 3 个要素。

- 状态迁移：状态如何迁移。
- 奖励：如何给予奖励。
- 策略：智能体如何决定行动。

**状态迁移**

假设智能体现在处于状态 $s$ 并执行了行动 $a$ ，那么迁移到下一个状态 $s'$ 的概率可以用如下方式表示。

$$
p(s'|s,a)
$$

竖杠 $\vert$ 的右侧是表示“条件”的概率变量。对于当前问题，条件对应于在状态 $s$ 选择了行动 $a$ 。在给定这两个条件的情况下，迁移到 $s'$ 的概率可以表示为 $p(s'|s,a)$ 。像 $p(s'|s,a)$ 这样的概率叫作状态迁移概率（state transition probability）。

$p(s'|s,a)$ 决定了下一个状态 $s'$ 只取决于当前状态 $s$ 和行动 $a$ 。

换句话说，状态迁移不需要过去的信息——此前处于什么状态以及执行了哪些行动。这个特性被称为 **马尔可夫性质** （Markov property）。

MDP通过假设马尔可夫性质的存在来模拟状态迁移和奖励。引入马尔可夫性质主要是为了使问题更容易解决。如果不假定马尔可夫性质，那么就必须考虑之前的所有状态和行动，而且组合的数量会呈指数级增长。

**奖励函数**

当智能体处于状态 $s$ ，执行行动 $a$，下一个状态是 $s'$ 时，得到的奖励由函数 $r(s,a,s')$ 定义。$r(s,a,s')$ 叫做 **奖励函数** 。

```ad-note
倒立摆环境中的奖励是确定性的，只要木杆不倒下，就给奖励 1 。

而通常情况下，奖励函数也是一个神经网络。
```

**策略**

策略表示智能体如何决定其行动。策略的关键在于它使得智能体仅根据当前状态来决定其行动。之所以说只基于当前状态就足够了，是因为环境的迁移是符合马尔可夫性质的。

环境的状态迁移只以当前状态 $s$ 和行动 $a$ 为条件来决定下一个状态 $s'$ ，而不需要先前的信息。同样，奖励也是基于当前状态 $s$ 、行动 $a$ 和迁移后的状态 $s'$ 来决定的。这意味着关于环境的所有必要信息都在当前状态中。因此，智能体只需基于当前状态即可决定其行动。

```ad-note
MDP的马尔可夫性质可以被看作是对环境而不是对智能体的约束。这意味着为了满足马尔可夫性质，环境需要保持某个“状态”。从智能体的角度来看，在当前状态下有足够的信息来做出最佳选择，所以它可以在此基础上采取行动。
```

智能体的行动是由随机性策略决定的，数学式如下所示。

$$
\pi(a|s)
$$

$\pi(a|s)$ 表示在状态 $s$ 下采取行动 $a$ 的概率。

我们已经成功地用数学式来表示状态迁移、奖励函数和策略。接下来让我们使用这三者来定义MDP的目标。

### MDP的目标

到目前为止，我们已经用数学式描述了环境和智能体的行动。简要回顾一下，智能体根据策略 $\pi(a|s)$ 采取行动。首先，它根据它的行动和状态迁移概率 $p(s'|s,a)$ 迁移到下一个状态。然后，它根据奖励函数 $r(s,a,s')$ 获得奖励。在这个框架内，MDP的目标是找到 **最优策略** （optimal policy）。最优策略是使收益最大化的策略。

**收益**

为了定义收益，我们思考这样一个场景：设时刻为 $t$ ，状态为 $S_t$ （其中 $t$ 是任意值）。然后，智能体根据策略 $\pi$ 执行行动 $A_t$ ，获得奖励 $R_t$ ，之后迁移到新状态 $S_{t+1}$，这个过程不断重复进行。在这种情况下，收益 $G_t$ 的定义如下所示。

$$
G_t = R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + \cdots
$$

$\gamma$ 为折扣因子。

**状态价值函数**

我们已经重新定义了“收益”。智能体的目标是使这种收益最大化。这里有一点需要注意，那就是智能体和环境的行动可能是“随机性”的。智能体可能随机地决定行动，状态也可能随机迁移。在这种情况下，获得的收益将呈现随机的特点。即使从相同的状态开始，不同回合的收益也随机变化。例如，某个回合的收益为 $10.4$ ，另一个回合的收益为 $8.7$ 。

为了处理这种随机行动，需要使用期望值或“收益的期望值”作为衡量标准。收益的期望值的数学式如下所示。

$$
v_\pi(s) = \mathbb{E}[G_t|S_t=s,\pi]\tag{1}
$$

我们指定的条件是状态 $S_t$ 为 $s$ 智能体的策略为 $\pi$ （其中时刻 $t$ 是任意值）。在这些条件下，智能体获得的收益的期望值为上面的公式。在这里，我们用特殊符号 $v_\pi(s)$ 来表示收益的期望值。就是 **状态价值函数** （state-value function）。

在公式的右侧，智能体的策略 $\pi$ 被作为条件给出。这是因为如果策略 $\pi$ 发生变化，那么智能体获得的奖励也会发生变化，而这些奖励的总和，即收益也会发生变化。为了明确这一点，状态价值函数通常会写作 $v_\pi(s)$ ，将 $\pi$ 写在 $v$ 的右下角。此外，上面的公式也可以写成下面的形式。

$$
v_\pi(s) = \mathbb{E}_\pi[G_t|S_t=s]\tag{2}
$$

在上面的式子中，记载 $\pi$ 的位置是 $\mathbb{E}_\pi$ 。与式（1）一样，这么写的含义是策略 $\pi$ 是作为条件给出的。从现在起，本教程将采取式（2）的风格来书写式子。

在强化学习中，我们的目标是获得最优策略。

最优策略的状态价值函数叫作 **最优状态价值函数**（optimal state-value function）。可以使用 $v_*$ 来表示最优状态价值函数。

## 5. 贝尔曼方程

贝尔曼方程是在MDP中成立的最重要的方程，为许多强化学习算法提供了重要基础。

这里使用骰子作为例子。我们使用的骰子是理想的六面体，每一面的数字出现的概率都是 $\frac{1}{6}$ 。在数学式中，我们用随机变量 $x$ 表示掷骰子出现的数字， $x$ 是 $1$ 和 $6$ 之间的整数。那么每个数字出现的概率就是 $\frac{1}{6}$ 。 我们用 $p(x)=\frac{1}{6}$ 表示骰子的数字出现的概率。现在来计算掷骰子的期望值。计算式如下所示。

$$
\begin{aligned}
\mathbb{E}[x] &= 1\times\frac{1}{6}+2\times\frac{1}{6}+3\times\frac{1}{6}+4\times\frac{1}{6}+5\times\frac{1}{6}+6\times\frac{1}{6} \\
&= 3.5
\end{aligned}
$$

如上所述，期望值是在所有情况下“出现的数字”和“概率”相乘并加在一起得到的和。顺带一提，如果使用 $\sum$ 符号，那么期望值的数学式如下所示。

$$
\mathbb{E}[x]=\sum_xxp(x)
$$

另外， $x$ 和 $y$ 同时发生的概率（这叫作“联合概率”）如下所示。

$$
p(x,y)=p(x)p(y|x)
$$

假设奖励用 $r(x,y)$ 来表示，那么奖励的期望值如下

$$
\begin{aligned}
\mathbb{E}[r(x,y)] &= \sum_x\sum_yp(x,y)r(x,y) \\
&= \sum_x\sum_yp(x)p(y|x)r(x,y)
\end{aligned}
$$

回顾 **收益（回报）** 的定义。

$$
G_t = R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + \cdots
$$

那么有以下公式

$$
G_{t+1} = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots
$$

所以有以下递推公式：

$$
\begin{aligned}
G_t &= R_t+\gamma R_{t+1}+\gamma^2R_{t+2}+\cdots \\
&= R_t + \gamma(R_{t+1}+\gamma R_{t+2}+\cdots) \\
&= R_t+\gamma G_{t+1}
\end{aligned}
$$

这种递推关系被用于强化学习的许多理论和算法中。

下面将递推公式代入状态价值函数的定义式中。状态价值函数是收益的期望值，其数学式的定义如下所示。

$$
v_\pi(s) = \mathbb{E}_\pi[G_t|S_t=s]
$$

如上面的公式所示，状态 $s$ 的价值函数被表示为 $v_\pi(s)$ 。将递推公式带入上面的式子的 $G_t$ 中，得到下面的式子：

$$
\begin{aligned}
v_\pi(s) &= \mathbb{E}_\pi[G_t|S_t=s] \\
&= \mathbb{E}_\pi[R_t+\gamma G_{t+1}|S_t=s] \\
&= \mathbb{E}_\pi[R_t|S_t=s] + \gamma\mathbb{E}_\pi[G_{t+1}|S_t=s]
\end{aligned}
$$

```ad-note
$$
\mathbb{E}[X+Y] = \mathbb{E}[X] + \mathbb{E}[Y]
$$
```

先来推导上面公式中的第一项 $\mathbb{E}_\pi[R_t|S_t=s]$ 。

$\mathbb{E}_\pi[R_t|S_t=s]$ 的含义是在 $t$ 时刻，环境的状态是 $s$ ，那么从 $t$ 时刻开始，获得的回报的期望是多少呢？那么需要考虑所有的情况，然后加起来就可以了。

$$
\mathbb{E}_\pi[R_t|S_t=s]=\sum_a\sum_{s'}\pi(a|s)p(s'|s,a)r(s,a,s')
$$

如上面的数学式所示，将智能体行动的概率 $\pi(a|s)$ 、要迁移的状态的概率 $p(s'|s,a)$ 和奖励函数 $r(s,a,s')$ 相乘。对所有候选项都进行上述计算，得到它们的总和。

第一项推导完毕，接下来推导第二项。

![[1.2.excalidraw|1000]]

剩下的项是 $\gamma\mathbb{E}_\pi[G_{t+1}|S_t=s]$ 。由于 $\gamma$ 是常数，因此我们要看的是 $\mathbb{E}_\pi[G_{t+1}|S_t=s]$ 。这个式子虽然与状态价值函数的定义式相似，但在 $G_{t+1}$ 的部分有所不同。状态价值函数的式子如下所示，式子中是 $G_t$ ，而不是 $G_{t+1}$ 。

$$
v_\pi(s) = \mathbb{E}_\pi[G_t|S_t=s]
$$

因此，我们首先要将 $t+1$ 代入上面的式子的 $t$ 中。式子变化如下。

$$
v_\pi(s) = \mathbb{E}_\pi[G_{t+1}|S_{t+1}=s]
$$

这就是状态 $S_{t+1}=s$ 时的价值函数。接下来要关注的是 $\mathbb{E}_\pi[G_{t+1}|S_t=s]$ 。这是在当前时刻为 $t$ 时，下一个时刻 $(t+1)$ 的收益期望值。解决的关键在于将条件 $S_t=s$ 变为 $S_{t+1}=s$ 的形式。换句话说，就是要进入下一个时刻。

通过观察可以得到

$$
\begin{aligned}
\mathbb{E}_\pi[G_{t+1}|S_t=s] &= \sum_{a,s'}\pi(a|s)p(s'|s,a)\mathbb{E}_\pi[G_{t+1}|S_{t+1}=s'] \\
&= \sum_{a,s'}\pi(a|s)p(s'|s,a)v_\pi(s')
\end{aligned}
$$

完成第二项的展开以后，汇总一下得到

$$
\boxed{
\begin{aligned}
v_\pi(s) &= \mathbb{E}_\pi[R_t|S_t=s] + \gamma\mathbb{E}_\pi[G_{t+1}|S_t=s] \\
&= \sum_{a,s'}\pi(a|s)p(s'|s,a)r(s,a,s')+\gamma\sum_{a,s'}\pi(a|s)p(s'|s,a)v_\pi(s') \\
&= \sum_{a,s'}\pi(a|s)p(s'|s,a)\{r(s,a,s')+\gamma v_\pi(s')\}
\end{aligned}
}
$$

上面的式子就是大名鼎鼎的 **贝尔曼方程** 。贝尔曼方程是表示状态 $s$ 的价值函数和下一个可能的状态 $s'$ 的价值函数之间关系的式子。这个贝尔曼方程对所有状态 $s$ 和所有策略 $\pi$ 都成立。

状态价值函数的贝尔曼方程的另一种重要的表示形式：**贝尔曼期望方程** 。

$$
\boxed{
\begin{aligned}
V_\pi(S_t)
&= \mathbb{E}_\pi[R_t+\gamma V_\pi(S_{t+1})|S_t] \\
\end{aligned}
}
$$

## 6. 下一步

智能体的目标是使收益最大化。这里有一点需要注意，那就是智能体和环境的行动可能是“随机性”的。智能体可能随机地决定行动，状态也可能随机迁移。在这种情况下，获得的收益将呈现随机的特点。即使从相同的状态开始，不同回合的收益也随机变化。例如，某个回合的收益为 $10.4$ ，另一个回合的收益为 $8.7$ 。

在倒立摆环境中，我们如何选择智能体的 “策略” 来让木杆多坚持一段时间呢？

接下来我们来学习 **策略梯度法** 。

