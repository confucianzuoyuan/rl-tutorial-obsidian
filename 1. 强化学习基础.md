## 1.1 基本概念

**强化学习（reinforcement learning，RL）** 讨论的问题是智能体（agent）怎么在复杂、不确定的环境（environment）中最大化它能获得的奖励。

![[1.1.excalidraw]]
- Agent：智能体
- Environment：环境
- State：状态
- Reward：奖励
- Action：动作

强化学习由两部分组成：智能体和环境。在强化学习过程中，智能体与环境一直在交互。智能体在环境中获取某个状态后，它会利用该状态输出一个动作 （action）。然后这个动作会在环境中被执行，环境会根据智能体采取的动作，输出下一个状态以及当前这个动作带来的奖励。智能体的目的就是尽可能多地从环境中获取奖励。

下面是一个倒立摆环境。

![[1.2.png]]

在这个环境中，智能体是推车。

- 动作空间：推车有两个动作，**向左推** 和 **向右推**
- 状态：
	- 推车的位置
	- 推车的速度
	- 杆子的角度
	- 杆子的角速度
- 奖励：推车采取向左推或者向右推的动作之后，只要杆子不倒下，奖励就是 `1` 。

游戏的结束条件：

- 推车将杆子推出了屏幕（失败）
- 杆子倒下（失败）
- 采取了 200 次推车的动作，杆子也没倒下（成功）

那么推车应该采取什么样的动作，杆子才能不倒下呢？或者说，推车应该采取什么样的 **策略（policy）** ，杆子才能不倒下呢？因为只有杆子不倒下，我们才能不停的获得奖励。

如果推车每采取一个动作，杆子还能保持平衡，那么我们获得的 **即时奖励** 是 1 。

如果到游戏结束，杆子也没有倒下，那么我们将获得最大的奖励 200 。

而推车每次要采取什么动作，是取决于环境的状态的。也就是推车要根据环境的状态来决定下一步采取什么动作。例如：此时杆子向左偏，那么推车可能需要采取向左推的动作，才能使杆子不倒下。

换句话说，推车要采取的策略是一个 **函数** 。

$$
\text{要采取的动作} = \textcolor{red}{\text{策略函数}}(环境的状态)
$$

而这里 **要采取的动作** 一般来说是一个概率分布。例如：向左推的概率=0.7，向右推的概率=0.3 。

然后从这个分布中进行采样，采样一个动作出来。当然也可以直接选概率最大的动作。

而我们使用什么样的策略呢？当然是能够让我们在游戏结束时，能够获得最大奖励的策略啦。

当然，这里选择策略有很多的讲究。如果我们每次采取的都是概率最大的动作，那么每次可能都会获得最大的即时奖励。如果这样做的话，可能到了一定的时候，不管向左推还是向右推获得的即时奖励可能都是 0 ，也就是不管怎么推，杆子都会倒下（杆子偏离的角度太大，怎么推都没用）。也就是说，我们太看重眼前利益，忽略了长远的利益。这样就是只 **利用** 而不 **探索** 。所以有时也需要采取一下概率低的动作，探索一下环境，万一未来获得的奖励更多呢。

所以偶尔也需要采取一下概率小的动作。

```ad-note
所谓从 **概率分布中采样** 的意思是，例如概率分布是 向左推 的概率是 0.7 ，向右推 的概率是 0.3 。那么从这个概率分布中采样一个动作出来，有 70% 的概率采取的动作是向左推。但也有 30% 的概率向右推。
```

举个例子：如果我们每次都去最熟悉的餐馆吃饭，可能体验都还可以。而如果去不熟悉的餐厅吃饭，可能体验不好，也可能体验超过了之前的餐馆。在舒适区只利用不探索，就会固步自封。冒险可能会受到伤害，但长期来看，可能会得到提升。

策略是智能体的动作模型，它决定了智能体的动作。它其实是一个函数，用于把输入的状态变成动作。

策略的数学符号是 $\pi$ ，所以策略也就是 $\pi$ 函数。即 $\pi(a | s)=p\left(a_{t}=a | s_{t}=s\right)$ 。输入一个状态 $s$ ，输出一个概率分布。用条件概率来看，就是在条件：状态为 $s$ 的情况下，策略采取动作 $a$ 的概率是多少。这个概率是智能体所有动作的概率，然后对这个概率分布进行采样，可得到智能体将采取的动作。比如可能是有 0.7 的概率往左，0.3 的概率往右，那么通过采样就可以得到智能体将采取的动作。

![[1.3.excalidraw|600x100]]

所以有如下：

$$
\begin{aligned}
\pi(a=a_{\text{向左推}}|s)=0.7 \\
\pi(a=a_{\text{向右推}}|s)=0.3
\end{aligned}
$$

如果这个 $\pi$ 函数是一个神经网络，那么这就是 **深度学习 + 强化学习 = 深度强化学习** 。当今 AI 界最热门的话题。

所以玩倒立摆游戏的一个 **回合（episode）** 就是环境的状态为 $S_0$ ，推车采取动作 $A_0$ ，然后获得奖励 $R_0$ ，然后环境的状态转移到了 $S_1$ ，推车接着采取动作 $A_1$ ，然后获得奖励 $R_1$ ...。下标是时刻，或者时间步。把它们写到一起，就是一个 **轨迹（trajectory）** 。轨迹用数学符号 $\tau$ 表示，读作 `掏` 。

$$
\boxed{
\tau = (S_0,A_0,R_0,S_1,A_1,R_1,S_2,A_2,R_2\dots)
}
$$

由于根据环境的状态，采取的动作是从一个概率分布中采样得到的，所以轨迹会有很多很多条。

![[1.4.excalidraw|600x100]]

## 1.2 价值函数

当位于时刻 $t$ 时，环境此时处于状态 $S_t$ ，然后我们根据策略函数开始采取动作，那么未来我们一共能获得多少奖励呢？环境处于状态 $S_t$ ，我们采取的动作是 $A_t$ ，获取的奖励是 $R_t$ ，然后环境的状态从 $S_t$ 转移到了 $S_{t+1}$ ，然后采取动作 $A_{t+1}$ ，然后获得即时奖励 $R_{t+1}$ ，然后环境的状态从 $S_{t+1}$ 转移到了 $S_{t+2}$ ，然后环境会给我们即时奖励 $R_{t+2}$ ，......。

但是未来的奖励不如现在的奖励有吸引力，所以需要 **打折** 。那么，从 $t$ 时刻起，未来一共获得的奖励叫做 **回报（或者收益）** 。

$$
\boxed{
G_t = R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + \cdots
}
$$
$\gamma$ 叫做折扣因子。随着时间的推移，奖励会被 $\gamma$ 指数级削弱。这个 $\gamma$ 被称为折扣因子（discount rate），其被设定为 $0.0$ 和 $1.0$ 之间的实数。如果折扣因子是 $0.9$，那么有以下式子成立。

$$
G_t = R_t + 0.9 R_{t+1} + 0.81 R_{t+2} + \cdots
$$

引入折现率主要是为了防止连续性任务的收益变得无穷大。在连续性任务中，如果没有折扣因子（或 $\gamma=1$ ），那么收益就会发散到无穷大 。因此，设置折扣因子可以防止收益的发散。

折扣因子也使近期的奖励显得更加重要。这解释了人类乃至生物的许多行动原理。例如，你会选择今天拿到 10000 元还是一年后拿到 20000元？如果折扣因子使未来的回报呈指数级下降，那么眼前的回报就会更有吸引力。

如果我们用倒立摆作为例子，然后我们运行两个时间步。得到下图。

![[1.5.excalidraw]]
可以看到，一共有 4 条轨迹。每条轨迹都有一个总的回报。而每条轨迹也都有一个产生的概率。那么我们如何评估在环境处于状态 $S_t$ 时，一直采取策略 $\pi$ 未来会获得多少回报呢？也就是未来的预期回报（回报的期望值）是多少呢？那就是 **状态价值函数** 。

$$
\boxed{
V_{\pi}(s) = \mathbb{E}_{\pi}\left[G_{t} \mid S_{t}=s\right]=\mathbb{E}_{\pi}\left[\sum_{k=0}^{\infty} \gamma^{k} R_{t+k} \mid S_{t}=s\right], \text{对于所有的} s \in S
}
$$

```ad-note
状态价值函数，衡量的是在环境处于状态 $s$ 时，一直按照策略 $\pi$ 来采取动作，最终的预期回报。
```

$$
\begin{aligned}
V_\pi(S_t) &= \mathbb{E}_\pi[G_t|S_t] \\
&= \mathbb{E}_\pi\left[\sum_{k=0}^\infty\gamma^kR_{t+k}\mid S_t\right] \\
&= \mathbb{E}_\pi\left[R_t+\gamma\sum_{k=0}^\infty\gamma^kR_{t+k+1}\mid S_t\right] \\
&= \mathbb{E}_\pi[R_t+\gamma G_{t+1}|S_t] \\
&= \mathbb{E}_\pi[R_t|S_t]+\gamma \mathbb{E}_\pi[G_{t+1}|S_t] \\
&= \mathbb{E}_\pi[R_t|S_t]+\gamma \mathbb{E}_\pi[\mathbb{E}_\pi [G_{t+1}|S_{t+1}]|S_t] \\
&= \mathbb{E}_\pi[R_t|S_t]+\gamma \mathbb{E}_\pi[V_\pi(S_{t+1})|S_t] \\
&= \mathbb{E}_\pi[R_t+\gamma V_\pi(S_{t+1})|S_t] \\
\end{aligned}
$$

直观解释：

当前环境对你的价值 = 当前环境现在给你的奖励 + 折扣因子 x 未来环境对你的价值

你对公司的评估 = 现在公司给你的薪资 + 折扣因子 x 你对公司未来的评估

也就是你觉得现在公司怎么样？得看公司现在给你开的钱和你觉得公司未来的前景，综合考虑。

现在给的钱少，但是你觉得公司前景不错（折扣因子0.99），那么你对公司的评估也不错。

现在给的钱很多，但是你觉得公司前景会倒闭（折扣因子0.01，但如果不倒闭公司未来前景家这也很不错，但是倒闭的概率很大），那么你对公司的评估也不错。

还有一种可能，现在钱很少，公司倒闭的可能性很大，但一旦公司每倒闭，就很牛逼。

```ad-note
全期望公式：$\mathbb{E}[X]=\mathbb{E}[\mathbb{E}[X|Y]]$
条件期望的迭代公式：$\mathbb{E}[X|Y]=\mathbb{E}[\mathbb{E}[X|Z]|Y]$
```

## 1.3 倒立摆环境编程实践

我们使用 OpenAI 编写的 Gym 库。

```bash
$ pip install gym==0.25.2
$ pip install pygame
```

然后我们来创建一个倒立摆环境

```python
import gym

env = gym.make('CartPole-v0')
```

这样就生成了倒立摆的环境。

![[1.2.png]]

将推车向右或向左移动，以保持杆子的平衡。倒立摆的结束条件是杆子的平衡被打破（杆子超过一定的角度），或者推车的移动位置超出了某个范围。

> [!NOTE]
> 倒立摆环境包含版本0（CartPole-v0）和版本1（CartPole-v1）。版本 0 的上限为 200 步。如果能在 200 步之内保持平衡，那么游戏就结束了。版本 1 的上限为 500 步。

下面继续执行以下代码。

```python
state = env.reset()
print(state) # 初始状态
action_space = env.action_space
print(action_space) # 行动的维度
```

上面的代码通过 `state = env.reset()` 获得了初始状态。观察它的输出，你会发现它是拥有 4 个元素的数组。作为参考，下面依次列出这 4 个元素。

- 推车的位置
- 推车的速度
- 杆子的角度
- 杆子的角速度

另外，我们可以通过 `env.action_space` 获得行动的维度（可采取的行动数）。它的输出是一个名为 `Discrete(2)` 的类实例。这意味着有两个候选行动。具体来说，0对应的是向左移动推车的行动，1对应的是向右移动推车的行动。下面实际地采取行动，向前推进一个时间步。

```python
action = 0 # 或者 1
next_state, reward, done, info = env.step(action)
print(next_state)
```

上面的代码通过 `env.step(action)` 采取行动。作为结果 ，我们得到了以下 4 个信息。

- 下一个状态（`next_state`）
- 奖励（`reward`）
- 是否结束的标志位（`done`）
- 附加信息（`info`）

`reward` 是标量值（`float`）。这次的任务在保持平衡的时候总是会得到奖励 `1` 。`info` 包含有助于调试的信息（如环境模型）。但在实现和评估强化学习的算法时，基本不会用到 `info` 。

我们先来实现一个随机智能体。也就是这个智能体的策略非常简单，无论环境处于什么状态，我们都是从两个动作中随机采样一个动作。

```python
import numpy as np
import gym

env = gym.make('CartPole-v0')
state = env.reset()
done = False

while not done:
	env.render()
	action = np.random.choice([0, 1])
	next_state, reward, done, info = env.step(action)
env.close()
```

如下图所示

![[1.6.excalidraw|600x100]]

由于我们使用的是随机策略，所以倒立摆游戏很快就结束了。

那么我们如何找到一个很好的策略，来让倒立摆不倒下呢？

我们想要训练一个神经网络来作为策略函数，希望神经网络输出的动作能让倒立摆没有那么快的倒下，这就是深度强化学习。

接下来我们学习 **策略梯度法** 。

