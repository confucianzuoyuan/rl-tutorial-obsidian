```ad-info
PPO: Proximal Policy Optimization，近端策略优化
```

## 策略梯度法存在的问题

![[2.13.excalidraw|1000]]

![[2.12.excalidraw|1000]]

![[2.17.excalidraw|1000]]

**步子迈得太大会导致灾难** 。但如果步子太小，模型学习速度就会太慢。把奖励函数想象成高耸的山峰。如果新策略走得太远，它采取的行动可能会差之毫厘，最终从悬崖上掉下来。当我们重新开始探索时，我们从一个表现不佳的状态开始，采用局部糟糕的策略。性能会崩溃，而且需要很长时间才能恢复。

![[3.2.excalidraw|1000]]

```ad-note
为什么在深度学习中这个问题没有那么严重呢？

深度学习中，使用的训练数据集是固定不变的。换句话说，不管神经网络的参数怎么变，训练数据集是不会变的。也就是说，神经网络的参数和训练数据集没关系。

而强化学习中，训练数据是每一轮根据策略来采样的新的轨迹。而采样的轨迹的质量好坏无法控制。也就是说训练数据集和策略神经网络的参数是有关系的。这就麻烦了。
```

![[3.3.excalidraw|1000]]

在强化学习中很难找到合适的学习率。假设学习率是专门针对上图黄点调整的。该区域相对平坦，因此为了获得良好的学习速度，学习率应该高于平均值。但是，一步走错，我们就会从悬崖上掉到红点。红点处的梯度很高，当前的学习率会触发爆炸式策略更新。由于学习率对地形不敏感，策略梯度算法的收敛问题非常严重。

````ad-attention
也就是策略梯度法存在的问题是训练出来的新策略可能比旧策略还要差。

```ad-danger
能否找到一个算法，使得训练出来的新策略 **一定** 比旧策略好呢？
```
````

## 近端策略优化：PPO

优化方法主要有两种：线性搜索和置信域。梯度下降是一种线性搜索。我们首先确定下降方向，然后朝该方向迈出一步。

![[3.8.excalidraw|1000]]

在置信域中，我们确定想要探索的最大步长，然后在该置信域内找到最优点。

![[3.9.excalidraw|1000]]

也就是，我们在梯度上升的时候，先周围看一圈，然后找一个安全的步长，然后进行梯度上升。

为了实现这个目的，我们的梯度公式 $\nabla_\theta J(\theta)$ 就要改变了。

PPO 算法是 $\text{Actor-Critic}$ 架构的，也就是 **演员-评论家** 架构。

所以我们先来回顾一下策略梯度法的演员-评论家架构的梯度公式。

策略梯度法的目标函数

$$
J(\theta) = \mathbb{E}_{\tau\sim\pi_{\theta}}[G(\tau)]
$$

策略梯度法的目标函数的梯度

$$
\boxed{
\nabla_{\theta}J(\theta)=\mathbb{E}_{\tau\sim\pi_{\theta}}\left[\sum_{t=0}^TA_t^{\pi_\theta}\nabla_{\theta}\log\pi_{\theta}(A_t|S_t)\right]
}
$$

其中 $A_t^{\pi_\theta} = R_t+\gamma V_\omega(S_{t+1})-V_\omega(S_t)$ 。这里的 $A$ 表示优势，也就是 $\text{Advantage}$ 。表示当前策略采取动作 $A_t$ 之后，相对于价值函数评估的价值的优势是多少。

```ad-danger
从策略梯度法的目标函数推导到 PPO 算法的目标函数非常的复杂。需要比较好的数学知识。
```

PPO的目标函数是：

$$
\boxed{
J(\theta)^{\text{ppo-clip}}=\mathbb{E}_{\tau\sim\pi_{{\pi_\theta}_{old}}}\left[{\sum_{t=0}^T\left[\min\left(\frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}A_t^{\pi_{old}},\text{clip}(\frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)},1-\epsilon,1+\epsilon)A_t^{\pi_{old}})\right)\right]}\right]
}
$$

而 PPO 的梯度公式是

$$
\boxed{
\nabla_\theta J(\theta)^{\text{ppo-clip}}=\nabla_\theta\mathbb{E}_{\tau\sim\pi_{{\pi_\theta}_{old}}}\left[{\sum_{t=0}^T\left[\min\left(\frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}A_t^{\pi_{old}},\text{clip}(\frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)},1-\epsilon,1+\epsilon)A_t^{\pi_{old}})\right)\right]}\right]
}
$$

其中 $A_t^{\pi_{old}} = R_t+\gamma V_\omega(S_{t+1})-V_\omega(S_t)$ 。

```ad-note
上面的 $A_t^\pi=R_t+\gamma V_\omega(S_{t+1})-V_\omega(S_t)$ 是 ==1== 步 **时序差分误差**（TD误差）。

也可以使用 [[附录C：广义优势估计]] ，在 PPO 以及之后的算法基本都用 **广义优势估计** 。也就是 $n$ 步时序差分误差。
```

![[ppo-object.excalidraw|1000]]

如果新策略与旧策略的概率比超出 $(1-\epsilon)$ 和 $(1+\epsilon)$ 的范围，则优势函数将被剪裁。在 PPO 论文中的实验中，$\epsilon$ 设置为 $0.2$ 。

![[1_MpPiARNoNGCxJE2a8m9itA.webp]]

实际上，如果策略发生重大变化超出了我们的舒适区，那么这将阻碍策略的实现。

我们假设

$$
p_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}
$$


|     | $p_t(\theta)>0$                         | $A_t$ | $\min$ 的结果                   | 目标函数是否被裁剪？ | 目标函数的符号 | 梯度  |
| --- | --------------------------------------- | ----- | ---------------------------- | ---------- | ------- | --- |
| 1   | $p_t(\theta)\in[1-\epsilon,1+\epsilon]$ | $+$   | $p_t(\theta)A_t$             | no         | $+$     | ✔   |
| 2   | $p_t(\theta)\in[1-\epsilon,1+\epsilon]$ | $-$   | $p_t(\theta)A_t$             | no         | $-$     | ✔   |
| 3   | $p_t(\theta)<1-\epsilon$                | $+$   | $p_t(\theta)A_t$             | no         | $+$     | ✔   |
| 4   | $p_t(\theta)<1-\epsilon$                | $-$   | $(1-\epsilon)p_t(\theta)A_t$ | yes        | $-$     | $0$ |
| 5   | $p_t(\theta)>1+\epsilon$                | $+$   | $(1+\epsilon)p_t(\theta)A_t$ | yes        | $+$     | $0$ |
| 6   | $p_t(\theta)>1+\epsilon$                | $-$   | $p_t(\theta)A_t$             | no         | $-$     | ✔   |
我们有六种不同的情况。首先记住，我们取裁剪目标和非裁剪目标中的最小值。

**情况 1 和 2：比率介于裁剪范围之内**

在情况 1 和 2 中， **由于比例介于范围 $[1-\epsilon,1+\epsilon]$ 之间，因此目标函数不会被裁剪** 。

在情况 1 中，我们具有正优势：该**动作优于该状态下所有动作的平均值** 。因此，我们应该鼓励当前策略提高在该状态下采取该动作的概率。

由于该比率是在区间之间的， **因此我们可以增加我们的策略在该状态下采取该行动的概率。**

在情况 2 中，我们有一个负优势：该动作比该状态下所有动作的平均值更差。因此，我们应该阻止当前策略在该状态下采取该动作。

由于该比率是在区间之间的， **因此我们可以降低我们的策略在该状态下采取该行动的概率。**

**情况 3 和 4：比率小于$1-\epsilon$**

如果概率比低于 $1-\epsilon$ ，则当前策略在该状态下采取该行动的概率比旧策略低得多。

如果像情况 3 中一样，优势估计为正（$A>0$），那么**我们希望增加在该状态下采取该行动的概率。**

但是，如果像情况 4 那样，优势估计为负， **我们不想进一步降低**在该状态下采取该行动的概率。因此，$\text{梯度} = 0$（因为我们在一条平线上），所以我们不会更新权重。

**情况 3 和 4：比率大于$1+\epsilon$**

如果概率比高于 $1+\epsilon$ ，则当前策略中该状态下采取该行动的概率**远高于前一策略。**

如果像情况 5 那样，优势为正， **我们不应该太贪心** 。因为在当前状态下，我们采取该行动的概率已经比之前的策略更高了。因此，梯度 = 0（因为我们在一条平线上），所以我们不会更新权重。

如果像情况 6 中那样，优势是负面的，我们希望降低在该状态下采取该行动的概率。

总结一下， **我们只用未裁剪的目标函数部分来更新策略** 。当最小值是裁剪后的目标函数部分时，我们不会更新策略权重，因为梯度将等于 0。

因此，我们仅在以下情况下更新我们的策略：

- 比率在 $(1-\epsilon,1+\epsilon)$ 之内
- 比率不在 $(1-\epsilon,1+\epsilon)$ 范围内，但优势使我们更接近这个范围。
	- 比率小于 $1-\epsilon$ ，但优势 > 0 。
	- 比率大于 $1+\epsilon$ ，但优势 < 0 。

**你可能会想，为什么当最小值是截断比率时，梯度为 0。** 当比率被截断时，在这种情况下的导数将不是 $p_t(\theta)A_t$ 的导数。​而是 $(1-\epsilon)A_t$ 或者 $(1+\epsilon)A_t$ 的导数，而两者的导数都是 0 。

```ad-note
$A_t$ 不是 $\theta$ 的函数。所以对 $\theta$ 求导为 0 。
```

总而言之，得益于这个裁剪的替代目标， **我们限制了当前策略与旧策略之间的差异范围。** 因为我们消除了比率超出区间的诱因，因为裁剪会对梯度产生影响。如果比率为 $>1+\epsilon$ 或 $<1-\epsilon$，则梯度将等于 0。

算法如下

```pseudo
    \begin{algorithm}
    \caption{2 带裁剪目标的PPO}
    \begin{algorithmic}
      \State 输入：初始化策略参数 $\theta_0$ ，初始化裁剪阈值参数 $\epsilon$
      \For{$k = 0,1,2,\dots$}
      \State 使用策略 $\pi_k=\pi(\theta_k)$ 采集一组策略 $\mathcal{D}_k$
      \State 计算优势 $A_t^{\pi_k}$ ，使用广义优势估计（GAE）
      \State 计算策略的更新（通过执行K步的微批次SGD，使用Adam优化器）
      \State
      $
      \theta_{k+1}=\underset{\theta}{\arg\max}\mathcal{L}_{\theta_k}^{CLIP}(\theta)
      $
      \State 其中
      \State
      $\quad\quad
      \mathcal{L}^{CLIP}_{\theta_k}=\underset{\tau\sim\pi_k}{\mathbb{E}}\left[{\sum_{t=0}^T[\min(p_t(\theta)A_t^{\pi_k},\text{clip}(r_t(\theta),1-\epsilon,1+\epsilon)A_t^{\pi_k})]}\right]
      $
      \EndFor
      \end{algorithmic}
    \end{algorithm}
```

这个新方法很简单，而且可以像 $\text{Adam}$ 一样使用梯度下降来优化。其实，对这个问题分析得这么详细，却得出这么简单的解决方案，实在是有点虎头蛇尾。

$\text{PPO}$ 增加了一个软约束，可以通过一阶优化器进行优化。我们偶尔可能会做出一些错误的决策，但它在优化速度上取得了良好的平衡。实验结果证明，这种平衡能够以最简单的方式实现最佳性能。

```ad-note
深度学习中的简单规则。
```

或者至少等我们发明超快的 GPU 。而这是不可能的。