```ad-info
PPO: Proximal Policy Optimization，近端策略优化
```

## 策略梯度法存在的问题

![[2.13.excalidraw|1000]]

![[2.12.excalidraw|1000]]

![[2.17.excalidraw|1000]]

**步子迈得太大会导致灾难** 。但如果步子太小，模型学习速度就会太慢。把奖励函数想象成高耸的山峰。如果新策略走得太远，它采取的行动可能会差之毫厘，最终从悬崖上掉下来。当我们重新开始探索时，我们从一个表现不佳的状态开始，采用局部糟糕的策略。性能会崩溃，而且需要很长时间才能恢复。

![[3.2.excalidraw|1000]]

```ad-note
为什么在深度学习中这个问题没有那么严重呢？

深度学习中，使用的训练数据集是固定不变的。

而强化学习中，训练数据是每一轮采样的新的轨迹。而采样的轨迹的质量好坏无法控制。
```

![[3.3.excalidraw|1000]]

在强化学习中很难找到合适的学习率。假设学习率是专门针对上图黄点调整的。该区域相对平坦，因此为了获得良好的学习速度，学习率应该高于平均值。但是，一步走错，我们就会从悬崖上掉到红点。红点处的梯度很高，当前的学习率会触发爆炸式策略更新。由于学习率对地形不敏感，策略梯度算法的收敛问题非常严重。

````ad-attention
也就是策略梯度法存在的问题是训练出来的新策略可能比旧策略还要差。

```ad-danger
能否找到一个算法，使得训练出来的新策略 **一定** 比旧策略好呢？
```
````

## 近端策略优化：PPO

优化方法主要有两种：线性搜索和置信域。梯度下降是一种线性搜索。我们首先确定下降方向，然后朝该方向迈出一步。

![[3.8.excalidraw|1000]]

在置信域中，我们确定想要探索的最大步长，然后在该置信域内找到最优点。

![[3.9.excalidraw|1000]]

也就是，我们在梯度上升的时候，先周围看一圈，然后找一个安全的步长，然后进行梯度上升。

为了实现这个目的，我们的梯度公式 $\nabla_\theta J(\theta)$ 就要改变了。

PPO 算法是 $\text{Actor-Critic}$ 架构的，也就是 **演员-评论家** 架构。

所以我们先来回顾一下演员-评论家架构的梯度公式。

$$
\boxed{
\nabla_{\theta}J(\theta)=\mathbb{E}_{\tau\sim\pi_{\theta}}\left[\sum_{t=0}^TA_t^{\pi_\theta}\nabla_{\theta}\log\pi_{\theta}(A_t|S_t)\right]
}
$$

其中 $A_t^{\pi_\theta} = R_t+\gamma V_\omega(S_{t+1})-V_\omega(S_t)$ 。这里的 $A$ 表示优势，也就是 Advantage 。表示当前策略比起价值函数评估的价值的优势是多少。

PPO的目标函数是：

$$
J(\theta)^{\text{ppo-clip}}=\mathbb{E}_{\tau\sim\pi_{{\pi_\theta}_{old}}}\left[{\sum_{t=0}^T\left[\min\left(\frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}A_t^{\pi_{old}},\text{clip}(\frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)},1-\epsilon,1+\epsilon)A_t^{\pi_{old}})\right)\right]}\right]
$$

而 PPO 的梯度公式是

$$
\nabla_\theta J(\theta)^{\text{ppo-clip}}=\nabla_\theta\mathbb{E}_{\tau\sim\pi_{{\pi_\theta}_{old}}}\left[{\sum_{t=0}^T\left[\min\left(\frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}A_t^{\pi_{old}},\text{clip}(\frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)},1-\epsilon,1+\epsilon)A_t^{\pi_{old}})\right)\right]}\right]
$$

其中 $A_t^{\pi_{old}} = R_t+\gamma V_\omega(S_{t+1})-V_\omega(S_t)$ 。

```ad-note
上面的 $A_t^\pi=R_t+\gamma V_\omega(S_{t+1})-V_\omega(S_t)$ 是 ==1== 步 **时序差分误差**（TD误差）。

也可以使用 [[附录C：广义优势估计]] ，在 PPO 以及之后的算法基本都用 **广义优势估计** 。也就是 $n$ 步时序差分误差。
```

![[ppo-object.excalidraw|1000]]

