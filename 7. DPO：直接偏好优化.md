
```ad-note
DPO：Direct Preference Optimization

直接偏好优化：你的大语言模型实际上是一个奖励模型
```

DPO，是基于人类反馈的强化学习（RLHF）的一种方法，它避免了真正的强化学习。

## 1. 训练、微调和对齐 LLM

让我们先回顾一下创建大语言模型（例如 ChatGPT 或 Claude）的流程。以下步骤是连续的，每个步骤都建立在前一个步骤的基础上：

1. 基于互联网规模的数据预训练一个基础模型。给定一段文本，训练该模型预测下一个token。这个概念上简单的任务具有极好的扩展性，并允许 LLM 从训练数据中编码大量知识。基础模型的示例包括 GPT-3、Llama3 和 DeepSeek-V3 等等。
2. 采用预先训练好的基础模型，并使用针对特定任务的数据集对与训练模型进行微调。例如，如果想创建像 ChatGPT 这样的实用的对话模型，则需要基于对话数据集进行微调，以使模型的输出听起来更像对话的片段，而不是维基百科页面。在此阶段，我们仍然使用下一个token预测任务，微调过程会更新我们的模型，使其预测结果更接近我们输入的高质量特定任务示例。例如：`Qwen2.5-3B-Instruct` 。
3. 最后，我们根据人类偏好对模型进行微调。人类偏好之所以强大，是因为它们能够轻松且低成本地表达出来。试想一下，比较两部电影并选出一部你最喜欢的电影是多么容易。然而，制作一部能够体现驱使你去影院观看的特质的电影是多么困难。同样，准确描述我们希望模型如何表现（就像我们在步骤 2 中尝试做的那样）也具有挑战性，但给定模型行为的示例，我们可以轻松地指出对特定行为类型的偏好。有一段时间，这种偏好调整是使用 RLHF 完成的。最近，由于 DPO 相对简单，RLHF 在某种程度上已被 DPO 取代。已经使用人类偏好进行调整的 LLM 包括 Llama 3 Instruct、ChatGPT-4、Claude 3 Opus 和 Gemini Ultra。

![[5.1.excalidraw|1000]]

## 2. 使用偏好数据集微调 LLM

为我们希望 LLM 模仿的行为创建高质量的示例数据集是一项艰巨而耗时的工作。聘请标注员来帮助我们创建此类数据的成本也相当高昂。然而，一旦我们拥有一个能够“足够好”地展示所需行为的模型，我们就可以全力以赴了。给定一个提示，我们可以通过注入少量随机性，从 LLM 中采样两种不同的响应。

```ad-note
使用相同的提示词，让 LLM 输出不同的回答。可以通过控制温度参数的方式来做到这一点。可以看一下这个 [网站](https://blog.lukesalamone.com/posts/what-is-temperature/) 理解一下温度的作用。
```

然后，让标注员表达对两种回答之一的偏好既便宜又容易。

在使用 ChatGPT 或 Gemini 时，你可能注意到，偶尔会被要求在两个相似的答案中选择一个来继续对话。这个偏好会被记录下来，并用于在未来的偏好调整中改进模型。同样，Chatbot Arena 也会收集偏好数据，以便根据人工评估对 LLM 进行评级：

![[5.2.excalidraw|1000]]

有许多公开可用的偏好数据集，例如 LMSys 的 Chatbot Arena Conversations 数据集、OpenAI 的 WebGPT Compares 数据集和 Anthropic 的 Helpfulness-Harmlessness RLHF 数据集（露骨/冒犯性内容警告）。

如果使用数学符号，那么这些数据集可以表示成

$$
\mathcal{D}=\{x^{(i)}, y_w^{(i)}, y_l^{(i)}\}^N_{i=1}
$$

其中 $x$ 是上下文或者提示词， $y_w$ 是人类喜欢的回答，而 $y_l$​ 是人类不喜欢的回答。

### Bradley-Terry 模型

那么，我们该如何处理这些偏好数据呢？我们希望利用这些数据来微调我们的 LLM，使其输出更符合偏好的响应。首先，让我们探索一个简单的概率模型：

$$
p^*(i \succ j) = \frac{s_i}{s_i+s_j}
$$

这是 Bradley-Terry 模型，用于对成对比较的结果进行建模。用通俗的话说，它就是“我们模拟真实的当 $i$ 的得分高于 $i$ 和 $j$ 的总得分时，结果 $i$ 优于结果 $j$ 的概率”。

> $p^*$ 中“星号”的含义：表示我们正在建模人类偏好的真实潜在分布。同样，我们很快就会看到 $r^*$，它表示对大模型输出的回答进行评分的真实潜在奖励函数；以及 $\pi^*$，它表示我们希望 LLM 模仿的最优策略。

Bradley-Terry 模型与 Elo 评分系统相关，该评分系统在国际象棋和其他竞技游戏中非常流行。Bradley-Terry 模型是 Elo 评分系统的推广，其中玩家 A 击败玩家 B 的概率为 $p(A\succ B) = \frac{1}{1+10^{(R_B-R_A)/400}}=\frac{s_A}{s_A+s_B}$ 。 这里 $R$ 表示一个玩家的评分，$s=10^{R/400}$ 。

```ad-note
所以如果玩家 A 的 Elo 的评分是 2000 以及玩家 B 的 Elo 的评分是 1600 ，那么玩家 A 获胜的可能性预计是玩家 B 的 10 倍，因为
- $p(A\succ B) = \frac{1}{1+10^{(1600-2000)/400}}=10/11$ ，
- $p(B\succ A)=\frac{1}{1+10^{(2000-1600)/400}}=1/11$ 。
```

在 Bradley-Terry 模型下，通常选择将分数参数化为 $s=e^r$，其中 $r$ 代表奖励。“奖励”一词源自强化学习，在强化学习中，一系列更理想的行动会获得更大的奖励——类似于在电子游戏中表现更好就能获得更高的分数。

通过这种参数化，我们的模型开始看起来相当不错——通过 $\text{Sigmoid}$ 函数传递的奖励值的简单差异

$$
p^*(i\succ j)=\frac{s_i}{s_i+s_j}=\frac{e^{R_i^*}}{e^{R_i^*}+e^{R_j^*}} = \frac{1}{1+e^{-(R^*_i-R_j^*)}} = \sigma(R_i^*-R_j^*)
$$

### 将 Bradley-Terry 模型应用到 LLM

现在，我们希望采用 Bradley-Terry 模型并利用其与偏好数据集来改进我们的 LLM 生成的输出。

在我们的偏好数据集（$\mathcal{D}$）中，我们有两个比较，我们希望建模其中一种补全方式比另一种更受欢迎的概率。从某种意义上说，每种补全方式都会根据其质量引发一些奖励，而我们的最终目标是推动我们的 LLM 产生更高质量的补全。因此，我们将使用 LLM 参数化奖励。我们将其称为 $R^*(x,y)$，这意味着奖励是上下文/提示（$x$）和补全（$y$）的函数。

因此，在调整我们的偏好模型以使用我们的参数化奖励函数之后，我们得到：

$$
p^*(y_1\succ y_2|x)=\sigma(R^*(x,y_1),R^*(x,y_2))
$$

但是，仅仅谈论最优解和奖励对我们毫无意义，因为我们无法获得最优奖励函数。实践中，我们通常会学习一个模拟最优奖励函数的奖励模型 $R_\phi(x,y)$ 。我们可以将该奖励模型的参数 $\phi$ 定义为一个二分类问题，其目标是最小化偏好数据集 $\mathcal{D}$ 上的以下负对数似然损失函数：

$$
\mathcal{L}_R(R_\phi,\mathcal{D})=-\mathbb{E}_{(x,y_w,y_l)\sim\mathcal{D}}[\log(\sigma(R_\phi(x,y_w),R_\phi(x,y_l)))]
$$

在 RLHF 框架下，我们可以在强化学习环境中利用这个学习到的奖励模型来优化 LLM，使其输出获得高奖励的完成。然而，DPO 采取了不同的策略——与两阶段 RLHF 流程不同，DPO 重新参数化了 Bradley-Terry 模型，以便我们可以使用类似的损失函数直接优化 LLM 的参数，使其产生人类观察者偏好的输出。

### 回答（补全）的概率

目前，基于偏好或奖励来优化 LLM 的想法可能显得相当抽象。因此，我们将花点时间介绍一个新的概率函数 $\pi(y|x)$ ，它代表 LLM 的实际输出。在强化学习符号中， $\pi$ 表示策略，策略经过优化以最大化奖励。具体来说， $\pi_\theta(y|x)$ 表示假设我们从提示词 $x$ 开始，基于参数为 $\theta$ 的 LLM 生成回答 $y$ 的概率。

“生成回答 $y$ 的概率”是什么意思？我们的 LLM 是一个自回归文本生成器，在每一步中，它都会为每个 token 计算一个概率值。

![[5.4.excalidraw|1000]]

因此，我们按顺序逐个处理回答 $y$ 中的每个 token，并计算出给定所有前面 token 的情况下，回答中下一个 token 的概率。现在，我们得到了回答中每个 token 的概率值！因此，我们可以计算生成 token 序列的联合概率，该概率是沿途观察到每个 token 的各个概率的乘积。

$$
\pi_\theta(y|x) = \prod_{t=0}^{|y|}p_{LLM_{\theta}}(y_t|x,y_{0:t})
$$

另一种思考方式是，存在一棵可能的补全树，我们正在计算从根（提示词的末尾）到叶子（停止 token ）追踪一条特定路径的概率。

![[5.5.excalidraw|1000]]

在训练时，我们提前知道整个文本的补全情况，因此，通过应用因果注意力掩码，我们可以通过 LLM 的单次前向传播来计算所有单独的下一个 token 的概率（以及 $\pi_\theta(y|x)$ ）。

## 3. 基于偏好优化我们的 LLM

好了，现在我们的框架已经搭建完毕。让我们回顾一下我们的目标：提升 LLM 的输出。换句话说，我们希望 LLM 为提示词 $x$ 提供的补全 $y$ 能够产生较大的奖励 $R(x,y)$ 。考虑到这一点，我们可以构建一个优化问题，其中我们想要找到 $\text{LLM}_{\theta}$ 的参数，使其能够最大化类似于实际操作中出现的提示的预期奖励。

$$
\max_\theta\mathbb{E}_{x\sim\mathcal{D},y\sim\pi_\theta(y|x)}[R(x,y)]
$$

然而，这有点过于简单了。在实践中，我们从微调后的基础模型的参数入手，并且我们坚信微调后的基础模型产生的输出相当不错，所以我们不希望模型的输出发生太大的变化，除非它们能够显著提高奖励。考虑到这一点，我们修改了优化问题，加入了正则化约束，以帮助强化这种信念。

$$
\max_\theta\mathbb{E}_{x\sim\mathcal{D},y\sim\pi_\theta(y|x)}[R(x,y)]-\beta\mathbb{D}_\text{KL}[\pi_\theta(y|x)\Vert\pi_\text{ref}(y|x)]
$$

$\mathbb{D}[P\Vert Q]$ 是 Kullback-Leibler 散度，也就是 KL 散度，是一种统计距离度量。它量化了概率分布 $P$ 与概率分布 $Q$ 的差异。这个基于 KL 散度的约束恰好体现了这样一种想法：我们希望根据模型 $\pi_\theta$ 输出与初始微调模型（例如参考模型 $\pi_\text{ref}$ ）输出的差异程度，对模型输出进行惩罚。$\beta$ 是一个标量超参数，用于控制约束的强度。

KL 散度是众多用于正则化强化学习智能体策略的传统方法之一。在 DPO 和 RLHF 的案例中，KL 散度是一个自然的选择，因为我们首先要有一个比较强大的参考模型——微调程序输出的 LLM 。

现在，我们想要推导出这个优化问题的最优解。这将依赖于吉布斯不等式， $\mathbb{D}_\text{KL}[P\Vert Q]\ge 0$ 。当且仅当 $P=Q$ 时， $\mathbb{D}_\text{KL}[P\Vert Q]=0$ 。

这里的直觉是，KL 散度是一种距离测量（某种程度上），如果 $P$ 和 $Q$ 相等，则它们之间没有距离，如果它们不相等，则必定存在一定距离。

$$
\begin{aligned}
& \max_\theta\mathbb{E}_{x\sim\mathcal{D},y\sim\pi_\theta(y|x)}[R(x,y)]-\beta\mathbb{D}_\text{KL}[\pi_\theta(y|x)\Vert\pi_\text{ref}(y|x)] \\
= & \max_\theta\mathbb{E}_{x\sim\mathcal{D},y\sim\pi_\theta(y|x)}[R(x,y)]-\beta\mathbb{E}_{y\sim\pi_\theta(y|x)}\left[{\log\frac{\pi_\theta(y|x)}{\pi_\text{ref}(y|x)}}\right] \\
= & \max_\theta\mathbb{E}_{x\sim\mathcal{D}}\mathbb{E}_{y\sim\pi_\theta(y|x)}\left[{R(x,y),\beta\log\frac{\pi_\theta(y|x)}{\pi_\text{ref}(y|x)}}\right] \\
= & \min_\theta\mathbb{E}_{x\sim\mathcal{D}}\mathbb{E}_{y\sim\pi_\theta(y|x)}\left[{\log\frac{\pi_\theta(y|x)}{\pi_\text{ref}(y|x)}-\frac{1}{\beta}R(x,y)}\right] \\
= & \min_{\theta}\mathbb{E}_{x\sim \mathcal{D}}\mathbb{E}_{y\sim \pi_\theta(y|x)}\left[\log\frac{\pi_\theta(y|x)}{\frac{1}{Z(x)}\pi_{\text{ref}}(y|x)e^{\frac{1}{\beta}R(x,y)}} - \log Z(x)\right] = \dots
\end{aligned}
$$

其中 $Z(x)=\sum_y\pi_{\text{ref}}(y|x)e^{\frac{1}{\beta}R(x,y)}$ 。重要的是，这个 $Z(x)$ 项仅取决于 $x$ 和 $\pi_\text{ref}$ ，而不取决于 $y$ 和 $\pi_\theta$ 。所以有如下式子

$$
\begin{aligned}
\dots &= \min_{\theta}\mathbb{E}_{x\sim \mathcal{D}}\left[\mathbb{E}_{y\sim \pi_\theta(y|x)}\left[\log\frac{\pi_\theta(y|x)}{\frac{1}{Z(x)}\pi_{ref}(y|x)e^{\frac{1}{\beta}R(x,y)}}\right] - \log Z(x)\right] \\
&= \min_{\theta}\mathbb{E}_{x\sim \mathcal{D}}\left[\mathbb{D}_\text{KL}\left(\pi_\theta(y|x)\ \Vert\ \frac{1}{Z(x)}\pi_{ref}(y|x)e^{\frac{1}{\beta}R(x,y)}\right) - \log Z(x)\right] \\
\end{aligned}
$$

我们快要成功了！由于 $Z(x)$ 不依赖于 $π_θ$，因此在推导最优解时我们可以忽略它。现在我们可以使用上面提到的吉布斯不等式：当且仅当 $\Vert$ 两边的两个分布相同时，$\mathbb{D}_\text{KL}\left(\pi_\theta(y|x)\ \Vert\ \frac{1}{Z(x)}\pi_{ref}(y|x)e^{\frac{1}{\beta}R(x,y)}\right)$ 最小为零。因此，对于所有 $x\in \mathcal{D}$，我们的优化问题的最优解（记为 $\pi^*$ ）是：

$$
\pi^*(y|x)=\pi_\theta(y|x)=\frac{1}{Z(x)}\pi_\text{ref}(y|x)e^{\frac{1}{\beta}R(x,y)}
$$

### 直接偏好优化

所以我们知道了优化问题的最优解，但我们能得到它吗？不能。$Z(x)=\sum_y\pi_\text{ref}(y|x)e^{\frac{1}{\beta}R(x,y)}$ 很难计算——计算它需要对所有可能的字符串进行求和。

相反，我们可以重新组织上面的最优解，以便用最优策略 $\pi_\theta$ 、参考策略 $\pi_\text{ref}$ 和难解函数 $Z$ 来表达奖励函数：

$$
R(x,y) = \beta\log\frac{\pi_\theta(y|x)}{\pi_\text{ref}(y|x)}+\beta\log Z(x)
$$

可以使用底层真实奖励 $R^*$ 及其对应的最优策略 $\pi^*$ 来应用相同的重组。

$$
R^*(x,y) = \beta\log\frac{\pi^*_\theta(y|x)}{\pi_\text{ref}(y|x)}+\beta\log Z(x)
$$

现在，DPO 的作者们注意到了一个巧妙的技巧。我们可以利用这个重新组织的优化问题最优解表达式，重新参数化上面的 Bradley-Terry 偏好模型，使其以最优策略 $\pi^*$ 的形式表示，而不是以底层奖励函数的形式！更妙的是，一旦我们把所有参数代入，就会发现难以处理的 $Z(x)$ 函数被抵消了！

$$
\begin{aligned}
p^*(y_1 \succ y_2 | x) &= \sigma(R^*(x,y_1)-R^*(x,y_2)) \\
&= \sigma\left({\beta\log\frac{\pi^*(y_1|x)}{\pi_\text{ref}(y_1|x)}} + \beta\log Z(x) - \left({{\beta\log\frac{\pi^*(y_2|x)}{\pi_\text{ref}(y_2|x)}} + \beta\log Z(x)}\right)\right) \\
&= \sigma\left({\beta\log\frac{\pi^*(y_1|x)}{\pi_\text{ref}(y_1|x)}-\beta\log\frac{\pi^*(y_2|x)}{\pi_\text{ref}(y_2|x)}}\right)
\end{aligned}
$$

现在，有了重新参数化的 Bradley-Terry 模型，我们可以使用监督学习直接学习一个模拟最优策略的策略。我们可以最小化偏好数据集 $\mathcal{D}$ 上的负对数似然损失函数，以估计策略 $\pi_\theta$ 的参数：

$$
\begin{aligned}
\mathcal{L}_{DPO}(\pi_\theta;\pi_{ref}) &= -\mathbb{E}_{(y_w,y_l,x)\sim \mathcal{D}}\left[\log\left(\sigma\left(\beta\log{\frac{\pi_\theta(y_w|x)}{\pi_{ref}(y_w|x)}} - \beta\log{\frac{\pi_\theta(y_l|x)}{\pi_{ref}(y_l|x)}}\right)\right)\right] \\
&= -\mathbb{E}_{(y_w,y_l,x)\sim \mathcal{D}}\left[\log\left(\sigma\left(\beta\left(\log{\frac{\pi_\theta(y_w|x)}{\pi_\theta(y_l|x)}} - \log{\frac{\pi_{ref}(y_w|x)}{\pi_{ref}(y_l|x)}}\right)\right)\right)\right]
\end{aligned}
$$

![[7.1.excalidraw|1000]]

回想一下，上面我们优化了一个负对数似然损失来估计奖励模型的参数，然后 RLHF 将其用于估计策略模型的参数。但现在我们直接根据人类偏好来优化 LLM 策略模型的参数！因此，我们称之为直接偏好优化。

![[5.3.excalidraw|1000]]

DPO 相对于 RLHF 的优势：

1. 避免了训练奖励模型来估计人类偏好
2. 避免进行任何类型的强化学习，因为众所周知，强化学习非常困难
3. 我们可以使用监督学习直接根据人类偏好优化我们的 LLM，这是一个更加直接和易于理解的过程

避免强化学习尤为重要。DPO 让偏好调整过程对那些缺乏时间、资源或专业知识来应对强化学习复杂性的从业者来说变得更加便捷。

### DPO 的性质和注意事项

DPO 的一个关键特性是，当 Bradley-Terry 模型完美拟合我们的偏好数据，并且 RLHF 学习到最优奖励函数时，RHLF 和 DPO 的全局优化器是相同的。

这是一个重要的等价结果；然而在实践中：

1. Bradley-Terry 模型通常不能完美地拟合偏好数据。
2. RLHF 学习到的奖励函数不会是最优的奖励函数。
3. 在高度非凸的损失景观（例如 LLM）上进行梯度下降找不到全局优化器。

```ad-note
例如，偏好循环会导致 Bradley-Terry 模型无法完美拟合数据。Bradley-Terry 模型假设偏好具有传递性。例如，如果 $A \succ B$ 和 $B \succ C$ 成立，则模型预期结果为 $A \succ B \succ C$ 。但如果结果为 $C \succ A$ ，则存在循环，传递性被破坏。
```

DPO 的另一个缺点是由于缺乏正则化，容易出现过拟合。Azar 等人提供了一个令人信服的例子：

```ad-cite
考虑一个简单的例子，我们有两个动作 $y_1$​ 和 $y_2$​ ，并且 $p^*(y_1\succ y_2)=1$ ，即 $y_1$ 总是优于 $y_2$ 。那么 Bradley-Terry 模型就要求 $(R(y_1)-R(y_2))\rightarrow\infty$ 必须满足。如果我们将其代入最优策略，就会得到 $\frac{\pi^*(y_2)}{\pi^*(y_1)}=0$ ，也就是 $\pi^*(y_2)=0$ 。 因此，偏好越确定，$\text{KL}$ 正则化的强度就越弱。
```

他们还指出，实际上，我们拥有的偏好数据量是有限的。因此，我们很可能仅仅因为只看到了 $y$ 和 $y'$ 之间少量的比较就凭经验估计出 $p(y_1\succ y_2)=1$ 。因此，无论试图使策略与参考策略保持相似的正则化项如何，经验最优策略都会推动 $\pi(y_2)=0$ 。

尽管存在这些缺点，DPO 仍然是一种非常有效的工具；许多最成功和性能最好的开源 LLM 都是使用 DPO 进行指令微调的。

## 4. DPO代码实现

```python
from modelscope import AutoModelForCausalLM, AutoTokenizer
import torch

device = 'cuda' if torch.cuda.is_available() else 'cpu'


def create_qwen_model():
    model = AutoModelForCausalLM.from_pretrained(
        "qwen/Qwen2-0.5B-Instruct",
        torch_dtype="auto",
        device_map="auto"
    )
    tokenizer = AutoTokenizer.from_pretrained("qwen/Qwen2-0.5B-Instruct")
    return model, tokenizer


# DPO训练的模型
model_pi, tokenizer = create_qwen_model()
# DPO参考的模型
model_ref, _ = create_qwen_model()


def chat(prompt, tokenizer, model):
    '''测试函数'''
    messages = [
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": prompt},
    ]
    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )
    # print(text)

    model_inputs = tokenizer([text], return_tensors="pt").to(device)

    generated_ids = model.generate(
        model_inputs.input_ids,
        max_new_tokens=512
    )
    generated_ids = [
        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
    ]

    response = tokenizer.batch_decode(
        generated_ids, skip_special_tokens=True)[0]
    return response


dpo_train_data = [
    {'prompt': '你是谁?', 'chosen': '我是你的医疗助手。',
        'reject': '我是阿里云开发的超大规模语言模型，我叫通义千问。'},
    {'prompt': '你是谁发明的?', 'chosen': '尚硅谷', 'reject': '阿里巴巴'},
]


def dpo_to_messages(dpo_pairs):
    '''将偏好数据集转换成模型的输入'''
    chosen_messages = []
    reject_messages = []
    for pair in dpo_pairs:
        chosen_messages.append([
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": pair['prompt']},
            {"role": "assistant", "content": pair['chosen']},
        ]
        )
        reject_messages.append([
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": pair['prompt']},
            {"role": "assistant", "content": pair['reject']},
        ]
        )
    return chosen_messages, reject_messages


def preprocess(tokenizer, batch_messages):
    '''对训练数据进行预处理'''
    input_list = []
    target_list = []

    im_start = tokenizer('<|im_start|>').input_ids
    im_end = tokenizer('<|im_end|>').input_ids
    newline = tokenizer('\n').input_ids
    pad = tokenizer('<|endoftext|>').input_ids
    ignore = [-100]

    for group in batch_messages:
        input_ids = []
        target_ids = []
        for msg in group:
            role = tokenizer(msg['role']).input_ids
            content = tokenizer(msg['content']).input_ids
            if msg['role'] in ['system', 'user']:
                ignore_parts = role+newline+content
                input_ids += im_start+ignore_parts+im_end+newline
                target_ids += im_start+ignore*len(ignore_parts)+im_end+newline
            else:
                ignore_parts = role+newline
                input_ids += im_start+ignore_parts+content+im_end+newline
                target_ids += im_start+ignore * \
                    len(ignore_parts)+content+im_end+newline
        input_list.append(input_ids)
        target_list.append(target_ids)

    # padding
    max_len = max([len(ids) for ids in input_list])
    for input_ids, target_ids in zip(input_list, target_list):
        input_ids += pad*(max_len-len(input_ids))
        target_ids += ignore*(max_len-len(target_ids))
    batch_input_ids = torch.tensor(input_list, dtype=torch.long)
    batch_target_ids = torch.tensor(target_list, dtype=torch.long)
    batch_mask = batch_input_ids.ne(pad[0]).type(torch.long)
    return batch_input_ids, batch_target_ids, batch_mask


model_pi.train()
model_ref.train()

# 优化器，只训练pi模型
optimizer = torch.optim.SGD(model_pi.parameters(), lr=1e-3)


def dpo_prob_calc(target_ids, pi_logits, ref_logits):
    '''DPO损失计算-辅助函数'''
    pi_probs = torch.log_softmax(pi_logits, dim=-1)      # softmax概率+log对数
    ref_probs = torch.log_softmax(ref_logits, dim=-1)

    ignore_mask = target_ids != -100  # ignore token掩码
    indexes = target_ids*ignore_mask  # 将-100变成0，以便后面gather可以运行

    # 取目标target token的概率，忽略-100 token
    pi_probs_of_target = torch.gather(
        pi_probs, dim=-1, index=indexes.unsqueeze(-1)).squeeze(-1) * ignore_mask
    ref_probs_of_target = torch.gather(
        ref_probs, dim=-1, index=indexes.unsqueeze(-1)).squeeze(-1) * ignore_mask

    # 求每一个样本的token prob均值
    pi_final_prob = pi_probs_of_target.sum(-1)/ignore_mask.sum(-1)
    ref_final_prob = ref_probs_of_target.sum(-1)/ignore_mask.sum(-1)
    return pi_final_prob, ref_final_prob


def dpo_loss(params):
    '''DPO损失函数 https://github.com/huggingface/trl/blob/main/trl/trainer/dpo_trainer.py'''
    # 两个模型的chosen输出
    chosen_target_ids = params['chosen_target_ids'][:, 1:]
    pi_chosen_logits = params['pi_chosen_logits'][:, :-1, :]
    ref_chosen_logits = params['ref_chosen_logits'][:, :-1, :]
    pi_chosen_prob, ref_chosen_prob = dpo_prob_calc(
        chosen_target_ids, pi_chosen_logits, ref_chosen_logits)

    # 两个模型的reject输出
    reject_target_ids = params['reject_target_ids'][:, 1:]
    pi_reject_logits = params['pi_reject_logits'][:, :-1, :]
    ref_reject_logits = params['ref_reject_logits'][:, :-1, :]
    pi_reject_prob, ref_reject_prob = dpo_prob_calc(
        reject_target_ids, pi_reject_logits, ref_reject_logits)

    # 计算DPO Loss
    pi_prob_diff = pi_chosen_prob-pi_reject_prob
    ref_prob_diff = ref_chosen_prob-ref_reject_prob
    beta = 0.1
    loss = -torch.nn.functional.logsigmoid(beta*(pi_prob_diff-ref_prob_diff))
    return loss.mean()


iterators = 50

vocab = tokenizer.get_vocab()
for i in range(iterators):
    # 一批模拟数据
    chosen_messages, reject_messages = dpo_to_messages(dpo_train_data)
    # model输入和输出
    chosen_input_ids, chosen_target_ids, chosen_mask = preprocess(
        tokenizer, chosen_messages)
    reject_input_ids, reject_target_ids, reject_mask = preprocess(
        tokenizer, reject_messages)
    # model_pi预测
    pi_chosen_logits = model_pi(input_ids=chosen_input_ids.to(
        device), attention_mask=chosen_mask.to(device)).logits
    pi_reject_logits = model_pi(input_ids=reject_input_ids.to(
        device), attention_mask=reject_mask.to(device)).logits
    # model_ref预测
    ref_chosen_logits = model_ref(chosen_input_ids.to(
        device), chosen_mask.to(device)).logits
    ref_reject_logits = model_ref(reject_input_ids.to(
        device), reject_mask.to(device)).logits
    # 求DPO损失
    loss = dpo_loss({
        'chosen_target_ids': chosen_target_ids.to(device),
        'reject_target_ids': reject_target_ids.to(device),
        'pi_chosen_logits': pi_chosen_logits.to(device),
        'pi_reject_logits': pi_reject_logits.to(device),
        'ref_chosen_logits': ref_chosen_logits.to(device),
        'ref_reject_logits': ref_reject_logits.to(device),
    })
    print(loss)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

model_pi.eval()

print(chat('你是谁?', tokenizer, model_pi))

print("===============")
print(chat('你是谁发明的?', tokenizer, model_pi))
print("===============")

print(chat('讲讲transformer模型', tokenizer, model_pi))
```

## 5. DPO训练智能客服（使用trl库）

代码如下：

```python
# train_dpo.py
import pprint
from datasets import load_dataset
from trl import DPOConfig, DPOTrainer
from modelscope import AutoModelForCausalLM, AutoTokenizer
# from transformers import AutoModelForCausalLM, AutoTokenizer
from torch.utils.data import Dataset


class dpo_dataset(Dataset):
    def __init__(self, file, tokenizer, max_seq_length):
        self.tokenizer = tokenizer
        self.max_seq_length = max_seq_length
        # 打开json文件 用transformers
        self.data_list = load_dataset("json", data_files=file)['train']

    def __len__(self):
        return len(self.data_list)

    def __getitem__(self, index):
        # 取出data_list的一条数据  --> {"chosen":xxx,"rejected":xxx,"prompt":xxx} 一条数据是这样的格式
        data = self.data_list[index]

        # 对prompt reject和chosen进行tokenize  判断是否需要截断 保证所有的input_ids都一样 不够长度的直接padding
        # 适配qwen 的 template  添加eos token
        prompt_input_ids = self.tokenizer.encode(
            '<|im_start|>' + data['prompt'] + '<|im_end|>', add_special_tokens=False)
        chosen_input_ids = self.tokenizer.encode(
            data['chosen'], add_special_tokens=False)
        rejected_input_ids = self.tokenizer.encode(
            data['rejected'], add_special_tokens=False)

        prompt_input_ids = prompt_input_ids + [self.tokenizer.pad_token_id]
        # 设置labels
        chosen_labels = [-100] * len(prompt_input_ids) + \
            chosen_input_ids + [self.tokenizer.pad_token_id]
        rejected_labels = [-100] * len(prompt_input_ids) + \
            rejected_input_ids + [self.tokenizer.pad_token_id]
        chosen_input_ids = prompt_input_ids + \
            chosen_input_ids + [self.tokenizer.pad_token_id]
        rejected_input_ids = prompt_input_ids + \
            rejected_input_ids + [self.tokenizer.pad_token_id]

        assert len(chosen_labels) == len(chosen_input_ids)
        assert len(rejected_labels) == len(rejected_input_ids)

        inputs = dict(
            prompt_input_ids=prompt_input_ids,
            prompt_attention_mask=[1]*len(prompt_input_ids),
            chosen_input_ids=chosen_input_ids,
            chosen_attention_mask=[1]*len(chosen_input_ids),
            chosen_labels=chosen_labels,
            rejected_input_ids=rejected_input_ids,
            rejected_attention_mask=[1]*len(rejected_input_ids),
            rejected_labels=rejected_labels,
        )
        return inputs

    def map(self, func, **kwargs):
        return self


model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen2.5-0.5B-Instruct")
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-0.5B-Instruct")
train_dataset = dpo_dataset(file='dpo_data.json', tokenizer=tokenizer, max_seq_length=50)
training_args = DPOConfig(output_dir="Qwen2.5-0.5B-DPO", num_train_epochs=20)
trainer = DPOTrainer(model=model, args=training_args,
                     processing_class=tokenizer, train_dataset=train_dataset)
trainer.train()
```

数据如下

```json
[
    {
        "prompt": "human\n你们这里有航天军工PCB板吗?",
        "chosen": "assistant\n您好，关于航天军工PCB板，我们需要先确认一下。麻烦您留下联系方式，稍后我们给您回电确认详情。",
        "rejected": "assistant\n对不起，我们这里没有航天军工PCB板，您可能需要去其他商家那里看看。"
    },
    {
        "prompt": "human\n你们这里有高速高频PCB吗?",
        "chosen": "assistant\n您好，关于高速高频PCB，我们需要先确认一下库存和型号。您可以留下您的联系方式吗？稍后我们给您回电确认。",
        "rejected": "assistant\n对不起，我们这里没有高速高频PCB，您可能需要去其他商家那里看看。"
    },
    {
        "prompt": "human\n你们这里有HDI盲埋孔板吗?",
        "chosen": "assistant\n您好，关于HDI盲埋孔板，我需要先确认一下库存和型号。您可以留下您的联系方式吗？稍后我们给您回电确认。",
        "rejected": "assistant\n对不起，我们这里没有HDI盲埋孔板，您可能需要去其他商家那里看看。"
    },
    {
        "prompt": "human\n你们这里有软硬结合板吗?",
        "chosen": "assistant\n您好，关于软硬结合板的库存情况，我需要先确认一下。您可以留下您的联系方式吗？稍后我们给您回电确认。",
        "rejected": "assistant\n对不起，我们这里没有软硬结合板，您可能需要去其他商家那里看看。"
    },
    {
        "prompt": "human\n你们这里有没有针对污染处理的菌种卖？",
        "chosen": "assistant\n您好，关于污染处理的菌种，我们需要先确认一下。您可以留下您的联系方式吗？稍后我们给您回电确认并提供详细信息。",
        "rejected": "assistant\n您好，我们这里没有针对污染处理的菌种。您可能需要去其他商家或者线下咨询相关专业人士。"
    },
    {
        "prompt": "human\n你们这里有没有处理印染废水有机的设备?",
        "chosen": "assistant\n您好，我们店有专门处理印染废水有机的设备，不过需要先确认一下您的具体需求。您可以留下联系方式，稍后我们给您回电详细介绍。",
        "rejected": "assistant\n您好，我们店里没有处理印染废水有机的设备，您可以去别家看看。"
    },
    {
        "prompt": "human\n你们这里有降解专用菌剂吗?",
        "chosen": "assistant\n您好，关于降解专用菌剂，我们需要先确认一下库存和型号。您可以留下联系方式吗？稍后我们给您回电确认。",
        "rejected": "assistant\n很抱歉，我们这里没有降解专用菌剂，您可能需要去其他商家那里看看。"
    },
    {
        "prompt": "human\n你们这里有氨氮去除菌剂和泄爆门吗?",
        "chosen": "assistant\n您好，我们店里有多种相关产品，氨氮去除菌剂和泄爆门的具体信息，需要您留下联系方式后，我们的客服会尽快给您回电确认并提供详细解答。",
        "rejected": "assistant\n很抱歉，我们店里没有氨氮去除菌剂和泄爆门，您可能需要去其他店铺或者线上平台看看。"
    },
    {
        "prompt": "human\n你们这里有防爆门吗?",
        "chosen": "assistant\n您好，我们有多种型号和规格的防爆门，您可以告诉我您的具体需求，我为您详细介绍一下。另外，如果您方便留下联系方式，我们可以安排专业人员为您提供更详细的解答和服务。",
        "rejected": "assistant\n对不起，我们这里没有防爆门，您可能需要去其他店铺或者线上平台寻找。"
    },
    {
        "prompt": "human\n你们这里有特种门窗吗？",
        "chosen": "assistant\n您好，关于特种门窗，我们需要先确认一下。您方便留下联系方式吗？稍后我们给您回电确认详情。",
        "rejected": "assistant\n对不起，我们这里没有特种门窗，您可能需要去其他商家那里看看。"
    },
    {
        "prompt": "human\n你们这里有防爆墙电池吗?",
        "chosen": "assistant\n您好，关于防爆墙电池，我们需要先确认一下库存和型号。您可以留下您的联系方式吗？稍后我们给您回电确认。",
        "rejected": "assistant\n抱歉，我们这里没有防爆墙电池，您可能需要去其他商家那里看看。"
    },
    {
        "prompt": "human\n定制防爆窗的价格是多少？",
        "chosen": "assistant\n您好，定制防爆窗的价格需要根据您的具体需求和规格来确定。您可以留下联系方式，我们的客服会尽快与您联系，为您提供详细的报价和服务。",
        "rejected": "assistant\n您好，我们店里没有定制防爆窗，您可以去其他店铺看看。"
    },
    {
        "prompt": "human\n你们店铺有钢制抗爆门吗?",
        "chosen": "assistant\n您好，关于钢制抗爆门，我们需要先确认一下。您可以留下您的联系方式吗？稍后我们给您回电确认详情。",
        "rejected": "assistant\n很抱歉，我们店里没有钢制抗爆门，您可能需要去其他店铺看看。"
    },
    {
        "prompt": "human\n你们这里有彩钢夹芯板吗？",
        "chosen": "assistant\n您好，关于彩钢夹芯板，我们需要先确认一下库存和型号。您可以留下您的联系方式吗？稍后我们给您回电确认详情。",
        "rejected": "assistant\n对不起，我们这里没有彩钢夹芯板，您可能需要去其他商家那里看看。"
    },
    {
        "prompt": "human\n你们这里有工业厂房抗爆窗吗?",
        "chosen": "assistant\n您好，我们确实有工业厂房抗爆窗，但需要先确认一下您的具体需求和规格。您可以留下联系方式，稍后我们给您回电详细解答。",
        "rejected": "assistant\n对不起，我们这里没有工业厂房抗爆窗，您可能需要去其他商家那里看看。"
    },
    {
        "prompt": "human\n你们这里有防火隔热泄爆窗吗?",
        "chosen": "assistant\n您好，我们确实有防火隔热泄爆窗，不过需要先确认一下您的具体需求和规格。您可以留下联系方式，稍后我们给您回电详细解答。",
        "rejected": "assistant\n对不起，我们这里没有防火隔热泄爆窗，您可能需要去其他商家那里看看。"
    },
    {
        "prompt": "human\n你们这里有卖油烟净化器的吗?",
        "chosen": "assistant\n您好，关于油烟净化器，我们需要先确认一下库存和型号。您可以留下您的联系方式吗？稍后我们给您回电确认详情。",
        "rejected": "assistant\n很抱歉，我们这里没有油烟净化器，您可能需要去其他商家那里看看。"
    },
    {
        "prompt": "human\n你们这里有油烟净化设备防盗门吗?",
        "chosen": "assistant\n您好，关于油烟净化设备防盗门，我们需要先确认一下。您可以留下您的联系方式，稍后我们给您回电确认是否有货以及详细信息。",
        "rejected": "assistant\n很抱歉，我们这里没有油烟净化设备防盗门，您可能需要去其他商家那里寻找。"
    },
    {
        "prompt": "human\n你们这里有防护栏卖吗？",
        "chosen": "assistant\n您好，关于防护栏的详细信息和库存情况，我需要先确认一下。您可以留下您的联系方式吗？稍后我们给您回电详细解答。",
        "rejected": "assistant\n对不起，我们这里没有防护栏，您可能需要去其他商家看看。"
    },
    {
        "prompt": "human\n你们这里能进行不锈钢加工吗?",
        "chosen": "assistant\n您好，我们店可以进行不锈钢加工，但需要具体了解您的需求。您方便留下联系方式吗？稍后我们客服会与您详细沟通。",
        "rejected": "assistant\n不好意思，我们这里不进行不锈钢加工，您可能需要去其他店铺看看。"
    },
    {
        "prompt": "human\n你们这有断桥铝窗吗?",
        "chosen": "assistant\n您好，我们有多种型号的断桥铝窗，不过需要先确认一下您具体的需求。您可以留下联系方式，稍后我们给您回电详细介绍。",
        "rejected": "assistant\n对不起，我们这里没有断桥铝窗，您可能需要去其他商家看看。"
    },
    {
        "prompt": "human\n你们这里有防护窗吗?",
        "chosen": "assistant\n您好，关于防护窗的详细情况，我需要先确认一下。您可以留下您的联系方式吗？稍后我们给您回电详细解答。",
        "rejected": "assistant\n对不起，我们这里没有防护窗，您可能需要去其他商家那里看看。"
    },
    {
        "prompt": "human\n你们有车库门卖吗?",
        "chosen": "assistant\n您好，我们有多种型号的车库门供您选择，您可以留下联系方式，我们的客服会与您详细沟通。",
        "rejected": "assistant\n对不起，我们没有车库门，您可能需要去其他地方看看。"
    }
]
```

微调前的回答

```
<zuoyuan>:
你们这有断桥铝窗吗?

<Qwen/Qwen2.5-0.5B-Instruct>:
是的，断桥铝窗是一种新型的玻璃幕墙结构，它利用断桥铝板作为主要框架，通过特殊的工艺和设计使得窗户具有轻质、高气密性、低热阻等优点。这种窗户在维护保养方面也相对简单，只需要定期清洁即可保持良好状态。

断桥铝窗的优点包括：
1. 轻便：相比普通钢化玻璃，断桥铝窗重量更轻。
2. 高气密性：能够有效隔绝空气中的水分和污染物进入室内。
3. 低热阻：有助于提高室内温度稳定性。
4. 易于安装和维修：比传统玻璃更加简便。
5. 环保节能：符合绿色建筑标准，有利于环境保护和节能减排。

总的来说，断桥铝窗因其独特的性能和环保优势，在现代住宅装饰中得到了广泛应用。如果您正在寻找一种适合家庭使用的轻质、高效的玻璃幕墙解决方案，请考虑使用断桥铝窗。
```

微调后的回答

```
<zuoyuan>:
你们这有断桥铝窗吗?

<Qwen2.5-0.5B-DPO/checkpoint-60/>:
您好，我们确实提供断桥铝窗产品。我们的专业团队将竭诚为您提供优质的定制服务，让您满意。如有任何问题，请随时与我们联系。祝您生活愉快！
```

