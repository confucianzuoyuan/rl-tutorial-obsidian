
```ad-note
Trust Region Policy Optimization(TRPO)
```

![[2.13.excalidraw]]

策略梯度法在强化学习中非常流行。其基本原理是利用梯度上升法来找到奖励增长最快的策略。然而，一阶优化器对于曲线区域来说，不太准确。我们可能会过度自信，做出错误的举动，从而破坏训练的进程。TRPO 是解决这一问题最常被引用的论文之一。然而，人们经常在解释 TRPO 时忽略其基本概念。在本章开始，我们将重点探讨策略梯度法的问题，并介绍三个基本概念：MM 算法、置信域和重要性采样。

## 策略梯度法的问题

在强化学习中，我们会优化策略 $\theta$ ，以获得最大预期折扣奖励。然而，仍存在一些挑战会影响策略梯度法的性能。

$$
\max_\theta J(\pi_\theta) = \underset{\tau\sim\pi_\theta}{\mathbb{E}}\left\lbrack{\sum_{t=0}^\infty\gamma^tr_t}\right\rbrack
$$

首先，策略梯度法计算奖励的最陡上升方向（ **策略梯度** $g$ ），并朝该方向更新策略。

![[3.1.excalidraw|600x100]]

然而，这种方法使用一阶导数，并将表面近似为平面。如果表面曲率较大，我们可能会做出可怕的举动。

![[3.2.excalidraw|600x100]]

**步子迈得太大会导致灾难** 。但如果步子太小，模型学习速度就会太慢。把奖励函数想象成高耸的山峰。如果新策略走得太远，它采取的行动可能会差之毫厘，最终从悬崖上掉下来。当我们重新开始探索时，我们从一个表现不佳的状态开始，采用局部糟糕的策略。性能会崩溃，而且需要很长时间才能恢复。

![[3.3.excalidraw|600x100]]

其次，在强化学习中很难找到合适的学习率。假设学习率是专门针对上图黄点调整的。该区域相对平坦，因此为了获得良好的学习速度，学习率应该高于平均值。但是，一步走错，我们就会从悬崖上掉到红点。红点处的梯度很高，当前的学习率会触发爆炸式策略更新。由于学习率对地形不敏感，策略梯度算法的收敛问题非常严重。

第三，我们是否应该限制策略变化，以免做出过于激进的举动？事实上，TRPO 就是这么做的。它限制了那些对地形敏感的参数变化。但提供这种解决方案并不显而易见。我们通过低水平的模型参数来调整策略。为了限制策略变化，模型参数的相应阈值是什么？我们如何将策略空间的变化转化为模型参数空间的变化？

第四，我们仅对一次策略更新采样整个轨迹。我们无法在每个时间步都更新策略。

![[3.4.excalidraw|600x100]]

为什么？将策略模型可视化为一张网络。增加某一点 $\pi(s)$ 的概率，也会拉高相邻点。轨迹内的状态相似，尤其是在用原始像素表示时。如果我们在每个时间步升级策略，我们实际上会在相似的点多次拉高网络。这些变化相互强化和放大，使训练变得非常敏感且不稳定。

![[3.5.excalidraw|600x100]]

考虑到一条轨迹可能有数百或数千步，每条轨迹都进行一次更新的 **样本效率** 不高。策略梯度法需要超过 1000 万甚至更多的训练时间步来进行小规模实验。对于机器人技术的实际模拟来说，这太昂贵了。

```ad-note
样本效率不高的意思是对样本的使用效率不高。因为策略梯度法每次采样完轨迹更新完策略后，这条轨迹就被抛弃了。
```

那么让我们从技术上总结一下策略梯度法面临的挑战：

- 重大的策略变化破坏了训练
- 无法轻松地映射策略和参数空间之间的变化，
- 不适当的学习率会导致梯度消失或爆炸
- 对样本的使用效率很差

```ad-note
事后看来，我们希望限制策略的变更，更好的是，任何变更都应该保证奖励的提升。我们需要一种更好、更精准的优化方法来制定更好的策略。
```

为了理解 TRPO，最好先讨论三个关键概念。

## 最小化最大化MM算法（Minorize-Maximization MM algorithm）

我们能保证任何策略更新都能提升预期奖励吗？这听起来似乎有些牵强，但理论上是可行的。**MM 算法** 通过最大化局部近似预期奖励的下界函数（下图中的蓝线） **来迭代** 实现这一点。

![[3.6.excalidraw|800x100]]

让我们更详细地讨论一下。我们从一个初始策略猜测开始。我们找到一个下限 $M$ ，它在当前猜测下局部近似于预期奖励 $\eta$ 。我们找到 $M$ 的最优点并将其用作下一个猜测。我们再次近似下限并重复迭代。最终，我们的猜测将收敛到最优策略。为了实现这一点， $M$ 应该比 $\eta$ 更容易优化。$M$ 可以是一个二次方程

$$
ax^2+bx+c
$$

矢量形式如下

$$
g \cdot (\theta-\theta_{old})-\frac{\beta}{2}(\theta-\theta_{old})^TF(\theta-\theta_{old})
$$

它是一个凸函数，而凸函数的优化已经研究的非常充分了。

**为什么 MM 算法会收敛到最优策略？** 如果 $M$ 是下限，它永远不会越过红线 $\eta$ 。但假设新策略的预期奖励 $\eta$ 更低，那么蓝线必然会越过 $\eta$ （如下图右侧所示），这与它是下限的说法相矛盾。

![[3.7.excalidraw|600x100]]

由于我们的策略有限，随着我们不断迭代，它将引导我们得到局部或全局最优策略。

```ad-note
通过优化局部近似 $\eta$ 的下限函数，它 **可以保证** 每次策略的改进，并最终引导我们找到最优策略。
```

## 置信域（信任区域）

优化方法主要有两种：线性搜索和置信域。梯度下降是一种线性搜索。我们首先确定下降方向，然后朝该方向迈出一步。

![[3.8.excalidraw|600x100]]

在置信域中，我们确定想要探索的最大步长，然后在该置信域内找到最优点。我们以初始最大步长 $\delta$ 作为置信域的半径（黄色圆圈）。

$$
\begin{split}
\max_{s\in\mathbb{R}^n}m_k(s) \\
s.t. \quad\Vert{s}\Vert\le\delta
\end{split}
$$

$m$ 是对原始目标函数 $f$ 的近似值。我们现在的目标是在半径 $\delta$ 内找到 $m$ 的最优点。我们不断重复这个过程，直到达到峰值。

![[3.9.excalidraw|600x100]]

为了更好地控制学习速度，我们可以根据曲面的曲率在运行时扩大或缩小 $\delta$ 。在传统的置信域方法中，由于我们用 $m$ 来近似目标函数 $f$ ，因此如果 $m$ 在最佳点处不能很好地近似 $f$ ，则一种可能性是缩小置信域。相反，如果近似值很好，我们就扩大它。但在强化学习中计算 $f$ 可能并不简单。或者，如果新策略和当前策略的差异变大（反之亦然），我们也可以缩小该区域。例如，为了不过度自信，如果策略变化太大，我们可以缩小置信域。

## 重要性采样

### 动机

重要性采样在抽样推理和强化学习中起着关键作用。在强化学习中，重要性采样利用先前从旧策略 $\pi'$ 中收集的样本来估计策略 $\pi$ 的价值函数。简单来说，计算采取某个动作的总奖励消耗的算力非常的大。但是，如果新动作与旧动作相对接近，重要性采样允许我们基于旧计算结果计算新的奖励。

具体来说，使用强化学习中的蒙特卡洛方法，每当我们更新策略 $\theta$ 时，我们都需要收集一个全新的轨迹来计算预期奖励。



一条轨迹可能包含数百步，单次更新效率极低。使用重要性采样，我们只需重复使用旧样本即可重新计算总奖励。然而，当当前策略与旧策略偏差过大时，准确率就会下降。因此，我们需要定期重新同步两个策略。

**重要性采样** 计算 $f(x)$ 的期望，其中 $x$ 具有数据分布 $p$ 。

$$
\mathbb{E}_{x\sim p}[f(x)]
$$

在重要性采样中，我们提供了不从 $p$ 中采样 $f(x)$ 值的选项。相反，我们从 $q$ 中采样数据，并使用 $p$ 和 $q$ 之间的概率比来重新校准结果。

$$
\mathbb{E}_{x\sim q}\left\lbrack{\frac{f(x)p(x)}{q(x)}}\right\rbrack
$$

在策略梯度法中，我们使用当前策略来计算策略梯度。


![[3.10.excalidraw|600x100]]

所以，每当策略改变时，我们都会收集新的样本。旧的样本无法重复使用。因此，策略梯度法的样本效率很差。 ==通过重要性采样，我们的目标可以重写，并且我们可以使用旧策略中的样本来计算策略梯度。==



![[3.11.excalidraw|600x100]]

但有一点需要注意，使用 $q$ 进行估计，

$$
\mathbb{E}_{x\sim q}\left[{\frac{f(x)p(x)}{q(x)}}\right]
$$

方差为

$$
\frac{1}{N}\left({\color{red}{\underset{x\sim P}{\mathbb{E}}\left[{\frac{P(x)}{Q(x)}f(x)^2}\right]}}-\underset{x\sim P}{\mathbb{E}}[f(x)]^2\right)
$$

如果 $P(x)/Q(x)$ 比率很高，估计的方差可能会爆炸式增长。所以，如果两个策略彼此差异很大，方差（也就是误差）可能会非常高。因此，我们不能长期使用旧样本。我们仍然需要使用当前策略相当频繁地重新采样轨迹（比如每 4 次迭代一次）。

### 使用重要性采样的目标函数

让我们详细讨论如何在策略梯度法中应用重要性采样概念。策略梯度方法中的方程如下：

$$
g = \nabla_\theta J(\pi_\theta) = \underset{\tau\sim\pi_\theta}{\mathbb{E}}\left\lbrack{\sum_{t=0}^\infty\gamma^t\nabla_\theta\log\pi_\theta(a_t|s_t)A^{\pi_\theta}(s_t,a_t)}\right\rbrack
$$

$$
\theta_{k+1} = \theta_k + \alpha g
$$

根据上面的梯度表达式，我们可以反向推导（积分）出目标函数。为了简单起见，我们将折扣因子 $\gamma$ 设置为 1 。

$$
L^{PG}(\theta) = \hat{\mathbb{E}_t}\left[{\log\pi_\theta(a_t|s_t)\hat{A_t}}\right]
$$

这也可以表示成重要性采样（IS）：

$$
L^{IS}_{\theta_{old}}(\theta) = \hat{\mathbb{E}_t}\left[{\frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}\hat{A_t}}\right]
$$

如下所示， ==两个目标函数相同==。即它们具有相同的最优解决方案。

![[3.12.excalidraw|600x100]]

优化目标中存在两个策略，这为我们提供了一种限制策略变更的正式方法。这是许多高级策略梯度方法的关键基础。此外，它还能让我们在实施变更之前评估可能的策略。

现在，我们完成了讨论所需的三个基本概念，并为 TRPO 的最后一部分做好了准备。

## 优化问题

在本节中，我们将定义想要优化的目标。我们将首先从数学角度进行阐述。数学部分比较难理解，但我们稍后会解释其直观含义，其实很简单。

**我们优化什么？** 优化策略 $\pi'$ 可以改写为：

$$
\max_{\pi'} J(\pi') = \max_{\pi'} J(\pi') - J(\pi)
$$

我们更改了符号，以遵循 TRPO 和 PPO 的论文。奖励函数符号已从 $\eta$ 更改为 $J$ 。$\pi'$ 是我们要优化的变量， $\pi$ 是旧策略。$J(\pi)$ 是旧策略 $\pi$ 的预期奖励（奖励的期望），它是一个常数，因此上述更改不会改变最终的答案。简而言之，最大化策略的奖励与最大化相对于特定策略的奖励相同。

## 函数 $\mathcal{L}$

现在，我们需要找到 MM 算法所需的下界函数。它的第一项是 $\mathcal{L}$， 定义为：

![[3.13.excalidraw|600x100]]
其中

$$
d^{\pi}(s)=(1-\gamma)\sum_{t=0}^\infty\gamma^tP(s_t=s|\pi)
$$
现在别被 $\mathcal{L}$ 吓到。$d$ 是带折扣因子的未来状态分布。如果 $\gamma=1$， $d$ 就是策略 $\pi$ 下的状态访问频率。$A$ 是优势函数（也就是校准后的预期奖励）。但我们可以简单地将 $\mathcal{L}$ 视为使用重要性采样来估计优势函数。

[TRPO 论文](https://arxiv.org/pdf/1502.05477.pdf)的附录 A 提供了两页的证明，确立了以下界限：

$$
|J(\pi')-(J(\pi)+\mathcal{L}_\pi(\pi'))|\le C\sqrt{\underset{s\sim d^\pi}{\mathbb{E}}\left[{D_{KL}(\pi'\Vert\pi)[s]}\right]}
$$

其中 $D_{KL}$ 是 $KL$ 散度，用于衡量两个数据分布 $p$ 和 $q$ 之间的差异。$KL$ 散度定义为：

$$
D_{KL}(P\Vert Q)=\sum_{x=1}^NP(x)\log\frac{P(x)}{Q(x)}
$$

经过一番思考，这是我们的最终下限 _M_ 。

![[3.14.excalidraw|600x100]]

我们的目标函数是

$$
\max_{\pi'}J(\pi')-J(\pi)
$$

总而言之，我们将使用 MM 算法来最大化：

$$
\max_{\pi'}\mathcal{L}_\pi(\pi')-C\sqrt{\underset{s\sim d^{\pi}}{\mathbb{E}}[D_{KL}(\pi'\Vert\pi)[s]]}
$$

![[3.15.excalidraw|800x100]]

下面的不等式很重要，因为我们可以为目标计算建立误差的上限。这为我们是否可以信任结果建立了一个置信域。

![[3.16.excalidraw|800x100]]

事实上，利用拉格朗日对偶法，我们的目标在数学上与使用置信域约束的目标相同。

![[3.17.excalidraw|800x100]]

## 回顾：目标函数

现在，我们建立每次 MM 迭代优化的目标函数。第一个目标称为 KL 惩罚函数，第二个目标称为 KL 约束函数。

![[3.18.excalidraw|800x100]]

## 保证单调改进

TRPO、PPO 和自然策略梯度的强大之处在于其建立在保证单调改进的概念之上。理论上，每次 TRPO 迭代中的策略更新都会创建一个更好的策略。接下来，我们将对保证单调改进进行一个简单的证明。我们可以证明（证明过程略去）

$$
\mathcal{L}_\pi(\pi) = 0
$$

所以

$$
M(\pi) = \mathcal{L}_\pi(\pi) - C\sqrt{\underset{s\sim d^\pi}{\mathbb{E}}[D_{KL}(\pi\Vert\pi)[s]]} = 0 - 0 = 0
$$

由上可以得到

$$
J(\pi')-J(\pi) \ge M(\pi') - M(\pi)
$$

由于 $M$ 是一个比较好优化的下界函数，所以我们只要优化 $M$ 使得

$$
M(\pi')-M(\pi)>0
$$

就一定能保证

$$
J(\pi')-J(\pi) > 0
$$

也就是一定能保证策略 $\pi'$ 比 $\pi$ 好。

事实上，新策略在真实目标函数上的改进会比下界近似值更大。

![[3.19.excalidraw|600x100]]

## 直观解释

我们能直观地解释这个等式吗？我们可以在当前策略周围局部近似预期优势函数。但是，当新策略与当前策略彼此偏离时，准确率会下降。不过，我们可以为误差设定一个上限。因此，只要我们在可信区域内优化局部近似，就能保证策略得到改进。在这个区域之外，就无法保证了。即使它可能有更好的计算值，其误差范围也无法保证策略得到改进。有了这样的保证，我们可以在可信区域内迭代地找到最优策略。因此，即使数学证明需要一些时间，但推理过程非常简单。

## KL惩罚 vs. KL约束

从数学上讲，如果我们拥有无限的计算资源，那么 KL 惩罚目标和 KL 约束目标是相同的。然而，在实践中，它们并非如此。

![[3.18.excalidraw|800x100]]

当 $\gamma$（折扣因子）接近 1 且相应的梯度步长过小时， $C$ 会变得非常高。一种解决方案是将 $C$ 和 $\delta$ 都设置为可调的超参数。

$$
C \propto \frac{\epsilon\gamma}{(1-\gamma)^2}
$$

在实践中， $δ$ 比 $C$ 更容易调整。$δ$ 施加了一个硬约束来控制策略空间中的不良情况。它限制了可能造成破坏性的策略变化。调整 $C$ 要困难得多。实证结果表明，它不能是一个固定值，需要更具自适应性。因此，置信域约束更受欢迎。需要注意的是，如果对 KL 惩罚方法进行某些增强（就像 PPO 方法一样），则可以解决这个问题。

我们已经定义了想要解决的优化问题。然而，解决这些问题并不容易。接下来，我们将探讨 **自然策略梯度方法** 如何解析地解决这个问题，以及 TRPO 如何弥补其不足。

## 自然策略梯度

自然策略梯度通过分析解决以下目标函数。

$$
\begin{split}
\pi_{k+1} = \arg\max_{\pi'}\mathcal{L}_{\pi_k}(\pi') \\
s.t. \quad\overset{\_}{D}_{KL}(\pi'\Vert\pi)\le\delta
\end{split}
$$

为了解决这个问题，我们可以使用泰勒级数将上述两个项展开到二阶。但是 $𝓛$ 的二阶比 KL 散度项小得多，因此会被忽略。

![[3.20.excalidraw|600x100]]
去掉所有零项，得到：

![[3.21.excalidraw|600x100]]
其中 $g$ 是策略梯度（与我们在策略梯度中学习到的相同）， $H$ 衡量策略相对于模型参数 $θ$ 的敏感度（曲率）。我们的目标是：

$$
\begin{split}
\theta_{k+1} = \arg & \max_{\theta}g^T(\theta-\theta_k) \\
& s.t. \quad\frac{1}{2}(\theta-\theta_k)^TH(\theta-\theta_k)\le\delta
\end{split}
$$
这是一个二次方程，可以通过解析解来求解：

![[3.22.excalidraw]]
下面的项将我们想要的策略空间中的变化映射到相应的参数空间中。

$$
H^{-1}g
$$
它提供了一种无论我们如何参数化策略模型都能起作用的解决方案。在不同的参数化方法中， $H$ 具有不同的值。但它会生成可映射到相同策略变化的参数变化（ $Δθ$ ） 。因此，我们的解决方案是模型不变的。梯度下降将空间近似为局部平坦的。这对应于我们熟悉的欧几里得距离（平方根距离）。如果我们将上面的 $H$ 替换为欧几里得距离，我们将意识到参数更新不是模型不变的。针对不同模型计算出的参数变化将参考不同的策略变化。这提供了自然策略梯度优于梯度上升的理论原因。

```ad-note
自然策略梯度是一种二阶优化方法，无论模型如何参数化（模型不变性），自然策略梯度方法都能起作用，且更加的精确。
```

$H$ 是具有以下一般形式的 Hessian 矩阵：

$$
\mathbf{H} = \nabla^2 f = 
\begin{bmatrix}
\frac{\partial^2 f}{\partial x_1^2} & \frac{\partial^2 f}{\partial x_1 \partial x_2} & \cdots & \frac{\partial^2 f}{\partial x_1 \partial x_n} \\
\frac{\partial^2 f}{\partial x_2 \partial x_1} & \frac{\partial^2 f}{\partial x_2^2} & \cdots & \frac{\partial^2 f}{\partial x_2 \partial x_n} \\
\vdots & \vdots & \ddots & \vdots \\
\frac{\partial^2 f}{\partial x_n \partial x_1} & \frac{\partial^2 f}{\partial x_n \partial x_2} & \cdots & \frac{\partial^2 f}{\partial x_n^2}
\end{bmatrix}
$$

这个特殊的矩阵衡量的是策略对数概率的曲率（二阶导数）。它有一个专门的术语： **Fisher 信息矩阵（FIM）** 。许多文献使用 $F$ 代替 $H$ 来表示 FIM。在本文中， $H$ 和 $F$ 均指 FIM。

$F$ 也可以用下面的表达式计算：

$$
F=\underset{s,a\sim\theta^k}{\mathbb{E}}\left[{\nabla_\theta\log\pi_\theta(a|s)|_{\theta_k}}{\nabla_\theta\log\pi_\theta(a|s)|_{\theta_k}^T}\right]
$$
以下是伪代码。我们可以从当前策略中采样轨迹，并用它们来计算优势函数。然后我们计算策略梯度，并使用上面的公式来计算 FIM。

```pseudo
    \begin{algorithm}
    \caption{1 自然策略梯度}
    \begin{algorithmic}
      \State 输入：初始化策略参数 $\theta_0$
      \For{$k = 0,1,2,\dots$}
      \State 使用策略 $\pi_k=\pi(\theta_k)$ 采集一组策略 $\mathcal{D}_k$
      \State 计算优势 $\hat{A}_t^{\pi_k}$
      \State 使用采集的一组轨迹计算
      \State 🔵 策略梯度 $\hat{g}_k$ （使用计算好的优势）
      \State 🔵 KL散度和Hessian/费舍尔信息矩阵 $\hat{H}_k$
      \State 计算自然策略梯度的更新
	  \State 
	  $
	  \quad\quad\quad\theta_{k+1}=\theta_k+\sqrt{\frac{2\delta}{\hat{g}_k^T\hat{H}_k^{-1}\hat{g}_k}}\hat{H}_k^{-1}\hat{g}_k
	  $
      \EndFor
      \end{algorithmic}
    \end{algorithm}
```

### 自然策略梯度存在的问题

如果策略由许多参数参数化，那么求 $H$ 的逆矩阵会非常昂贵，尤其是在深度网络中。此外，逆矩阵通常数值不稳定（数据不精确很容易放大误差）。

$$
\Delta\theta=\sqrt{\frac{2\delta}{\hat{g}_k^T\hat{H}_k^{-1}\hat{g}_k}}{\color{red}{\hat{H}_k^{-1}}}\hat{g}_k
$$

### 截断自然策略梯度

我们不想求 FIM 的逆，而是直接计算以下组合项

$$
x_k\approx\hat{H}_k^{-1}\hat{g}_k
$$

其中 $x$ 可解为

$$
\hat{H}_kx_k\approx\hat{g}
$$

我们将此方程转化为二次方程的优化问题：

$$
\begin{aligned}
& \text{求解}\quad Ax=b \\
& \text{等价于} \\
& \underset{x}{\text{最小化}}\quad f(x)=\frac{1}{2}x^TAx-b^Tx \\
& \text{因为}\quad f'(x)=Ax-b=0
\end{aligned}
$$
简而言之，我们可以将问题转化为优化以下二次方程：

$$
\min_{x\in\mathbb{R}^n}\frac{1}{2}x^THx-g^Tx
$$
为了优化这一点，我们应用了共轭梯度法。这个概念与梯度上升非常相似，但可以用更少的迭代次数完成。

在梯度上升过程中，我们总是遵循最陡的梯度。在我们的例子中，我们希望从黄点上升到红点。假设第一步沿着梯度轮廓线向右移动。为了再次遵循最陡的梯度，第二步可能会向上并稍微向左移动。这里我们可以提出一个问题：为什么第二步会向左移动？这听起来像是在抵消第一步取得的一些进展，即使每一步都更接近红点。

![[3.23.excalidraw|600x100]]

如果目标函数是二次函数，我们可以使用 **共轭梯度法** 来避免这种低效。如果模型有 $N$ 个参数，我们最多可以在 $N$ 次上升中找到最优点。第一步，我们沿着最深的梯度方向 $d$ ，并稳定在该方向的最优点（即目标函数在 $d$ 方向上的峰值）。然后，对于下一个搜索方向 $d_j$ ，它必须与所有先前的方向 $d_i$ 正交（共轭）。从数学上讲，这意味着

$$
d^T_{(i)}Ad_{(j)}=0
$$

我们可以想象这些向量经过 $A$ 的某种变换后彼此垂直。

![[3.24.excalidraw|600x100]]
共轭梯度法是指每次都找到一个与所有先前方向 $A$ 正交的搜索方向。这样我们就能确保之前取得的进展不会半途而废。然后，我们沿着这条搜索线，朝新的方向前进，直至找到最优点。

![[3.25.excalidraw|600x100]]

这些向量（方向）彼此独立，跨越 $N$ 维空间。因此，我们最多只需 $n$ 步即可解决这个问题。解释听起来很复杂，但实际上，该算法的复杂度远低于计算 $F$ 的逆。现在，我们可以计算

$$
x_k\approx\hat{H}_k^{-1}\hat{g}_k
$$

在 $N$ 次迭代中。截断自然策略梯度 (TNPG) 使用下面的共轭方法（红色部分）来代替计算 FIM 逆的要求。

```pseudo
    \begin{algorithm}
    \caption{2 截断自然策略梯度}
    \begin{algorithmic}
      \State 输入：初始化策略参数 $\theta_0$
      \For{$k = 0,1,2,\dots$}
      \State 使用策略 $\pi_k=\pi(\theta_k)$ 采集一组策略 $\mathcal{D}_k$
      \State 计算优势 $\hat{A}_t^{\pi_k}$
      \State 使用采集的一组轨迹计算
      \State 🔵 策略梯度 $\hat{g}_k$ （使用计算好的优势）
      \State 🔵 KL散度和Hessian向量积函数 $f(v)=\hat{H}_kv$
      \State $\color{red}使用共轭梯度法（CG）迭代 n_{cg} 次来计算 x_k\approx\hat{H}^{-1}_k\hat{g}_k$
      \State 估计更新的步长 $\Delta_k\approx\sqrt{\frac{2\delta}{x_k^T\hat{H}_kx_k}}x_k$
      \State $\theta_{k+1}=\theta_k+\Delta_k$
      \EndFor
      \end{algorithmic}
    \end{algorithm}
```

## 置信域策略优化（TRPO）

最后，我们将 TRPO 的所有内容整合在一起。TRPO 将共轭梯度法应用于自然策略梯度。但这还不够。如果某件事好得令人难以置信，它就可能并非如此。自然策略梯度的置信域非常小。我们将其放宽到更大的可调值。此外，我们进行的二次近似也降低了准确率。这些因素可能会给策略更新带来问题。在某些训练迭代中，性能可能会下降。一种缓解措施是在提交更改之前先验证新策略。为此，我们验证

